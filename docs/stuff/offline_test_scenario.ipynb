{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 17:54:41,777 - __main__ - INFO - [1201286376.py:52] - Looking for OpenAI configuration in current environment...\n",
      "2025-03-11 17:54:41,779 - __main__ - INFO - [1201286376.py:71] - Azure OpenAI configuration verified\n",
      "2025-03-11 17:54:41,782 - __main__ - INFO - [1201286376.py:812] - Found latest integrated solution folder: c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\n",
      "2025-03-11 17:54:41,784 - __main__ - INFO - [1201286376.py:816] - Found 7 code files\n",
      "2025-03-11 17:54:41,791 - __main__ - INFO - [1201286376.py:824] - Created output directories: test_scenarios_20250311_175441\n",
      "2025-03-11 17:54:41,794 - __main__ - INFO - [1201286376.py:71] - Azure OpenAI configuration verified\n",
      "2025-03-11 17:54:41,898 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_141\n",
      "2025-03-11 17:54:41,922 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_141 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_141_code.py\n",
      "2025-03-11 17:55:05,689 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:55:05,737 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_141_description.json\n",
      "2025-03-11 17:55:05,739 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_141\n",
      "2025-03-11 17:55:05,741 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_142\n",
      "2025-03-11 17:55:05,747 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_142 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_142_code.py\n",
      "2025-03-11 17:55:22,380 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:55:22,393 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_142_description.json\n",
      "2025-03-11 17:55:22,396 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_142\n",
      "2025-03-11 17:55:22,397 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_143\n",
      "2025-03-11 17:55:22,404 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_143 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_143_code.py\n",
      "2025-03-11 17:55:43,943 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:55:43,953 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_143_description.json\n",
      "2025-03-11 17:55:43,955 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_143\n",
      "2025-03-11 17:55:43,956 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_144\n",
      "2025-03-11 17:55:43,962 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_144 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_144_code.py\n",
      "2025-03-11 17:56:04,351 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:56:04,364 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_144_description.json\n",
      "2025-03-11 17:56:04,366 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_144\n",
      "2025-03-11 17:56:04,368 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_145\n",
      "2025-03-11 17:56:04,379 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_145 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_145_code.py\n",
      "2025-03-11 17:56:04,845 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:56:04,848 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 2.000000 seconds\n",
      "2025-03-11 17:56:30,100 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:56:30,107 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_145_description.json\n",
      "2025-03-11 17:56:30,108 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_145\n",
      "2025-03-11 17:56:30,109 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_146\n",
      "2025-03-11 17:56:30,114 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_146 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_146_code.py\n",
      "2025-03-11 17:57:00,066 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:57:00,075 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_146_description.json\n",
      "2025-03-11 17:57:00,078 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_146\n",
      "2025-03-11 17:57:00,079 - __main__ - INFO - [1201286376.py:319] - Processing code file for US_147\n",
      "2025-03-11 17:57:00,086 - __main__ - INFO - [1201286376.py:219] - Analyzing code for US_147 from c:\\Users\\ADITYANJ\\Downloads\\stuff\\integrated_solution_20250311_022136\\US_147_code.py\n",
      "2025-03-11 17:57:22,293 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:57:22,301 - __main__ - INFO - [1201286376.py:280] - Saved code description to test_scenarios_20250311_175441\\code_descriptions\\US_147_description.json\n",
      "2025-03-11 17:57:22,304 - __main__ - INFO - [1201286376.py:336] - Successfully generated description for US_147\n",
      "2025-03-11 17:57:22,305 - __main__ - INFO - [1201286376.py:828] - Generated 7 code descriptions\n",
      "2025-03-11 17:57:22,853 - __main__ - INFO - [1201286376.py:460] - Using columns: User Story = 'userstory', Tech Spec = 'tech spec'\n",
      "2025-03-11 17:57:22,859 - __main__ - INFO - [1201286376.py:493] - Successfully extracted 7 user stories and tech specs from Excel file\n",
      "2025-03-11 17:57:22,861 - __main__ - INFO - [1201286376.py:832] - Read 7 user stories and tech specs from Excel\n",
      "2025-03-11 17:57:22,862 - __main__ - INFO - [1201286376.py:71] - Azure OpenAI configuration verified\n",
      "2025-03-11 17:57:22,992 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_141\n",
      "2025-03-11 17:57:23,001 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_141\n",
      "2025-03-11 17:57:24,461 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:57:24,464 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2025-03-11 17:57:50,551 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:57:50,570 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_141_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_141_test_scenarios.md\n",
      "2025-03-11 17:57:50,572 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_141\n",
      "2025-03-11 17:57:50,575 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_142\n",
      "2025-03-11 17:57:50,585 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_142\n",
      "2025-03-11 17:57:50,931 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:57:50,934 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 10.000000 seconds\n",
      "2025-03-11 17:58:21,910 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:58:21,925 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_142_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_142_test_scenarios.md\n",
      "2025-03-11 17:58:21,929 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_142\n",
      "2025-03-11 17:58:21,930 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_143\n",
      "2025-03-11 17:58:21,940 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_143\n",
      "2025-03-11 17:58:22,268 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:58:22,271 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2025-03-11 17:58:55,574 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:58:55,593 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_143_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_143_test_scenarios.md\n",
      "2025-03-11 17:58:55,598 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_143\n",
      "2025-03-11 17:58:55,599 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_144\n",
      "2025-03-11 17:58:55,608 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_144\n",
      "2025-03-11 17:58:55,973 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:58:55,977 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 7.000000 seconds\n",
      "2025-03-11 17:59:26,900 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 17:59:26,917 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_144_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_144_test_scenarios.md\n",
      "2025-03-11 17:59:26,921 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_144\n",
      "2025-03-11 17:59:26,923 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_145\n",
      "2025-03-11 17:59:26,931 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_145\n",
      "2025-03-11 17:59:27,294 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-11 17:59:27,297 - openai._base_client - INFO - [_base_client.py:1051] - Retrying request to /chat/completions in 8.000000 seconds\n",
      "2025-03-11 18:00:11,966 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 18:00:11,983 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_145_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_145_test_scenarios.md\n",
      "2025-03-11 18:00:11,986 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_145\n",
      "2025-03-11 18:00:11,988 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_146\n",
      "2025-03-11 18:00:11,996 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_146\n",
      "2025-03-11 18:00:37,971 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 18:00:37,989 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_146_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_146_test_scenarios.md\n",
      "2025-03-11 18:00:37,993 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_146\n",
      "2025-03-11 18:00:37,996 - __main__ - INFO - [1201286376.py:709] - Generating test scenarios for US_147\n",
      "2025-03-11 18:00:38,006 - __main__ - INFO - [1201286376.py:529] - Generating test scenarios for US_147\n",
      "2025-03-11 18:00:57,218 - httpx - INFO - [_client.py:1026] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-11 18:00:57,232 - __main__ - INFO - [1201286376.py:596] - Saved test scenarios to test_scenarios_20250311_175441\\test_scenarios\\US_147_test_scenarios.json and test_scenarios_20250311_175441\\test_scenarios\\US_147_test_scenarios.md\n",
      "2025-03-11 18:00:57,234 - __main__ - INFO - [1201286376.py:728] - Successfully generated test scenarios for US_147\n",
      "2025-03-11 18:00:57,235 - __main__ - INFO - [1201286376.py:836] - Generated test scenarios for 7 modules\n",
      "2025-03-11 18:00:57,242 - __main__ - INFO - [1201286376.py:790] - Created summary report at test_scenarios_20250311_175441\\summary_report.json and test_scenarios_20250311_175441\\summary_report.md\n",
      "2025-03-11 18:00:57,243 - __main__ - INFO - [1201286376.py:841] - Test scenario generation completed successfully. Output directory: test_scenarios_20250311_175441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test scenario generation completed successfully. Output directory: test_scenarios_20250311_175441\n",
      "\n",
      "Workflow completed successfully!\n",
      "Test scenarios and code descriptions saved to: test_scenarios_20250311_175441\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Scenario Generator for Integrated Solution\n",
    "\n",
    "This script provides an end-to-end solution that:\n",
    "1. Finds the latest integrated_solution folder\n",
    "2. Analyzes each US_[number]_code.py file to create detailed descriptions\n",
    "3. Saves these descriptions in JSON format\n",
    "4. Reads the original technical specifications from the Excel file\n",
    "5. Combines code descriptions, tech specs, and user stories to generate test scenarios\n",
    "6. Saves the test scenarios in both JSON and Markdown formats\n",
    "\n",
    "The workflow is completely automated from integrated solution to test scenarios.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import textwrap\n",
    "from typing import TypedDict, Annotated, List, Dict, Tuple, Optional, Set, Any\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "def check_openai_config():\n",
    "    \"\"\"Check if Azure OpenAI config is set in environment variables.\"\"\"\n",
    "    required_vars = [\n",
    "        \"AZURE_OPENAI_API_KEY\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"AZURE_OPENAI_API_VERSION\",\n",
    "        \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"\n",
    "    ]\n",
    "    \n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        # Check if we can set values explicitly\n",
    "        logger.info(\"Looking for OpenAI configuration in current environment...\")\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_ENDPOINT\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_VERSION\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "    \n",
    "    # Verify all variables are set\n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing Azure OpenAI configuration: {', '.join(missing)}\")\n",
    "    \n",
    "    logger.info(\"Azure OpenAI configuration verified\")\n",
    "\n",
    "#############################################################\n",
    "# Part 1: Code Description Generation\n",
    "#############################################################\n",
    "\n",
    "class CodeDescriptionState(TypedDict):\n",
    "    \"\"\"State management for code description process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_description: dict\n",
    "    user_story_id: str\n",
    "    file_path: str\n",
    "\n",
    "# Define prompts for code description\n",
    "code_analyzer_prompt = \"\"\"\n",
    "Role: Python Code Analyst\n",
    "Task: Generate a detailed description of the provided Python code.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the provided code and create a comprehensive description of its functionality, components, and architecture.\n",
    "2. Focus on the following aspects:\n",
    "   - Overall purpose and functionality\n",
    "   - Key classes and their responsibilities\n",
    "   - Main functions and their roles\n",
    "   - Data structures used\n",
    "   - Input/output handling\n",
    "   - Error handling mechanisms\n",
    "   - Any notable algorithms or patterns\n",
    "   - Dependencies and external libraries\n",
    "   - Configuration and settings\n",
    "\n",
    "3. Organize your analysis in a structured manner, with clear sections for each major component.\n",
    "\n",
    "Code to analyze:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "Your response must be a valid JSON object with the following structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"module_name\": \"{module_name}\",\n",
    "  \"overall_purpose\": \"A concise description of what this code does\",\n",
    "  \"architecture\": {{\n",
    "    \"description\": \"Overall architectural approach\",\n",
    "    \"patterns_used\": [\"pattern1\", \"pattern2\"]\n",
    "  }},\n",
    "  \"key_components\": [\n",
    "    {{\n",
    "      \"name\": \"ComponentName\",\n",
    "      \"type\": \"class/function\",\n",
    "      \"purpose\": \"What this component does\",\n",
    "      \"functionality\": \"Detailed description of how it works\"\n",
    "    }}\n",
    "  ],\n",
    "  \"data_flow\": \"Description of how data moves through the system\",\n",
    "  \"input_handling\": \"How inputs are processed\",\n",
    "  \"output_handling\": \"How outputs are generated\",\n",
    "  \"error_handling\": \"Error handling approach\",\n",
    "  \"dependencies\": [\"dependency1\", \"dependency2\"],\n",
    "  \"notable_algorithms\": [\n",
    "    {{\n",
    "      \"name\": \"Algorithm name\",\n",
    "      \"purpose\": \"What it accomplishes\",\n",
    "      \"description\": \"How it works\"\n",
    "    }}\n",
    "  ],\n",
    "  \"configuration\": \"How the code is configured\",\n",
    "  \"assumptions\": [\"assumption1\", \"assumption2\"],\n",
    "  \"limitations\": [\"limitation1\", \"limitation2\"]\n",
    "}}\n",
    "```\n",
    "\n",
    "Remember to make your JSON valid. Escape quotes within strings and ensure the structure matches exactly what is requested.\n",
    "\"\"\"\n",
    "\n",
    "def find_latest_integrated_solution_folder(base_dir=None):\n",
    "    \"\"\"Find the latest integrated_solution folder based on creation time.\"\"\"\n",
    "    if base_dir is None:\n",
    "        base_dir = os.getcwd()  # Current working directory\n",
    "    \n",
    "    integrated_folders = [d for d in os.listdir(base_dir) if d.startswith(\"integrated_solution_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not integrated_folders:\n",
    "        raise FileNotFoundError(\"No integrated_solution folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    integrated_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, integrated_folders[0])\n",
    "\n",
    "def find_code_files(solution_folder):\n",
    "    \"\"\"Find all US_[number]_code.py files in the integrated solution folder.\"\"\"\n",
    "    pattern = os.path.join(solution_folder, \"US_*_code.py\")\n",
    "    return glob(pattern)\n",
    "\n",
    "def create_output_directory():\n",
    "    \"\"\"Create output directory for code descriptions and test scenarios.\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = f\"test_scenarios_{timestamp}\"\n",
    "    \n",
    "    # Create main directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    descriptions_dir = os.path.join(output_dir, \"code_descriptions\")\n",
    "    test_scenarios_dir = os.path.join(output_dir, \"test_scenarios\")\n",
    "    \n",
    "    os.makedirs(descriptions_dir, exist_ok=True)\n",
    "    os.makedirs(test_scenarios_dir, exist_ok=True)\n",
    "    \n",
    "    return output_dir, descriptions_dir, test_scenarios_dir\n",
    "\n",
    "def extract_user_story_id(file_path):\n",
    "    \"\"\"Extract user story ID from filename.\"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    match = re.match(r'US_(\\d+)_code\\.py', filename)\n",
    "    if match:\n",
    "        return f\"US_{match.group(1)}\"\n",
    "    return None\n",
    "\n",
    "class CodeDescriptionGenerator:\n",
    "    \"\"\"Main class for generating detailed code descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self, model, output_dir, system_analyzer=\"\"):\n",
    "        self.system_analyzer = system_analyzer\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeDescriptionState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"analyzer\", self.analyzer)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"analyzer\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"analyzer\")\n",
    "        self.graph = graph.compile()\n",
    "        self.model = model\n",
    "    \n",
    "    def analyzer(self, state: CodeDescriptionState):\n",
    "        \"\"\"Generate code description\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'unknown_id')\n",
    "        file_path = state.get('file_path', '')\n",
    "        \n",
    "        logger.info(f\"Analyzing code for {user_story_id} from {file_path}\")\n",
    "        \n",
    "        if self.system_analyzer:\n",
    "            # Read code file\n",
    "            with open(file_path, 'r') as f:\n",
    "                code = f.read()\n",
    "            \n",
    "            module_name = os.path.basename(file_path)\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = self.system_analyzer.format(\n",
    "                code=code,\n",
    "                module_name=module_name\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)]\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        description = self._extract_json_from_response(response_text, user_story_id)\n",
    "        \n",
    "        # Save description\n",
    "        self._save_description(description, user_story_id)\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_description': description,\n",
    "            'user_story_id': user_story_id,\n",
    "            'file_path': file_path\n",
    "        }\n",
    "    \n",
    "    def _extract_json_from_response(self, response_text, user_story_id):\n",
    "        \"\"\"Extract JSON from the LLM response.\"\"\"\n",
    "        try:\n",
    "            # Try to find JSON block first\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, response_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                return json.loads(json_str)\n",
    "            \n",
    "            # If no JSON block, try to parse the whole response\n",
    "            return json.loads(response_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to extract JSON for {user_story_id}: {e}\")\n",
    "            \n",
    "            # Create a fallback description\n",
    "            return {\n",
    "                \"module_name\": user_story_id,\n",
    "                \"overall_purpose\": \"Could not extract description\",\n",
    "                \"error\": str(e),\n",
    "                \"raw_response\": response_text\n",
    "            }\n",
    "    \n",
    "    def _save_description(self, description, user_story_id):\n",
    "        \"\"\"Save code description to JSON file.\"\"\"\n",
    "        file_path = os.path.join(self.output_dir, f\"{user_story_id}_description.json\")\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(description, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved code description to {file_path}\")\n",
    "\n",
    "def process_code_files(code_files, descriptions_dir):\n",
    "    \"\"\"\n",
    "    Process all code files to generate descriptions\n",
    "    \n",
    "    Args:\n",
    "        code_files: List of paths to code files\n",
    "        descriptions_dir: Directory to save descriptions\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping user story IDs to their descriptions\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI model\n",
    "    check_openai_config()\n",
    "    \n",
    "    model = AzureChatOpenAI(\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize code description generator\n",
    "    code_desc_gen = CodeDescriptionGenerator(\n",
    "        model=model,\n",
    "        output_dir=descriptions_dir,\n",
    "        system_analyzer=code_analyzer_prompt\n",
    "    )\n",
    "    \n",
    "    descriptions = {}\n",
    "    \n",
    "    # Process each code file\n",
    "    for file_path in code_files:\n",
    "        user_story_id = extract_user_story_id(file_path)\n",
    "        if not user_story_id:\n",
    "            logger.warning(f\"Could not extract user story ID from {file_path}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Processing code file for {user_story_id}\")\n",
    "        \n",
    "        # Setup initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [],\n",
    "            \"current_description\": {},\n",
    "            \"user_story_id\": user_story_id,\n",
    "            \"file_path\": file_path\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run the graph\n",
    "            result = code_desc_gen.graph.invoke(initial_state)\n",
    "            \n",
    "            # Store description\n",
    "            if 'current_description' in result and result['current_description']:\n",
    "                descriptions[user_story_id] = result['current_description']\n",
    "                logger.info(f\"Successfully generated description for {user_story_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing code file for {user_story_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "#############################################################\n",
    "# Part 2: Test Case Generation\n",
    "#############################################################\n",
    "\n",
    "class TestScenarioState(TypedDict):\n",
    "    \"\"\"State management for test scenario generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_test_scenarios: dict\n",
    "    user_story_id: str\n",
    "    code_description: dict\n",
    "    tech_spec: str\n",
    "    user_story: str\n",
    "\n",
    "# Define prompts for test scenario generation\n",
    "test_scenario_generator_prompt = \"\"\"\n",
    "Role: Test Scenario Designer\n",
    "Task: Generate test scenarios for a software module based on its description, technical specifications, and user story.\n",
    "\n",
    "Provided Information:\n",
    "- Code Description: A detailed analysis of the module's functionality and components\n",
    "- Technical Specification: The original technical requirements that guided development\n",
    "- User Story: The business need that the code addresses\n",
    "\n",
    "Instructions:\n",
    "1. Analyze all provided information to understand the module's purpose, functionality, and requirements.\n",
    "2. Design test scenarios that cover:\n",
    "   - Main functionality (core features and capabilities)\n",
    "   - Alternative flows (different ways to achieve the same goal)\n",
    "   - Edge cases (extreme or unusual situations)\n",
    "   - Error handling (how the system should respond to invalid inputs)\n",
    "   - Integration points (how it connects with other components)\n",
    "\n",
    "3. For each test scenario, provide:\n",
    "   - A unique identifier\n",
    "   - A descriptive name\n",
    "   - A brief description of the scenario context\n",
    "   - What should be tested\n",
    "   - The expected outcome\n",
    "   - Test category (e.g., functional, edge case, error, integration)\n",
    "\n",
    "Code Description:\n",
    "{code_description}\n",
    "\n",
    "Technical Specification:\n",
    "{tech_spec}\n",
    "\n",
    "User Story:\n",
    "{user_story}\n",
    "\n",
    "## Output Format\n",
    "Your response must be a valid JSON object with the following structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"module_id\": \"{user_story_id}\",\n",
    "  \"test_suite_name\": \"Test Scenarios for {user_story_id}\",\n",
    "  \"summary\": \"A brief overview of the test approach\",\n",
    "  \"test_scenarios\": [\n",
    "    {{\n",
    "      \"id\": \"TS-{user_story_id}-001\",\n",
    "      \"name\": \"Scenario descriptive name\",\n",
    "      \"category\": \"functional|edge_case|error|integration|performance\",\n",
    "      \"description\": \"A description of the scenario context and what is being tested\",\n",
    "      \"test_objective\": \"What specific aspect or functionality is being verified\",\n",
    "      \"expected_outcome\": \"What should happen if the system works correctly\",\n",
    "      \"relevant_requirements\": \"References to specific requirements or user story elements\"\n",
    "    }},\n",
    "    {{\n",
    "      \"id\": \"TS-{user_story_id}-002\",\n",
    "      \"name\": \"Another test scenario\",\n",
    "      ...\n",
    "    }}\n",
    "  ],\n",
    "  \"coverage\": {{\n",
    "    \"functional_areas\": [\"area1\", \"area2\"],\n",
    "    \"edge_cases\": [\"case1\", \"case2\"],\n",
    "    \"not_covered\": [\"limitation1\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "Generate at least 5-8 test scenarios that together provide good coverage of the module's functionality. Focus on capturing the essence of what needs to be tested rather than detailed test steps.\n",
    "\"\"\"\n",
    "\n",
    "def read_tech_specs_and_user_stories(excel_file_path):\n",
    "    \"\"\"Read technical specifications and user stories from Excel file.\"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Find the user story column and tech spec column\n",
    "        user_story_col = None\n",
    "        tech_spec_col = None\n",
    "        \n",
    "        # Determine column names - assuming first row has column headers\n",
    "        col_names = df.columns.tolist()\n",
    "        \n",
    "        # Find user story column\n",
    "        for col in col_names:\n",
    "            if 'user' in str(col).lower() and 'story' in str(col).lower():\n",
    "                user_story_col = col\n",
    "                break\n",
    "        \n",
    "        # Find tech spec column\n",
    "        for col in col_names:\n",
    "            if ('tech' in str(col).lower() and 'spec' in str(col).lower()) or 'requirement' in str(col).lower():\n",
    "                tech_spec_col = col\n",
    "                break\n",
    "        \n",
    "        # If we didn't find the right columns, default to the first two\n",
    "        if user_story_col is None and len(col_names) > 0:\n",
    "            user_story_col = col_names[0]\n",
    "        \n",
    "        if tech_spec_col is None and len(col_names) > 1:\n",
    "            tech_spec_col = col_names[1]\n",
    "        \n",
    "        logger.info(f\"Using columns: User Story = '{user_story_col}', Tech Spec = '{tech_spec_col}'\")\n",
    "        \n",
    "        # Extract user stories and tech specs\n",
    "        user_stories = {}\n",
    "        tech_specs = {}\n",
    "        \n",
    "        # Skip the first row if it's empty\n",
    "        start_row = 1 if df.iloc[0].isna().all() else 0\n",
    "        \n",
    "        for idx, row in df.iloc[start_row:].iterrows():\n",
    "            if pd.isna(row[user_story_col]) or pd.isna(row[tech_spec_col]):\n",
    "                logger.warning(f\"Skipping row {idx} due to missing data\")\n",
    "                continue\n",
    "                \n",
    "            user_story_text = str(row[user_story_col])\n",
    "            tech_spec_text = str(row[tech_spec_col])\n",
    "            \n",
    "            # Extract user story ID\n",
    "            match = re.search(r'User\\s+Story\\s+ID\\s*:\\s*(\\d+)', user_story_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                user_story_id = f\"US_{match.group(1)}\"\n",
    "                user_stories[user_story_id] = user_story_text\n",
    "                tech_specs[user_story_id] = tech_spec_text\n",
    "            else:\n",
    "                # Alternative pattern\n",
    "                match = re.search(r'^(?:(?:user)?story|us)(\\d+)', user_story_text, re.IGNORECASE | re.MULTILINE)\n",
    "                if match:\n",
    "                    user_story_id = f\"US_{match.group(1)}\"\n",
    "                    user_stories[user_story_id] = user_story_text\n",
    "                    tech_specs[user_story_id] = tech_spec_text\n",
    "                else:\n",
    "                    logger.warning(f\"Could not extract user story ID from: {user_story_text[:50]}...\")\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(user_stories)} user stories and tech specs from Excel file\")\n",
    "        return user_stories, tech_specs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading Excel file: {e}\")\n",
    "        return {}, {}\n",
    "\n",
    "class TestScenarioGenerator:\n",
    "    \"\"\"Main class for generating test scenarios\"\"\"\n",
    "    \n",
    "    def __init__(self, model, output_dir, system_generator=\"\"):\n",
    "        self.system_generator = system_generator\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(TestScenarioState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"generator\", self.generator)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"generator\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"generator\")\n",
    "        self.graph = graph.compile()\n",
    "        self.model = model\n",
    "    \n",
    "    def generator(self, state: TestScenarioState):\n",
    "        \"\"\"Generate test scenarios\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'unknown_id')\n",
    "        code_description = state.get('code_description', {})\n",
    "        tech_spec = state.get('tech_spec', '')\n",
    "        user_story = state.get('user_story', '')\n",
    "        \n",
    "        logger.info(f\"Generating test scenarios for {user_story_id}\")\n",
    "        \n",
    "        if self.system_generator:\n",
    "            # Format the prompt\n",
    "            formatted_prompt = self.system_generator.format(\n",
    "                code_description=json.dumps(code_description, indent=2),\n",
    "                tech_spec=tech_spec,\n",
    "                user_story=user_story,\n",
    "                user_story_id=user_story_id\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)]\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        test_scenarios = self._extract_json_from_response(response_text, user_story_id)\n",
    "        \n",
    "        # Save test scenarios\n",
    "        self._save_test_scenarios(test_scenarios, user_story_id)\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_test_scenarios': test_scenarios,\n",
    "            'user_story_id': user_story_id,\n",
    "            'code_description': code_description,\n",
    "            'tech_spec': tech_spec,\n",
    "            'user_story': user_story\n",
    "        }\n",
    "    \n",
    "    def _extract_json_from_response(self, response_text, user_story_id):\n",
    "        \"\"\"Extract JSON from the LLM response.\"\"\"\n",
    "        try:\n",
    "            # Try to find JSON block first\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, response_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(1)\n",
    "                return json.loads(json_str)\n",
    "            \n",
    "            # If no JSON block, try to parse the whole response\n",
    "            return json.loads(response_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to extract JSON for test scenarios {user_story_id}: {e}\")\n",
    "            \n",
    "            # Create a fallback structure\n",
    "            return {\n",
    "                \"module_id\": user_story_id,\n",
    "                \"test_suite_name\": f\"Test Scenarios for {user_story_id}\",\n",
    "                \"summary\": \"Error extracting test scenarios\",\n",
    "                \"test_scenarios\": [],\n",
    "                \"error\": str(e),\n",
    "                \"raw_response\": response_text\n",
    "            }\n",
    "    \n",
    "    def _save_test_scenarios(self, test_scenarios, user_story_id):\n",
    "        \"\"\"Save test scenarios to JSON and MD files.\"\"\"\n",
    "        # Save as JSON\n",
    "        json_path = os.path.join(self.output_dir, f\"{user_story_id}_test_scenarios.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(test_scenarios, f, indent=2)\n",
    "        \n",
    "        # Save as Markdown\n",
    "        md_path = os.path.join(self.output_dir, f\"{user_story_id}_test_scenarios.md\")\n",
    "        with open(md_path, 'w') as f:\n",
    "            f.write(self._convert_to_markdown(test_scenarios))\n",
    "        \n",
    "        logger.info(f\"Saved test scenarios to {json_path} and {md_path}\")\n",
    "    \n",
    "    def _convert_to_markdown(self, test_scenarios):\n",
    "        \"\"\"Convert test scenarios JSON to Markdown format.\"\"\"\n",
    "        md_content = []\n",
    "        \n",
    "        # Add header\n",
    "        md_content.append(f\"# {test_scenarios.get('test_suite_name', 'Test Scenarios')}\")\n",
    "        md_content.append(\"\")\n",
    "        \n",
    "        # Add summary\n",
    "        md_content.append(\"## Summary\")\n",
    "        md_content.append(test_scenarios.get('summary', 'No summary provided.'))\n",
    "        md_content.append(\"\")\n",
    "        \n",
    "        # Add test coverage\n",
    "        coverage = test_scenarios.get('coverage', {})\n",
    "        if coverage:\n",
    "            md_content.append(\"## Test Coverage\")\n",
    "            \n",
    "            # Functional areas\n",
    "            functional = coverage.get('functional_areas', [])\n",
    "            if functional:\n",
    "                md_content.append(\"### Functional Areas\")\n",
    "                for area in functional:\n",
    "                    md_content.append(f\"- {area}\")\n",
    "                md_content.append(\"\")\n",
    "            \n",
    "            # Edge cases\n",
    "            edge_cases = coverage.get('edge_cases', [])\n",
    "            if edge_cases:\n",
    "                md_content.append(\"### Edge Cases\")\n",
    "                for case in edge_cases:\n",
    "                    md_content.append(f\"- {case}\")\n",
    "                md_content.append(\"\")\n",
    "            \n",
    "            # Not covered\n",
    "            not_covered = coverage.get('not_covered', [])\n",
    "            if not_covered:\n",
    "                md_content.append(\"### Areas Not Covered\")\n",
    "                for area in not_covered:\n",
    "                    md_content.append(f\"- {area}\")\n",
    "                md_content.append(\"\")\n",
    "        \n",
    "        # Add test scenarios\n",
    "        md_content.append(\"## Test Scenarios\")\n",
    "        md_content.append(\"\")\n",
    "        \n",
    "        for ts in test_scenarios.get('test_scenarios', []):\n",
    "            md_content.append(f\"### {ts.get('id', 'TS-XXX')}: {ts.get('name', 'Unnamed Test Scenario')}\")\n",
    "            md_content.append(\"\")\n",
    "            \n",
    "            md_content.append(f\"**Category:** {ts.get('category', 'Unknown')}\")\n",
    "            md_content.append(\"\")\n",
    "            \n",
    "            md_content.append(\"**Description:**\")\n",
    "            md_content.append(ts.get('description', 'No description provided.'))\n",
    "            md_content.append(\"\")\n",
    "            \n",
    "            md_content.append(\"**Test Objective:**\")\n",
    "            md_content.append(ts.get('test_objective', 'No objective provided.'))\n",
    "            md_content.append(\"\")\n",
    "            \n",
    "            md_content.append(\"**Expected Outcome:**\")\n",
    "            md_content.append(ts.get('expected_outcome', 'No expected outcome provided.'))\n",
    "            md_content.append(\"\")\n",
    "            \n",
    "            # Relevant requirements\n",
    "            relevant_requirements = ts.get('relevant_requirements', '')\n",
    "            if relevant_requirements:\n",
    "                md_content.append(f\"**Relevant Requirements:** {relevant_requirements}\")\n",
    "                md_content.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(md_content)\n",
    "\n",
    "def generate_test_scenarios(descriptions, user_stories, tech_specs, test_scenarios_dir):\n",
    "    \"\"\"\n",
    "    Generate test scenarios for each module\n",
    "    \n",
    "    Args:\n",
    "        descriptions: Dict mapping user story IDs to their code descriptions\n",
    "        user_stories: Dict mapping user story IDs to user story texts\n",
    "        tech_specs: Dict mapping user story IDs to tech spec texts\n",
    "        test_scenarios_dir: Directory to save test scenarios\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping user story IDs to their test scenarios\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI model\n",
    "    check_openai_config()\n",
    "    \n",
    "    model = AzureChatOpenAI(\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize test scenario generator\n",
    "    test_gen = TestScenarioGenerator(\n",
    "        model=model,\n",
    "        output_dir=test_scenarios_dir,\n",
    "        system_generator=test_scenario_generator_prompt\n",
    "    )\n",
    "    \n",
    "    test_scenarios = {}\n",
    "    \n",
    "    # Process each module\n",
    "    for user_story_id, description in descriptions.items():\n",
    "        # Get user story and tech spec if available\n",
    "        user_story = user_stories.get(user_story_id, f\"User story for {user_story_id} not available\")\n",
    "        tech_spec = tech_specs.get(user_story_id, f\"Technical specification for {user_story_id} not available\")\n",
    "        \n",
    "        logger.info(f\"Generating test scenarios for {user_story_id}\")\n",
    "        \n",
    "        # Setup initial state\n",
    "        initial_state = {\n",
    "            \"messages\": [],\n",
    "            \"current_test_scenarios\": {},\n",
    "            \"user_story_id\": user_story_id,\n",
    "            \"code_description\": description,\n",
    "            \"tech_spec\": tech_spec,\n",
    "            \"user_story\": user_story\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Run the graph\n",
    "            result = test_gen.graph.invoke(initial_state)\n",
    "            \n",
    "            # Store test scenarios\n",
    "            if 'current_test_scenarios' in result and result['current_test_scenarios']:\n",
    "                test_scenarios[user_story_id] = result['current_test_scenarios']\n",
    "                logger.info(f\"Successfully generated test scenarios for {user_story_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating test scenarios for {user_story_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return test_scenarios\n",
    "\n",
    "def create_summary_report(output_dir, descriptions, test_scenarios):\n",
    "    \"\"\"Create a summary report of all code descriptions and test scenarios.\"\"\"\n",
    "    report = {\n",
    "        \"generation_timestamp\": datetime.now().isoformat(),\n",
    "        \"modules_analyzed\": len(descriptions),\n",
    "        \"test_scenarios_generated\": sum(len(ts.get('test_scenarios', [])) for ts in test_scenarios.values()),\n",
    "        \"modules\": [\n",
    "            {\n",
    "                \"id\": module_id,\n",
    "                \"purpose\": desc.get('overall_purpose', 'No purpose provided'),\n",
    "                \"scenario_count\": len(test_scenarios.get(module_id, {}).get('test_scenarios', [])),\n",
    "                \"scenario_categories\": list(set(ts.get('category', 'unknown') \n",
    "                                           for ts in test_scenarios.get(module_id, {}).get('test_scenarios', [])))\n",
    "            }\n",
    "            for module_id, desc in descriptions.items()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save as JSON\n",
    "    report_path = os.path.join(output_dir, \"summary_report.json\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Save as Markdown\n",
    "    md_report_path = os.path.join(output_dir, \"summary_report.md\")\n",
    "    \n",
    "    md_content = [\n",
    "        \"# Test Scenario Generation Summary Report\",\n",
    "        \"\",\n",
    "        f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        f\"**Modules Analyzed:** {report['modules_analyzed']}\",\n",
    "        f\"**Total Test Scenarios:** {report['test_scenarios_generated']}\",\n",
    "        \"\",\n",
    "        \"## Module Summary\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    # Add table header\n",
    "    md_content.append(\"| Module ID | Purpose | Scenario Count | Scenario Categories |\")\n",
    "    md_content.append(\"| --- | --- | --- | --- |\")\n",
    "    \n",
    "    # Add table rows\n",
    "    for module in report['modules']:\n",
    "        purpose = module['purpose']\n",
    "        if len(purpose) > 50:\n",
    "            purpose = purpose[:47] + \"...\"\n",
    "        \n",
    "        categories = \", \".join(module['scenario_categories']) if module['scenario_categories'] else \"None\"\n",
    "        \n",
    "        md_content.append(f\"| {module['id']} | {purpose} | {module['scenario_count']} | {categories} |\")\n",
    "    \n",
    "    with open(md_report_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(md_content))\n",
    "    \n",
    "    logger.info(f\"Created summary report at {report_path} and {md_report_path}\")\n",
    "\n",
    "#############################################################\n",
    "# Main Workflow\n",
    "#############################################################\n",
    "\n",
    "def main(excel_file_path=\"tech.xlsx\"):\n",
    "    \"\"\"\n",
    "    Main workflow function that runs the test scenario generation process\n",
    "    \n",
    "    Args:\n",
    "        excel_file_path: Path to Excel file with user stories and tech specs\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the output directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Configure OpenAI\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Step 2: Find the latest integrated solution folder\n",
    "        solution_folder = find_latest_integrated_solution_folder()\n",
    "        logger.info(f\"Found latest integrated solution folder: {solution_folder}\")\n",
    "        \n",
    "        # Step 3: Find all US_*_code.py files\n",
    "        code_files = find_code_files(solution_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found\")\n",
    "            return None\n",
    "        \n",
    "        # Step 4: Create output directories\n",
    "        output_dir, descriptions_dir, test_scenarios_dir = create_output_directory()\n",
    "        logger.info(f\"Created output directories: {output_dir}\")\n",
    "        \n",
    "        # Step 5: Process code files to generate descriptions\n",
    "        descriptions = process_code_files(code_files, descriptions_dir)\n",
    "        logger.info(f\"Generated {len(descriptions)} code descriptions\")\n",
    "        \n",
    "        # Step 6: Read user stories and tech specs from Excel\n",
    "        user_stories, tech_specs = read_tech_specs_and_user_stories(excel_file_path)\n",
    "        logger.info(f\"Read {len(user_stories)} user stories and tech specs from Excel\")\n",
    "        \n",
    "        # Step 7: Generate test scenarios\n",
    "        test_scenarios = generate_test_scenarios(descriptions, user_stories, tech_specs, test_scenarios_dir)\n",
    "        logger.info(f\"Generated test scenarios for {len(test_scenarios)} modules\")\n",
    "        \n",
    "        # Step 8: Create summary report\n",
    "        create_summary_report(output_dir, descriptions, test_scenarios)\n",
    "        \n",
    "        logger.info(f\"Test scenario generation completed successfully. Output directory: {output_dir}\")\n",
    "        print(f\"Test scenario generation completed successfully. Output directory: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the test scenario generation process\n",
    "    output_dir = main(\"tech.xlsx\")\n",
    "    \n",
    "    if output_dir:\n",
    "        print(f\"\\nWorkflow completed successfully!\")\n",
    "        print(f\"Test scenarios and code descriptions saved to: {output_dir}\")\n",
    "    else:\n",
    "        print(\"\\nWorkflow completed with errors. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
