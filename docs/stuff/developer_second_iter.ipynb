{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: str\n",
    "    validation_status: bool\n",
    "    error_messages: list[str]\n",
    "    is_valid: bool  # Using is_valid to match the notebook's conditional edge structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "developer_prompt = \"\"\"\n",
    "Role: Python Developer\n",
    "Task: Generate complete, production-ready Python code based on the requirements specification.\n",
    "\n",
    "Requirements:\n",
    "{requirements}\n",
    "\n",
    "Your code must include:\n",
    "1. All necessary imports and dependencies\n",
    "2. Complete implementation with:\n",
    "   - Well-structured classes and functions\n",
    "   - Configuration management (using dataclasses or similar)\n",
    "   - Comprehensive error handling and validation\n",
    "   - Type hints throughout\n",
    "   - Logging with appropriate levels\n",
    "   - Unit tests where applicable\n",
    "3. Clear documentation:\n",
    "   - Module docstrings\n",
    "   - Function/method docstrings with parameters and return values\n",
    "   - Inline comments for complex logic\n",
    "\n",
    "Focus on implementing EVERY aspect mentioned in the requirements. Do not leave any required functionality unimplemented.\n",
    "\n",
    "## Output Format\n",
    "Your response should be the complete, production-ready Python code without surrounding explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "\n",
    "# Your Python code here\n",
    "\"\"\"\n",
    "\n",
    "validator_prompt = \"\"\"\n",
    "Role: Senior Code Reviewer\n",
    "Task: Perform a thorough validation of the provided Python code against the requirements.\n",
    "\n",
    "Requirements:\n",
    "{Requirements}\n",
    "\n",
    "Validation Process:\n",
    "1. Carefully compare the code against EACH requirement in the specification\n",
    "2. For each requirement, determine if it has been fully, partially, or not implemented\n",
    "3. Identify any missing functionality, edge cases, or requirements\n",
    "4. Evaluate code quality, error handling, security, and performance\n",
    "\n",
    "Validation Checklist:\n",
    "1. Code Completeness:\n",
    "   - All imports and dependencies present\n",
    "   - Full implementation of required functionality (check EACH requirement)\n",
    "   - No placeholder code or TODOs\n",
    "\n",
    "2. Code Quality:\n",
    "   - Follows PEP 8 standards\n",
    "   - Clear variable/function naming\n",
    "   - Appropriate modularization\n",
    "   - Avoids code duplication\n",
    "   - Maintainable architecture\n",
    "\n",
    "3. Technical Implementation:\n",
    "   - Proper error handling with specific exceptions\n",
    "   - Complete type annotations\n",
    "   - Correct algorithm implementation\n",
    "   - Efficient resource usage\n",
    "   - Security considerations addressed\n",
    "\n",
    "4. Documentation:\n",
    "   - Comprehensive docstrings\n",
    "   - Clear inline comments where needed\n",
    "\n",
    "## Output Format\n",
    "Return your validation report as a structured JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"validation_report\": {{\n",
    "    \"overall_assessment\": \"Pass/Fail\",\n",
    "    \"issues_found\": [\n",
    "      \"Issue 1 description\",\n",
    "      \"Issue 2 description\",\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"suggested_improvements\": [\n",
    "      {{\n",
    "        \"description\": \"Improvement 1\",\n",
    "        \"priority\": \"high/medium/low\"\n",
    "      }},\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"implementation_vs_requirements\": {{\n",
    "      \"match\": true/false,\n",
    "      \"details\": [\n",
    "        {{\n",
    "          \"requirement_section\": \"Requirement name/section\",\n",
    "          \"status\": \"Implemented/Partially Implemented/Not Implemented\",\n",
    "          \"notes\": \"Notes about implementation\"\n",
    "        }},\n",
    "        \"...\"\n",
    "      ]\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Be strict in your assessment. If ANY requirement is not fully implemented, the overall assessment should be \"Fail\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corrector_prompt = \"\"\"\n",
    "Role: Senior Python Developer\n",
    "Task: Refactor and fix the code based on the validation feedback.\n",
    "Original Requirements:\n",
    "{requirements}\n",
    "Validation Feedback:\n",
    "{ValidationFeedback}\n",
    "Correction Instructions:\n",
    "\n",
    "Address ALL issues identified in the validation feedback\n",
    "Pay particular attention to any requirements marked as \"Not Implemented\" or \"Partially Implemented\"\n",
    "Maintain the original architectural approach unless fundamentally flawed\n",
    "Ensure complete implementation of ALL requirements from the original specification\n",
    "Add or improve:\n",
    "\n",
    "Error handling for all edge cases\n",
    "Type hints throughout the codebase\n",
    "Documentation (docstrings and comments)\n",
    "Logging for important operations\n",
    "Performance optimizations where possible\n",
    "\n",
    "\n",
    "\n",
    "Important: Make sure you implement EVERY feature mentioned in the requirements that was flagged as missing or incomplete in the validation feedback.\n",
    "Output Format\n",
    "Your response should be the complete, corrected, production-ready Python code without explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "Your corrected Python code here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model, checkpointer, system_developer=\"\", system_validator=\"\", system_corrector=\"\"):\n",
    "        self.system_developer = system_developer\n",
    "        self.system_validator = system_validator\n",
    "        self.system_corrector = system_corrector\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = f\"code_generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        \n",
    "        # Add conditional edges (matching notebook pattern)\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\", \n",
    "            lambda state: state[\"is_valid\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        graph.add_edge(\"correction\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.model = model\n",
    "        \n",
    "        # Try to display graph visualization if possible\n",
    "        try:\n",
    "            display(Image(self.graph.get_graph().draw_mermaid_png()))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        \"\"\"Extract code from between triple backticks or triple single quotes\"\"\"\n",
    "        pattern = r\"```(?:python)?\\\\s*(.*?)```\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        # Try with triple single quotes\n",
    "        pattern = r\"'''(?:python)?\\\\s*(.*?)'''\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        return text  # Return original if no code blocks found\n",
    "\n",
    "    def save_code_attempt(self, code: str, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt and return directory path\"\"\"\n",
    "        attempt_dir = os.path.join(self.output_dir, f\"attempt_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        # Save code\n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved code attempt to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def developer(self, state: CodeGenerationState):\n",
    "        \"\"\"Generate initial code\"\"\"\n",
    "        messages = state['messages']\n",
    "        print(messages)\n",
    "        print(\"developer\", \"*\" * 50)\n",
    "        \n",
    "        if self.system_developer:\n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_developer.format(\n",
    "                requirements=messages[0].content,  # Changed from Requirements to requirements\n",
    "                TechnicalSpecifications=messages[0].content\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        \n",
    "        # Extract code from response\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save code\n",
    "        self.save_code_attempt(code_only)\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'validation_status': None,\n",
    "            'error_messages': [],\n",
    "            'is_valid': False\n",
    "        }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState):\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        messages = state.get('messages', [])\n",
    "        current_code = state.get('current_code', '')\n",
    "        \n",
    "        print(messages)\n",
    "        print(\"validate\", \"*\" * 50)\n",
    "        \n",
    "        if self.system_validator:\n",
    "            original_message = state[\"messages\"][0].content if state[\"messages\"] else \"\"\n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_validator.format(\n",
    "                Requirements=original_message,\n",
    "                TechnicalSpecifications=original_message\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\").lower()\n",
    "        \n",
    "        # Attempt to determine if validation passed by extracting JSON\n",
    "        is_valid = False\n",
    "        try:\n",
    "            # Try to extract JSON from the message\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                is_valid = (validation_json.get(\"validation_report\", {}).get(\"overall_assessment\", \"\").lower() == \"pass\")\n",
    "        except:\n",
    "            # Fallback to the original logic if JSON extraction fails\n",
    "            is_valid = \"pass\" in response_text and \"correctly implements\" in response_text\n",
    "        \n",
    "        # Save validation results to JSON if possible\n",
    "        try:\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                json_path = os.path.join(self.output_dir, \"validation_results.json\")\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(validation_json, f, indent=2)\n",
    "                logger.info(f\"Saved validation results to {json_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save validation results: {e}\")\n",
    "        \n",
    "        if is_valid:\n",
    "            self.save_code_attempt(current_code, \"validated_pass\")\n",
    "        else:\n",
    "            self.save_code_attempt(current_code, \"validated_fail\")\n",
    "            \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': current_code,\n",
    "            'is_valid': is_valid,\n",
    "            'error_messages': [] if is_valid else [\"Validation failed\"]\n",
    "        }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState):\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        messages = state['messages']\n",
    "        \n",
    "        print(messages)\n",
    "        print(\"correction\", \"*\" * 50)\n",
    "        \n",
    "        if self.system_corrector:\n",
    "            # Get original requirements from the first human message in the chain\n",
    "            original_requirements = \"\"\n",
    "            for msg in state['messages']:\n",
    "                if isinstance(msg, HumanMessage) and msg.content:\n",
    "                    original_requirements = msg.content\n",
    "                    break\n",
    "            \n",
    "            # Get validation feedback from the most recent message\n",
    "            validation_feedback = messages[0].content if messages else \"\"\n",
    "            \n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_corrector.format(\n",
    "                requirements=original_requirements,  # Changed from Requirements to requirements\n",
    "                ValidationFeedback=validation_feedback\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save corrected code\n",
    "        self.save_code_attempt(code_only, \"correction\")\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'is_valid': False,\n",
    "            'error_messages': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='\\n## 1. **Overview**  \\nThis technical specification provides a detailed plan for implementing a feature that allows users to upload training datasets from local storage to a cloud training bucket, validate the datasets for integrity and structure, and provide real-time feedback. It addresses all functional requirements, edge cases, and non-functional requirements.\\n \\n---\\n \\n## 2. **System Architecture**  \\n \\n### 2.1 **High-Level Architecture**  \\nThe system consists of the following components:  \\n1. **Frontend Module**: Provides the user interface for file selection, dataset type input, and real-time feedback display.  \\n2. **Backend API**: Handles dataset validation, file extraction, cloud upload, and feedback delivery.  \\n3. **Cloud Storage Module**: Stores validated datasets securely.  \\n4. **Validation Module**: Performs structural, file integrity, and format validation.  \\n \\n---\\n \\n### 2.2 **Component Breakdown**  \\n \\n#### **Frontend Module**  \\n- **Responsibilities**:  \\n  - Allow users to select a zip file and dataset type.  \\n  - Provide real-time progress updates for file upload and validation processes.  \\n  - Display success, warning, or error messages.  \\n- **Technologies**:  \\n  - React.js or Angular for UI.  \\n  - WebSocket or Server-Sent Events (SSE) for real-time feedback.  \\n  - Axios or Fetch API for communication with the backend.  \\n \\n#### **Backend API**  \\n- **Responsibilities**:  \\n  - Handle dataset upload and validation requests.  \\n  - Manage asynchronous tasks for large file uploads and validations.  \\n  - Communicate real-time progress and feedback to the frontend.  \\n- **Technologies**:  \\n  - Python (FastAPI or Flask) for API development.  \\n  - Celery with Redis for task management.  \\n \\n#### **Cloud Storage Module**  \\n- **Responsibilities**:  \\n  - Store datasets securely in a scalable cloud storage bucket.  \\n  - Use pre-signed URLs for efficient and secure file uploads.  \\n- **Technologies**:  \\n  - AWS S3, Azure Blob Storage, or Google Cloud Storage.  \\n  - Encryption protocols (e.g., HTTPS, TLS).  \\n \\n#### **Validation Module**  \\n- **Responsibilities**:  \\n  - Ensure datasets meet structural and format requirements based on the selected dataset type.  \\n  - Validate file integrity and detect corrupted files.  \\n- **Technologies**:  \\n  - Python libraries: `os`, `zipfile`, `Pillow`, and `pyzipper`.  \\n \\n---\\n \\n## 3. **Detailed Design**  \\n \\n### 3.1 **Data Structures**  \\n \\n#### **Frontend State Management**  \\n```javascript\\n{\\n  file: File,  // File object selected by the user\\n  datasetType: \"classification\" | \"segmentation\",  // Dataset type selected by user\\n  progress: number,  // Progress percentage (0-100)\\n  feedback: {        // Real-time feedback messages\\n    status: \"success\" | \"warning\" | \"error\",\\n    message: string\\n  }\\n}\\n```\\n \\n#### **Backend Request Body**  \\n```json\\n{\\n  \"filePath\": \"/path/to/zipfile.zip\",\\n  \"datasetType\": \"classification\"\\n}\\n```\\n \\n#### **Validation Module Output**  \\n```python\\n{\\n  \"is_valid\": bool,        # Whether the dataset passed all validations\\n  \"errors\": list[str],     # List of validation errors\\n  \"warnings\": list[str],   # List of validation warnings\\n  \"progress\": int          # Validation progress percentage\\n}\\n```\\n \\n---\\n \\n### 3.2 **API Endpoints**  \\n \\n#### **1. Upload Dataset**  \\n- **URL**: `/api/dataset/upload`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset (\"classification\" or \"segmentation\").  \\n- **Response**:  \\n  ```json\\n  {\\n    \"status\": \"success\" | \"error\",\\n    \"message\": \"Feedback message describing the operation result\"\\n  }\\n  ```\\n \\n#### **2. Validate Dataset**  \\n- **URL**: `/api/dataset/validate`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset.  \\n- **Response**:  \\n  ```json\\n  {\\n    \"is_valid\": true,\\n    \"errors\": [],\\n    \"warnings\": [\"Empty sub-folder in class \\'dogs\\'.\"]\\n  }\\n  ```\\n \\n#### **3. Task Progress**  \\n- **URL**: `/api/task/progress/{task_id}`  \\n- **Method**: GET  \\n- **Response**:  \\n  ```json\\n  {\\n    \"task_id\": \"unique_task_id\",\\n    \"progress\": 45,  // Current progress percentage\\n    \"status\": \"in_progress\",  // Task status: \"in_progress\", \"completed\", or \"failed\"\\n    \"message\": \"Validation in progress\"\\n  }\\n  ```\\n \\n---\\n \\n### 3.3 **Validation Logic**  \\n \\n#### **Classification Dataset Validation**  \\n1. Ensure the zip file contains sub-folders for each class.  \\n2. Verify that each sub-folder contains at least one image file.  \\n3. Log warnings for empty sub-folders.  \\n \\n#### **Segmentation Dataset Validation**  \\n1. Check for \"images\" and \"mask\" folders.  \\n2. Verify that each image in the \"images\" folder has a corresponding mask in the \"mask\" folder.  \\n3. Reject the dataset if the \"mask\" folder is empty or masks are missing.  \\n \\n#### **File Integrity Validation**  \\n1. Attempt to open each image file using `Pillow`.  \\n2. Reject the dataset if corrupted files are found.  \\n \\n#### **Image Format Validation**  \\n1. Accept only `.jpg`, `.png`, and `.bmp` formats.  \\n2. Reject datasets with unsupported or mixed formats.  \\n \\n---\\n \\n### 3.4 **Chunked File Upload Implementation**  \\n1. Split files exceeding 500MB into chunks.  \\n2. Upload each chunk sequentially to cloud storage using pre-signed URLs.  \\n3. Reassemble the file on the backend after all chunks are uploaded.  \\n \\n---\\n \\n## 4. **Performance, Security, and Scalability**  \\n \\n### 4.1 **Performance**  \\n- Use asynchronous tasks for handling large files and validations.  \\n- Provide real-time feedback using WebSockets or SSE.  \\n- Limit dataset size to 10GB and use chunked uploads for files larger than 500MB.  \\n \\n### 4.2 **Security**  \\n- Enforce HTTPS and TLS for secure data transfer.  \\n- Reject password-protected or encrypted zip files.  \\n- Use role-based access control (RBAC) for cloud storage operations.  \\n \\n### 4.3 **Scalability**  \\n- Use scalable cloud storage (e.g., AWS S3).  \\n- Employ load balancing for backend APIs to handle high traffic.  \\n- Use Redis for task queue management to ensure fault tolerance.  \\n \\n---\\n \\n## 5. **Constraints and Dependencies**  \\n \\n### 5.1 **Constraints**  \\n1. Maximum dataset size: 10GB.  \\n2. Supported image formats: `.jpg`, `.png`, and `.bmp`.  \\n3. No support for password-protected zip files.  \\n \\n### 5.2 **Dependencies**  \\n1. Cloud storage provider (AWS S3 or equivalent).  \\n2. Python libraries for file validation (`Pillow`, `zipfile`).  \\n3. WebSocket or SSE for real-time communication.  \\n \\n---\\n \\n## 6. **Edge Case Handling**  \\n \\n### **Classification Dataset**  \\n1. Empty sub-folders: Log warnings and allow upload.  \\n2. Corrupted files: Reject the dataset.  \\n \\n### **Segmentation Dataset**  \\n1. Missing masks: Reject the dataset.  \\n2. Empty \"mask\" folder: Reject the dataset.  \\n \\n### **General Cases**  \\n1. Unsupported formats: Reject the dataset.  \\n2. Mixed formats: Reject the dataset.  \\n3. Cloud storage failure: Retry upload up to 3 times with exponential backoff.  \\n \\n---\\n \\n## 7. **Monitoring and Logging**  \\n \\n- Use Prometheus and Grafana for performance monitoring.  \\n- Log all errors, warnings, and progress updates to CloudWatch or ELK Stack.  \\n- Set up alerts for critical failures using Slack or PagerDuty.  \\n', additional_kwargs={}, response_metadata={})]\n",
      "developer **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:47:59,498 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 16:47:59,500 - __main__ - INFO - [4021668688.py:72] - Saved code attempt to code_generation_20250304_164704/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='\\n## 1. **Overview**  \\nThis technical specification provides a detailed plan for implementing a feature that allows users to upload training datasets from local storage to a cloud training bucket, validate the datasets for integrity and structure, and provide real-time feedback. It addresses all functional requirements, edge cases, and non-functional requirements.\\n \\n---\\n \\n## 2. **System Architecture**  \\n \\n### 2.1 **High-Level Architecture**  \\nThe system consists of the following components:  \\n1. **Frontend Module**: Provides the user interface for file selection, dataset type input, and real-time feedback display.  \\n2. **Backend API**: Handles dataset validation, file extraction, cloud upload, and feedback delivery.  \\n3. **Cloud Storage Module**: Stores validated datasets securely.  \\n4. **Validation Module**: Performs structural, file integrity, and format validation.  \\n \\n---\\n \\n### 2.2 **Component Breakdown**  \\n \\n#### **Frontend Module**  \\n- **Responsibilities**:  \\n  - Allow users to select a zip file and dataset type.  \\n  - Provide real-time progress updates for file upload and validation processes.  \\n  - Display success, warning, or error messages.  \\n- **Technologies**:  \\n  - React.js or Angular for UI.  \\n  - WebSocket or Server-Sent Events (SSE) for real-time feedback.  \\n  - Axios or Fetch API for communication with the backend.  \\n \\n#### **Backend API**  \\n- **Responsibilities**:  \\n  - Handle dataset upload and validation requests.  \\n  - Manage asynchronous tasks for large file uploads and validations.  \\n  - Communicate real-time progress and feedback to the frontend.  \\n- **Technologies**:  \\n  - Python (FastAPI or Flask) for API development.  \\n  - Celery with Redis for task management.  \\n \\n#### **Cloud Storage Module**  \\n- **Responsibilities**:  \\n  - Store datasets securely in a scalable cloud storage bucket.  \\n  - Use pre-signed URLs for efficient and secure file uploads.  \\n- **Technologies**:  \\n  - AWS S3, Azure Blob Storage, or Google Cloud Storage.  \\n  - Encryption protocols (e.g., HTTPS, TLS).  \\n \\n#### **Validation Module**  \\n- **Responsibilities**:  \\n  - Ensure datasets meet structural and format requirements based on the selected dataset type.  \\n  - Validate file integrity and detect corrupted files.  \\n- **Technologies**:  \\n  - Python libraries: `os`, `zipfile`, `Pillow`, and `pyzipper`.  \\n \\n---\\n \\n## 3. **Detailed Design**  \\n \\n### 3.1 **Data Structures**  \\n \\n#### **Frontend State Management**  \\n```javascript\\n{\\n  file: File,  // File object selected by the user\\n  datasetType: \"classification\" | \"segmentation\",  // Dataset type selected by user\\n  progress: number,  // Progress percentage (0-100)\\n  feedback: {        // Real-time feedback messages\\n    status: \"success\" | \"warning\" | \"error\",\\n    message: string\\n  }\\n}\\n```\\n \\n#### **Backend Request Body**  \\n```json\\n{\\n  \"filePath\": \"/path/to/zipfile.zip\",\\n  \"datasetType\": \"classification\"\\n}\\n```\\n \\n#### **Validation Module Output**  \\n```python\\n{\\n  \"is_valid\": bool,        # Whether the dataset passed all validations\\n  \"errors\": list[str],     # List of validation errors\\n  \"warnings\": list[str],   # List of validation warnings\\n  \"progress\": int          # Validation progress percentage\\n}\\n```\\n \\n---\\n \\n### 3.2 **API Endpoints**  \\n \\n#### **1. Upload Dataset**  \\n- **URL**: `/api/dataset/upload`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset (\"classification\" or \"segmentation\").  \\n- **Response**:  \\n  ```json\\n  {\\n    \"status\": \"success\" | \"error\",\\n    \"message\": \"Feedback message describing the operation result\"\\n  }\\n  ```\\n \\n#### **2. Validate Dataset**  \\n- **URL**: `/api/dataset/validate`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset.  \\n- **Response**:  \\n  ```json\\n  {\\n    \"is_valid\": true,\\n    \"errors\": [],\\n    \"warnings\": [\"Empty sub-folder in class \\'dogs\\'.\"]\\n  }\\n  ```\\n \\n#### **3. Task Progress**  \\n- **URL**: `/api/task/progress/{task_id}`  \\n- **Method**: GET  \\n- **Response**:  \\n  ```json\\n  {\\n    \"task_id\": \"unique_task_id\",\\n    \"progress\": 45,  // Current progress percentage\\n    \"status\": \"in_progress\",  // Task status: \"in_progress\", \"completed\", or \"failed\"\\n    \"message\": \"Validation in progress\"\\n  }\\n  ```\\n \\n---\\n \\n### 3.3 **Validation Logic**  \\n \\n#### **Classification Dataset Validation**  \\n1. Ensure the zip file contains sub-folders for each class.  \\n2. Verify that each sub-folder contains at least one image file.  \\n3. Log warnings for empty sub-folders.  \\n \\n#### **Segmentation Dataset Validation**  \\n1. Check for \"images\" and \"mask\" folders.  \\n2. Verify that each image in the \"images\" folder has a corresponding mask in the \"mask\" folder.  \\n3. Reject the dataset if the \"mask\" folder is empty or masks are missing.  \\n \\n#### **File Integrity Validation**  \\n1. Attempt to open each image file using `Pillow`.  \\n2. Reject the dataset if corrupted files are found.  \\n \\n#### **Image Format Validation**  \\n1. Accept only `.jpg`, `.png`, and `.bmp` formats.  \\n2. Reject datasets with unsupported or mixed formats.  \\n \\n---\\n \\n### 3.4 **Chunked File Upload Implementation**  \\n1. Split files exceeding 500MB into chunks.  \\n2. Upload each chunk sequentially to cloud storage using pre-signed URLs.  \\n3. Reassemble the file on the backend after all chunks are uploaded.  \\n \\n---\\n \\n## 4. **Performance, Security, and Scalability**  \\n \\n### 4.1 **Performance**  \\n- Use asynchronous tasks for handling large files and validations.  \\n- Provide real-time feedback using WebSockets or SSE.  \\n- Limit dataset size to 10GB and use chunked uploads for files larger than 500MB.  \\n \\n### 4.2 **Security**  \\n- Enforce HTTPS and TLS for secure data transfer.  \\n- Reject password-protected or encrypted zip files.  \\n- Use role-based access control (RBAC) for cloud storage operations.  \\n \\n### 4.3 **Scalability**  \\n- Use scalable cloud storage (e.g., AWS S3).  \\n- Employ load balancing for backend APIs to handle high traffic.  \\n- Use Redis for task queue management to ensure fault tolerance.  \\n \\n---\\n \\n## 5. **Constraints and Dependencies**  \\n \\n### 5.1 **Constraints**  \\n1. Maximum dataset size: 10GB.  \\n2. Supported image formats: `.jpg`, `.png`, and `.bmp`.  \\n3. No support for password-protected zip files.  \\n \\n### 5.2 **Dependencies**  \\n1. Cloud storage provider (AWS S3 or equivalent).  \\n2. Python libraries for file validation (`Pillow`, `zipfile`).  \\n3. WebSocket or SSE for real-time communication.  \\n \\n---\\n \\n## 6. **Edge Case Handling**  \\n \\n### **Classification Dataset**  \\n1. Empty sub-folders: Log warnings and allow upload.  \\n2. Corrupted files: Reject the dataset.  \\n \\n### **Segmentation Dataset**  \\n1. Missing masks: Reject the dataset.  \\n2. Empty \"mask\" folder: Reject the dataset.  \\n \\n### **General Cases**  \\n1. Unsupported formats: Reject the dataset.  \\n2. Mixed formats: Reject the dataset.  \\n3. Cloud storage failure: Retry upload up to 3 times with exponential backoff.  \\n \\n---\\n \\n## 7. **Monitoring and Logging**  \\n \\n- Use Prometheus and Grafana for performance monitoring.  \\n- Log all errors, warnings, and progress updates to CloudWatch or ELK Stack.  \\n- Set up alerts for critical failures using Slack or PagerDuty.  \\n', additional_kwargs={}, response_metadata={}), AIMessage(content='# Production-ready Python code for the backend implementation of the dataset upload, validation, and feedback system.\\n\\n```python\\nimport os\\nimport zipfile\\nimport logging\\nimport uuid\\nfrom typing import Dict, List\\nfrom fastapi import FastAPI, UploadFile, Form, HTTPException, BackgroundTasks\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nimport boto3\\nfrom botocore.exceptions import NoCredentialsError, PartialCredentialsError\\nfrom redis import Redis\\nfrom celery import Celery\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\\nlogger = logging.getLogger(__name__)\\n\\n# FastAPI application\\napp = FastAPI()\\n\\n# Celery configuration\\nredis_url = \"redis://localhost:6379/0\"\\ncelery_app = Celery(\"tasks\", broker=redis_url, backend=redis_url)\\n\\n# AWS S3 Configuration\\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\\nAWS_BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")\\n\\n# Dataset validation configuration\\nMAX_DATASET_SIZE_MB = 10240  # 10 GB\\nSUPPORTED_IMAGE_FORMATS = {\".jpg\", \".png\", \".bmp\"}\\n\\n\\nclass ValidationResult(BaseModel):\\n    is_valid: bool\\n    errors: List[str]\\n    warnings: List[str]\\n    progress: int\\n\\n\\nclass ProgressResponse(BaseModel):\\n    task_id: str\\n    progress: int\\n    status: str\\n    message: str\\n\\n\\n@app.post(\"/api/dataset/upload\")\\nasync def upload_dataset(\\n    file: UploadFile, dataset_type: str = Form(...), background_tasks: BackgroundTasks\\n):\\n    \"\"\"\\n    Upload a dataset to the server and validate it.\\n    Parameters:\\n        file (UploadFile): The uploaded zip file containing the dataset.\\n        dataset_type (str): Type of dataset (\"classification\" or \"segmentation\").\\n        background_tasks (BackgroundTasks): FastAPI background task manager.\\n    Returns:\\n        JSON response indicating upload and validation success or failure.\\n    \"\"\"\\n    try:\\n        if file.content_type != \"application/zip\":\\n            raise HTTPException(status_code=400, detail=\"Only zip files are supported.\")\\n\\n        # Save the uploaded file temporarily\\n        temp_file_path = f\"/tmp/{uuid.uuid4()}_{file.filename}\"\\n        with open(temp_file_path, \"wb\") as temp_file:\\n            temp_file.write(await file.read())\\n\\n        # Enqueue validation task\\n        task_id = str(uuid.uuid4())\\n        background_tasks.add_task(validate_dataset, temp_file_path, dataset_type, task_id)\\n\\n        return {\"status\": \"success\", \"message\": \"File uploaded successfully.\", \"task_id\": task_id}\\n\\n    except Exception as e:\\n        logger.error(f\"Error uploading dataset: {e}\")\\n        raise HTTPException(status_code=500, detail=\"An error occurred during upload.\")\\n\\n\\n@app.get(\"/api/task/progress/{task_id}\", response_model=ProgressResponse)\\ndef get_task_progress(task_id: str):\\n    \"\"\"\\n    Retrieve progress of an asynchronous task.\\n    Parameters:\\n        task_id (str): Unique identifier of the task.\\n    Returns:\\n        Task progress details.\\n    \"\"\"\\n    task_result = celery_app.AsyncResult(task_id)\\n    if not task_result:\\n        raise HTTPException(status_code=404, detail=\"Task not found.\")\\n\\n    return {\\n        \"task_id\": task_id,\\n        \"progress\": task_result.info.get(\"progress\", 0) if task_result.info else 0,\\n        \"status\": task_result.status,\\n        \"message\": task_result.info.get(\"message\", \"No updates available\") if task_result.info else \"\",\\n    }\\n\\n\\n@celery_app.task(bind=True)\\ndef validate_dataset(self, file_path: str, dataset_type: str, task_id: str):\\n    \"\"\"\\n    Validate a dataset according to the specified type.\\n    Parameters:\\n        file_path (str): Path to the zip file.\\n        dataset_type (str): Type of dataset (classification or segmentation).\\n        task_id (str): Unique identifier for the task.\\n    \"\"\"\\n    try:\\n        self.update_state(state=\"STARTED\", meta={\"progress\": 0, \"message\": \"Validation started.\"})\\n\\n        # Check file size limit\\n        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\\n        if file_size_mb > MAX_DATASET_SIZE_MB:\\n            raise ValueError(\"Dataset exceeds maximum allowed size of 10 GB.\")\\n\\n        # Extract zip file\\n        extraction_dir = f\"/tmp/{uuid.uuid4()}\"\\n        os.makedirs(extraction_dir, exist_ok=True)\\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\\n            zip_ref.extractall(extraction_dir)\\n\\n        # Perform validation based on dataset type\\n        errors, warnings = [], []\\n        if dataset_type == \"classification\":\\n            validate_classification_dataset(extraction_dir, errors, warnings)\\n        elif dataset_type == \"segmentation\":\\n            validate_segmentation_dataset(extraction_dir, errors, warnings)\\n        else:\\n            raise ValueError(\"Unsupported dataset type.\")\\n\\n        # Update progress\\n        self.update_state(state=\"PROGRESS\", meta={\"progress\": 80, \"message\": \"Validation nearing completion.\"})\\n\\n        # Upload to cloud storage if valid\\n        if not errors:\\n            upload_to_cloud_storage(extraction_dir)\\n\\n        # Cleanup\\n        os.remove(file_path)\\n        os.rmdir(extraction_dir)\\n\\n        self.update_state(state=\"SUCCESS\", meta={\"progress\": 100, \"message\": \"Validation complete.\"})\\n        return {\"is_valid\": not errors, \"errors\": errors, \"warnings\": warnings}\\n\\n    except Exception as e:\\n        logger.error(f\"Validation failed: {e}\")\\n        self.update_state(state=\"FAILURE\", meta={\"progress\": 100, \"message\": str(e)})\\n\\n\\ndef validate_classification_dataset(directory: str, errors: List[str], warnings: List[str]):\\n    \"\"\"\\n    Validate a classification dataset structure and format.\\n    \"\"\"\\n    subfolders = [f.path for f in os.scandir(directory) if f.is_dir()]\\n    if not subfolders:\\n        errors.append(\"No sub-folders found for classification dataset.\")\\n\\n    for subfolder in subfolders:\\n        image_files = [\\n            f for f in os.listdir(subfolder) if os.path.isfile(os.path.join(subfolder, f))\\n        ]\\n        if not image_files:\\n            warnings.append(f\"Empty sub-folder: {subfolder}\")\\n        else:\\n            for image_file in image_files:\\n                validate_image(os.path.join(subfolder, image_file), errors)\\n\\n\\ndef validate_segmentation_dataset(directory: str, errors: List[str], warnings: List[str]):\\n    \"\"\"\\n    Validate a segmentation dataset structure and format.\\n    \"\"\"\\n    images_dir = os.path.join(directory, \"images\")\\n    masks_dir = os.path.join(directory, \"masks\")\\n\\n    if not os.path.isdir(images_dir) or not os.path.isdir(masks_dir):\\n        errors.append(\"Missing \\'images\\' or \\'masks\\' folder in segmentation dataset.\")\\n        return\\n\\n    image_files = set(os.listdir(images_dir))\\n    mask_files = set(os.listdir(masks_dir))\\n\\n    if not image_files:\\n        errors.append(\"No images found in \\'images\\' folder.\")\\n    if not mask_files:\\n        errors.append(\"No masks found in \\'masks\\' folder.\")\\n\\n    for image_file in image_files:\\n        if image_file not in mask_files:\\n            errors.append(f\"Missing mask for image: {image_file}\")\\n\\n        validate_image(os.path.join(images_dir, image_file), errors)\\n        validate_image(os.path.join(masks_dir, image_file), errors)\\n\\n\\ndef validate_image(file_path: str, errors: List[str]):\\n    \"\"\"\\n    Validate individual image for format and integrity.\\n    \"\"\"\\n    try:\\n        _, ext = os.path.splitext(file_path)\\n        if ext.lower() not in SUPPORTED_IMAGE_FORMATS:\\n            errors.append(f\"Unsupported image format for file: {file_path}\")\\n\\n        # Check file integrity\\n        with Image.open(file_path) as img:\\n            img.verify()\\n    except Exception:\\n        errors.append(f\"Corrupted or unreadable image file: {file_path}\")\\n\\n\\ndef upload_to_cloud_storage(directory: str):\\n    \"\"\"\\n    Upload validated dataset to AWS S3.\\n    \"\"\"\\n    try:\\n        s3_client = boto3.client(\\n            \"s3\",\\n            aws_access_key_id=AWS_ACCESS_KEY_ID,\\n            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n        )\\n        for root, _, files in os.walk(directory):\\n            for file in files:\\n                file_path = os.path.join(root, file)\\n                s3_client.upload_file(file_path, AWS_BUCKET_NAME, os.path.relpath(file_path, directory))\\n    except (NoCredentialsError, PartialCredentialsError) as e:\\n        logger.error(f\"Cloud storage upload failed: {e}\")\\n        raise\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1824, 'prompt_tokens': 3799, 'total_tokens': 5623, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_a42ed5ff0c', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-2ca61fb5-f3eb-460e-90a8-277edd66a649-0', usage_metadata={'input_tokens': 3799, 'output_tokens': 1824, 'total_tokens': 5623, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "validate **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:47:59,962 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-04 16:47:59,962 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 6.000000 seconds\n",
      "2025-03-04 16:48:16,078 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 16:48:16,080 - __main__ - INFO - [4021668688.py:148] - Saved validation results to code_generation_20250304_164704/validation_results.json\n",
      "2025-03-04 16:48:16,081 - __main__ - INFO - [4021668688.py:72] - Saved code attempt to code_generation_20250304_164704/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='\\n## 1. **Overview**  \\nThis technical specification provides a detailed plan for implementing a feature that allows users to upload training datasets from local storage to a cloud training bucket, validate the datasets for integrity and structure, and provide real-time feedback. It addresses all functional requirements, edge cases, and non-functional requirements.\\n \\n---\\n \\n## 2. **System Architecture**  \\n \\n### 2.1 **High-Level Architecture**  \\nThe system consists of the following components:  \\n1. **Frontend Module**: Provides the user interface for file selection, dataset type input, and real-time feedback display.  \\n2. **Backend API**: Handles dataset validation, file extraction, cloud upload, and feedback delivery.  \\n3. **Cloud Storage Module**: Stores validated datasets securely.  \\n4. **Validation Module**: Performs structural, file integrity, and format validation.  \\n \\n---\\n \\n### 2.2 **Component Breakdown**  \\n \\n#### **Frontend Module**  \\n- **Responsibilities**:  \\n  - Allow users to select a zip file and dataset type.  \\n  - Provide real-time progress updates for file upload and validation processes.  \\n  - Display success, warning, or error messages.  \\n- **Technologies**:  \\n  - React.js or Angular for UI.  \\n  - WebSocket or Server-Sent Events (SSE) for real-time feedback.  \\n  - Axios or Fetch API for communication with the backend.  \\n \\n#### **Backend API**  \\n- **Responsibilities**:  \\n  - Handle dataset upload and validation requests.  \\n  - Manage asynchronous tasks for large file uploads and validations.  \\n  - Communicate real-time progress and feedback to the frontend.  \\n- **Technologies**:  \\n  - Python (FastAPI or Flask) for API development.  \\n  - Celery with Redis for task management.  \\n \\n#### **Cloud Storage Module**  \\n- **Responsibilities**:  \\n  - Store datasets securely in a scalable cloud storage bucket.  \\n  - Use pre-signed URLs for efficient and secure file uploads.  \\n- **Technologies**:  \\n  - AWS S3, Azure Blob Storage, or Google Cloud Storage.  \\n  - Encryption protocols (e.g., HTTPS, TLS).  \\n \\n#### **Validation Module**  \\n- **Responsibilities**:  \\n  - Ensure datasets meet structural and format requirements based on the selected dataset type.  \\n  - Validate file integrity and detect corrupted files.  \\n- **Technologies**:  \\n  - Python libraries: `os`, `zipfile`, `Pillow`, and `pyzipper`.  \\n \\n---\\n \\n## 3. **Detailed Design**  \\n \\n### 3.1 **Data Structures**  \\n \\n#### **Frontend State Management**  \\n```javascript\\n{\\n  file: File,  // File object selected by the user\\n  datasetType: \"classification\" | \"segmentation\",  // Dataset type selected by user\\n  progress: number,  // Progress percentage (0-100)\\n  feedback: {        // Real-time feedback messages\\n    status: \"success\" | \"warning\" | \"error\",\\n    message: string\\n  }\\n}\\n```\\n \\n#### **Backend Request Body**  \\n```json\\n{\\n  \"filePath\": \"/path/to/zipfile.zip\",\\n  \"datasetType\": \"classification\"\\n}\\n```\\n \\n#### **Validation Module Output**  \\n```python\\n{\\n  \"is_valid\": bool,        # Whether the dataset passed all validations\\n  \"errors\": list[str],     # List of validation errors\\n  \"warnings\": list[str],   # List of validation warnings\\n  \"progress\": int          # Validation progress percentage\\n}\\n```\\n \\n---\\n \\n### 3.2 **API Endpoints**  \\n \\n#### **1. Upload Dataset**  \\n- **URL**: `/api/dataset/upload`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset (\"classification\" or \"segmentation\").  \\n- **Response**:  \\n  ```json\\n  {\\n    \"status\": \"success\" | \"error\",\\n    \"message\": \"Feedback message describing the operation result\"\\n  }\\n  ```\\n \\n#### **2. Validate Dataset**  \\n- **URL**: `/api/dataset/validate`  \\n- **Method**: POST  \\n- **Request Body**:  \\n  - `filePath`: Path to the zip file.  \\n  - `datasetType`: Type of dataset.  \\n- **Response**:  \\n  ```json\\n  {\\n    \"is_valid\": true,\\n    \"errors\": [],\\n    \"warnings\": [\"Empty sub-folder in class \\'dogs\\'.\"]\\n  }\\n  ```\\n \\n#### **3. Task Progress**  \\n- **URL**: `/api/task/progress/{task_id}`  \\n- **Method**: GET  \\n- **Response**:  \\n  ```json\\n  {\\n    \"task_id\": \"unique_task_id\",\\n    \"progress\": 45,  // Current progress percentage\\n    \"status\": \"in_progress\",  // Task status: \"in_progress\", \"completed\", or \"failed\"\\n    \"message\": \"Validation in progress\"\\n  }\\n  ```\\n \\n---\\n \\n### 3.3 **Validation Logic**  \\n \\n#### **Classification Dataset Validation**  \\n1. Ensure the zip file contains sub-folders for each class.  \\n2. Verify that each sub-folder contains at least one image file.  \\n3. Log warnings for empty sub-folders.  \\n \\n#### **Segmentation Dataset Validation**  \\n1. Check for \"images\" and \"mask\" folders.  \\n2. Verify that each image in the \"images\" folder has a corresponding mask in the \"mask\" folder.  \\n3. Reject the dataset if the \"mask\" folder is empty or masks are missing.  \\n \\n#### **File Integrity Validation**  \\n1. Attempt to open each image file using `Pillow`.  \\n2. Reject the dataset if corrupted files are found.  \\n \\n#### **Image Format Validation**  \\n1. Accept only `.jpg`, `.png`, and `.bmp` formats.  \\n2. Reject datasets with unsupported or mixed formats.  \\n \\n---\\n \\n### 3.4 **Chunked File Upload Implementation**  \\n1. Split files exceeding 500MB into chunks.  \\n2. Upload each chunk sequentially to cloud storage using pre-signed URLs.  \\n3. Reassemble the file on the backend after all chunks are uploaded.  \\n \\n---\\n \\n## 4. **Performance, Security, and Scalability**  \\n \\n### 4.1 **Performance**  \\n- Use asynchronous tasks for handling large files and validations.  \\n- Provide real-time feedback using WebSockets or SSE.  \\n- Limit dataset size to 10GB and use chunked uploads for files larger than 500MB.  \\n \\n### 4.2 **Security**  \\n- Enforce HTTPS and TLS for secure data transfer.  \\n- Reject password-protected or encrypted zip files.  \\n- Use role-based access control (RBAC) for cloud storage operations.  \\n \\n### 4.3 **Scalability**  \\n- Use scalable cloud storage (e.g., AWS S3).  \\n- Employ load balancing for backend APIs to handle high traffic.  \\n- Use Redis for task queue management to ensure fault tolerance.  \\n \\n---\\n \\n## 5. **Constraints and Dependencies**  \\n \\n### 5.1 **Constraints**  \\n1. Maximum dataset size: 10GB.  \\n2. Supported image formats: `.jpg`, `.png`, and `.bmp`.  \\n3. No support for password-protected zip files.  \\n \\n### 5.2 **Dependencies**  \\n1. Cloud storage provider (AWS S3 or equivalent).  \\n2. Python libraries for file validation (`Pillow`, `zipfile`).  \\n3. WebSocket or SSE for real-time communication.  \\n \\n---\\n \\n## 6. **Edge Case Handling**  \\n \\n### **Classification Dataset**  \\n1. Empty sub-folders: Log warnings and allow upload.  \\n2. Corrupted files: Reject the dataset.  \\n \\n### **Segmentation Dataset**  \\n1. Missing masks: Reject the dataset.  \\n2. Empty \"mask\" folder: Reject the dataset.  \\n \\n### **General Cases**  \\n1. Unsupported formats: Reject the dataset.  \\n2. Mixed formats: Reject the dataset.  \\n3. Cloud storage failure: Retry upload up to 3 times with exponential backoff.  \\n \\n---\\n \\n## 7. **Monitoring and Logging**  \\n \\n- Use Prometheus and Grafana for performance monitoring.  \\n- Log all errors, warnings, and progress updates to CloudWatch or ELK Stack.  \\n- Set up alerts for critical failures using Slack or PagerDuty.  \\n', additional_kwargs={}, response_metadata={}), AIMessage(content='# Production-ready Python code for the backend implementation of the dataset upload, validation, and feedback system.\\n\\n```python\\nimport os\\nimport zipfile\\nimport logging\\nimport uuid\\nfrom typing import Dict, List\\nfrom fastapi import FastAPI, UploadFile, Form, HTTPException, BackgroundTasks\\nfrom pydantic import BaseModel\\nfrom PIL import Image\\nimport boto3\\nfrom botocore.exceptions import NoCredentialsError, PartialCredentialsError\\nfrom redis import Redis\\nfrom celery import Celery\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\\nlogger = logging.getLogger(__name__)\\n\\n# FastAPI application\\napp = FastAPI()\\n\\n# Celery configuration\\nredis_url = \"redis://localhost:6379/0\"\\ncelery_app = Celery(\"tasks\", broker=redis_url, backend=redis_url)\\n\\n# AWS S3 Configuration\\nAWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\\nAWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\\nAWS_BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")\\n\\n# Dataset validation configuration\\nMAX_DATASET_SIZE_MB = 10240  # 10 GB\\nSUPPORTED_IMAGE_FORMATS = {\".jpg\", \".png\", \".bmp\"}\\n\\n\\nclass ValidationResult(BaseModel):\\n    is_valid: bool\\n    errors: List[str]\\n    warnings: List[str]\\n    progress: int\\n\\n\\nclass ProgressResponse(BaseModel):\\n    task_id: str\\n    progress: int\\n    status: str\\n    message: str\\n\\n\\n@app.post(\"/api/dataset/upload\")\\nasync def upload_dataset(\\n    file: UploadFile, dataset_type: str = Form(...), background_tasks: BackgroundTasks\\n):\\n    \"\"\"\\n    Upload a dataset to the server and validate it.\\n    Parameters:\\n        file (UploadFile): The uploaded zip file containing the dataset.\\n        dataset_type (str): Type of dataset (\"classification\" or \"segmentation\").\\n        background_tasks (BackgroundTasks): FastAPI background task manager.\\n    Returns:\\n        JSON response indicating upload and validation success or failure.\\n    \"\"\"\\n    try:\\n        if file.content_type != \"application/zip\":\\n            raise HTTPException(status_code=400, detail=\"Only zip files are supported.\")\\n\\n        # Save the uploaded file temporarily\\n        temp_file_path = f\"/tmp/{uuid.uuid4()}_{file.filename}\"\\n        with open(temp_file_path, \"wb\") as temp_file:\\n            temp_file.write(await file.read())\\n\\n        # Enqueue validation task\\n        task_id = str(uuid.uuid4())\\n        background_tasks.add_task(validate_dataset, temp_file_path, dataset_type, task_id)\\n\\n        return {\"status\": \"success\", \"message\": \"File uploaded successfully.\", \"task_id\": task_id}\\n\\n    except Exception as e:\\n        logger.error(f\"Error uploading dataset: {e}\")\\n        raise HTTPException(status_code=500, detail=\"An error occurred during upload.\")\\n\\n\\n@app.get(\"/api/task/progress/{task_id}\", response_model=ProgressResponse)\\ndef get_task_progress(task_id: str):\\n    \"\"\"\\n    Retrieve progress of an asynchronous task.\\n    Parameters:\\n        task_id (str): Unique identifier of the task.\\n    Returns:\\n        Task progress details.\\n    \"\"\"\\n    task_result = celery_app.AsyncResult(task_id)\\n    if not task_result:\\n        raise HTTPException(status_code=404, detail=\"Task not found.\")\\n\\n    return {\\n        \"task_id\": task_id,\\n        \"progress\": task_result.info.get(\"progress\", 0) if task_result.info else 0,\\n        \"status\": task_result.status,\\n        \"message\": task_result.info.get(\"message\", \"No updates available\") if task_result.info else \"\",\\n    }\\n\\n\\n@celery_app.task(bind=True)\\ndef validate_dataset(self, file_path: str, dataset_type: str, task_id: str):\\n    \"\"\"\\n    Validate a dataset according to the specified type.\\n    Parameters:\\n        file_path (str): Path to the zip file.\\n        dataset_type (str): Type of dataset (classification or segmentation).\\n        task_id (str): Unique identifier for the task.\\n    \"\"\"\\n    try:\\n        self.update_state(state=\"STARTED\", meta={\"progress\": 0, \"message\": \"Validation started.\"})\\n\\n        # Check file size limit\\n        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\\n        if file_size_mb > MAX_DATASET_SIZE_MB:\\n            raise ValueError(\"Dataset exceeds maximum allowed size of 10 GB.\")\\n\\n        # Extract zip file\\n        extraction_dir = f\"/tmp/{uuid.uuid4()}\"\\n        os.makedirs(extraction_dir, exist_ok=True)\\n        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\\n            zip_ref.extractall(extraction_dir)\\n\\n        # Perform validation based on dataset type\\n        errors, warnings = [], []\\n        if dataset_type == \"classification\":\\n            validate_classification_dataset(extraction_dir, errors, warnings)\\n        elif dataset_type == \"segmentation\":\\n            validate_segmentation_dataset(extraction_dir, errors, warnings)\\n        else:\\n            raise ValueError(\"Unsupported dataset type.\")\\n\\n        # Update progress\\n        self.update_state(state=\"PROGRESS\", meta={\"progress\": 80, \"message\": \"Validation nearing completion.\"})\\n\\n        # Upload to cloud storage if valid\\n        if not errors:\\n            upload_to_cloud_storage(extraction_dir)\\n\\n        # Cleanup\\n        os.remove(file_path)\\n        os.rmdir(extraction_dir)\\n\\n        self.update_state(state=\"SUCCESS\", meta={\"progress\": 100, \"message\": \"Validation complete.\"})\\n        return {\"is_valid\": not errors, \"errors\": errors, \"warnings\": warnings}\\n\\n    except Exception as e:\\n        logger.error(f\"Validation failed: {e}\")\\n        self.update_state(state=\"FAILURE\", meta={\"progress\": 100, \"message\": str(e)})\\n\\n\\ndef validate_classification_dataset(directory: str, errors: List[str], warnings: List[str]):\\n    \"\"\"\\n    Validate a classification dataset structure and format.\\n    \"\"\"\\n    subfolders = [f.path for f in os.scandir(directory) if f.is_dir()]\\n    if not subfolders:\\n        errors.append(\"No sub-folders found for classification dataset.\")\\n\\n    for subfolder in subfolders:\\n        image_files = [\\n            f for f in os.listdir(subfolder) if os.path.isfile(os.path.join(subfolder, f))\\n        ]\\n        if not image_files:\\n            warnings.append(f\"Empty sub-folder: {subfolder}\")\\n        else:\\n            for image_file in image_files:\\n                validate_image(os.path.join(subfolder, image_file), errors)\\n\\n\\ndef validate_segmentation_dataset(directory: str, errors: List[str], warnings: List[str]):\\n    \"\"\"\\n    Validate a segmentation dataset structure and format.\\n    \"\"\"\\n    images_dir = os.path.join(directory, \"images\")\\n    masks_dir = os.path.join(directory, \"masks\")\\n\\n    if not os.path.isdir(images_dir) or not os.path.isdir(masks_dir):\\n        errors.append(\"Missing \\'images\\' or \\'masks\\' folder in segmentation dataset.\")\\n        return\\n\\n    image_files = set(os.listdir(images_dir))\\n    mask_files = set(os.listdir(masks_dir))\\n\\n    if not image_files:\\n        errors.append(\"No images found in \\'images\\' folder.\")\\n    if not mask_files:\\n        errors.append(\"No masks found in \\'masks\\' folder.\")\\n\\n    for image_file in image_files:\\n        if image_file not in mask_files:\\n            errors.append(f\"Missing mask for image: {image_file}\")\\n\\n        validate_image(os.path.join(images_dir, image_file), errors)\\n        validate_image(os.path.join(masks_dir, image_file), errors)\\n\\n\\ndef validate_image(file_path: str, errors: List[str]):\\n    \"\"\"\\n    Validate individual image for format and integrity.\\n    \"\"\"\\n    try:\\n        _, ext = os.path.splitext(file_path)\\n        if ext.lower() not in SUPPORTED_IMAGE_FORMATS:\\n            errors.append(f\"Unsupported image format for file: {file_path}\")\\n\\n        # Check file integrity\\n        with Image.open(file_path) as img:\\n            img.verify()\\n    except Exception:\\n        errors.append(f\"Corrupted or unreadable image file: {file_path}\")\\n\\n\\ndef upload_to_cloud_storage(directory: str):\\n    \"\"\"\\n    Upload validated dataset to AWS S3.\\n    \"\"\"\\n    try:\\n        s3_client = boto3.client(\\n            \"s3\",\\n            aws_access_key_id=AWS_ACCESS_KEY_ID,\\n            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\\n        )\\n        for root, _, files in os.walk(directory):\\n            for file in files:\\n                file_path = os.path.join(root, file)\\n                s3_client.upload_file(file_path, AWS_BUCKET_NAME, os.path.relpath(file_path, directory))\\n    except (NoCredentialsError, PartialCredentialsError) as e:\\n        logger.error(f\"Cloud storage upload failed: {e}\")\\n        raise\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1824, 'prompt_tokens': 3799, 'total_tokens': 5623, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_a42ed5ff0c', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-2ca61fb5-f3eb-460e-90a8-277edd66a649-0', usage_metadata={'input_tokens': 3799, 'output_tokens': 1824, 'total_tokens': 5623, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='```json\\n{\\n  \"validation_report\": {\\n    \"overall_assessment\": \"Fail\",\\n    \"issues_found\": [\\n      \"Error handling for cloud storage upload failures is incomplete; there is no retry mechanism as specified in the requirements.\",\\n      \"The implementation for progress updates during validation is inconsistent with the specification (e.g., no granular updates).\",\\n      \"Security requirements regarding password-protected zip files are not explicitly enforced.\",\\n      \"Edge case handling for mixed image formats is missing; datasets with mixed formats are not rejected.\",\\n      \"Chunked uploads for large files (>500MB) are not implemented as required in the specification.\"\\n    ],\\n    \"suggested_improvements\": [\\n      {\\n        \"description\": \"Implement retry logic with exponential backoff for cloud storage upload failures.\",\\n        \"priority\": \"high\"\\n      },\\n      {\\n        \"description\": \"Add granular progress updates for validation tasks to align with real-time feedback requirements.\",\\n        \"priority\": \"medium\"\\n      },\\n      {\\n        \"description\": \"Explicitly validate and reject password-protected or encrypted zip files during file upload.\",\\n        \"priority\": \"high\"\\n      },\\n      {\\n        \"description\": \"Ensure mixed image formats in datasets are correctly identified and rejected in the validation logic.\",\\n        \"priority\": \"high\"\\n      },\\n      {\\n        \"description\": \"Implement chunked upload functionality for datasets exceeding 500MB to meet scalability requirements.\",\\n        \"priority\": \"high\"\\n      }\\n    ],\\n    \"implementation_vs_requirements\": {\\n      \"match\": false,\\n      \"details\": [\\n        {\\n          \"requirement_section\": \"Cloud Storage Failure Handling\",\\n          \"status\": \"Partially Implemented\",\\n          \"notes\": \"Retry logic for cloud storage failures is not implemented.\"\\n        },\\n        {\\n          \"requirement_section\": \"Real-Time Feedback\",\\n          \"status\": \"Partially Implemented\",\\n          \"notes\": \"Progress updates are provided, but they lack granularity and real-time communication mechanisms like WebSocket/SSE.\"\\n        },\\n        {\\n          \"requirement_section\": \"Security Requirements\",\\n          \"status\": \"Partially Implemented\",\\n          \"notes\": \"Password-protected zip files are not explicitly rejected.\"\\n        },\\n        {\\n          \"requirement_section\": \"Validation Logic\",\\n          \"status\": \"Partially Implemented\",\\n          \"notes\": \"Mixed image formats are not properly rejected during validation.\"\\n        },\\n        {\\n          \"requirement_section\": \"Chunked File Upload Implementation\",\\n          \"status\": \"Not Implemented\",\\n          \"notes\": \"There is no implementation for chunked uploads for large datasets.\"\\n        }\\n      ]\\n    }\\n  }\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 535, 'prompt_tokens': 5855, 'total_tokens': 6390, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_a42ed5ff0c', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-c0705896-883d-45cf-8b41-4a68e2c4beae-0', usage_metadata={'input_tokens': 5855, 'output_tokens': 535, 'total_tokens': 6390, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "correction **************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:48:16,749 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-04 16:48:16,749 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 51.000000 seconds\n",
      "2025-03-04 16:49:50,434 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 16:49:50,435 - __main__ - INFO - [4021668688.py:72] - Saved code attempt to code_generation_20250304_164704/attempt_correction/code.py\n"
     ]
    }
   ],
   "source": [
    "# Requirements specification is the technical specification document\n",
    "Requirements = '''\n",
    "## 1. **Overview**  \n",
    "This technical specification provides a detailed plan for implementing a feature that allows users to upload training datasets from local storage to a cloud training bucket, validate the datasets for integrity and structure, and provide real-time feedback. It addresses all functional requirements, edge cases, and non-functional requirements.\n",
    " \n",
    "---\n",
    " \n",
    "## 2. **System Architecture**  \n",
    " \n",
    "### 2.1 **High-Level Architecture**  \n",
    "The system consists of the following components:  \n",
    "1. **Frontend Module**: Provides the user interface for file selection, dataset type input, and real-time feedback display.  \n",
    "2. **Backend API**: Handles dataset validation, file extraction, cloud upload, and feedback delivery.  \n",
    "3. **Cloud Storage Module**: Stores validated datasets securely.  \n",
    "4. **Validation Module**: Performs structural, file integrity, and format validation.  \n",
    " \n",
    "---\n",
    " \n",
    "### 2.2 **Component Breakdown**  \n",
    " \n",
    "#### **Frontend Module**  \n",
    "- **Responsibilities**:  \n",
    "  - Allow users to select a zip file and dataset type.  \n",
    "  - Provide real-time progress updates for file upload and validation processes.  \n",
    "  - Display success, warning, or error messages.  \n",
    "- **Technologies**:  \n",
    "  - React.js or Angular for UI.  \n",
    "  - WebSocket or Server-Sent Events (SSE) for real-time feedback.  \n",
    "  - Axios or Fetch API for communication with the backend.  \n",
    " \n",
    "#### **Backend API**  \n",
    "- **Responsibilities**:  \n",
    "  - Handle dataset upload and validation requests.  \n",
    "  - Manage asynchronous tasks for large file uploads and validations.  \n",
    "  - Communicate real-time progress and feedback to the frontend.  \n",
    "- **Technologies**:  \n",
    "  - Python (FastAPI or Flask) for API development.  \n",
    "  - Celery with Redis for task management.  \n",
    " \n",
    "#### **Cloud Storage Module**  \n",
    "- **Responsibilities**:  \n",
    "  - Store datasets securely in a scalable cloud storage bucket.  \n",
    "  - Use pre-signed URLs for efficient and secure file uploads.  \n",
    "- **Technologies**:  \n",
    "  - AWS S3, Azure Blob Storage, or Google Cloud Storage.  \n",
    "  - Encryption protocols (e.g., HTTPS, TLS).  \n",
    " \n",
    "#### **Validation Module**  \n",
    "- **Responsibilities**:  \n",
    "  - Ensure datasets meet structural and format requirements based on the selected dataset type.  \n",
    "  - Validate file integrity and detect corrupted files.  \n",
    "- **Technologies**:  \n",
    "  - Python libraries: `os`, `zipfile`, `Pillow`, and `pyzipper`.  \n",
    " \n",
    "---\n",
    " \n",
    "## 3. **Detailed Design**  \n",
    " \n",
    "### 3.1 **Data Structures**  \n",
    " \n",
    "#### **Frontend State Management**  \n",
    "```javascript\n",
    "{\n",
    "  file: File,  // File object selected by the user\n",
    "  datasetType: \"classification\" | \"segmentation\",  // Dataset type selected by user\n",
    "  progress: number,  // Progress percentage (0-100)\n",
    "  feedback: {        // Real-time feedback messages\n",
    "    status: \"success\" | \"warning\" | \"error\",\n",
    "    message: string\n",
    "  }\n",
    "}\n",
    "```\n",
    " \n",
    "#### **Backend Request Body**  \n",
    "```json\n",
    "{\n",
    "  \"filePath\": \"/path/to/zipfile.zip\",\n",
    "  \"datasetType\": \"classification\"\n",
    "}\n",
    "```\n",
    " \n",
    "#### **Validation Module Output**  \n",
    "```python\n",
    "{\n",
    "  \"is_valid\": bool,        # Whether the dataset passed all validations\n",
    "  \"errors\": list[str],     # List of validation errors\n",
    "  \"warnings\": list[str],   # List of validation warnings\n",
    "  \"progress\": int          # Validation progress percentage\n",
    "}\n",
    "```\n",
    " \n",
    "---\n",
    " \n",
    "### 3.2 **API Endpoints**  \n",
    " \n",
    "#### **1. Upload Dataset**  \n",
    "- **URL**: `/api/dataset/upload`  \n",
    "- **Method**: POST  \n",
    "- **Request Body**:  \n",
    "  - `filePath`: Path to the zip file.  \n",
    "  - `datasetType`: Type of dataset (\"classification\" or \"segmentation\").  \n",
    "- **Response**:  \n",
    "  ```json\n",
    "  {\n",
    "    \"status\": \"success\" | \"error\",\n",
    "    \"message\": \"Feedback message describing the operation result\"\n",
    "  }\n",
    "  ```\n",
    " \n",
    "#### **2. Validate Dataset**  \n",
    "- **URL**: `/api/dataset/validate`  \n",
    "- **Method**: POST  \n",
    "- **Request Body**:  \n",
    "  - `filePath`: Path to the zip file.  \n",
    "  - `datasetType`: Type of dataset.  \n",
    "- **Response**:  \n",
    "  ```json\n",
    "  {\n",
    "    \"is_valid\": true,\n",
    "    \"errors\": [],\n",
    "    \"warnings\": [\"Empty sub-folder in class 'dogs'.\"]\n",
    "  }\n",
    "  ```\n",
    " \n",
    "#### **3. Task Progress**  \n",
    "- **URL**: `/api/task/progress/{task_id}`  \n",
    "- **Method**: GET  \n",
    "- **Response**:  \n",
    "  ```json\n",
    "  {\n",
    "    \"task_id\": \"unique_task_id\",\n",
    "    \"progress\": 45,  // Current progress percentage\n",
    "    \"status\": \"in_progress\",  // Task status: \"in_progress\", \"completed\", or \"failed\"\n",
    "    \"message\": \"Validation in progress\"\n",
    "  }\n",
    "  ```\n",
    " \n",
    "---\n",
    " \n",
    "### 3.3 **Validation Logic**  \n",
    " \n",
    "#### **Classification Dataset Validation**  \n",
    "1. Ensure the zip file contains sub-folders for each class.  \n",
    "2. Verify that each sub-folder contains at least one image file.  \n",
    "3. Log warnings for empty sub-folders.  \n",
    " \n",
    "#### **Segmentation Dataset Validation**  \n",
    "1. Check for \"images\" and \"mask\" folders.  \n",
    "2. Verify that each image in the \"images\" folder has a corresponding mask in the \"mask\" folder.  \n",
    "3. Reject the dataset if the \"mask\" folder is empty or masks are missing.  \n",
    " \n",
    "#### **File Integrity Validation**  \n",
    "1. Attempt to open each image file using `Pillow`.  \n",
    "2. Reject the dataset if corrupted files are found.  \n",
    " \n",
    "#### **Image Format Validation**  \n",
    "1. Accept only `.jpg`, `.png`, and `.bmp` formats.  \n",
    "2. Reject datasets with unsupported or mixed formats.  \n",
    " \n",
    "---\n",
    " \n",
    "### 3.4 **Chunked File Upload Implementation**  \n",
    "1. Split files exceeding 500MB into chunks.  \n",
    "2. Upload each chunk sequentially to cloud storage using pre-signed URLs.  \n",
    "3. Reassemble the file on the backend after all chunks are uploaded.  \n",
    " \n",
    "---\n",
    " \n",
    "## 4. **Performance, Security, and Scalability**  \n",
    " \n",
    "### 4.1 **Performance**  \n",
    "- Use asynchronous tasks for handling large files and validations.  \n",
    "- Provide real-time feedback using WebSockets or SSE.  \n",
    "- Limit dataset size to 10GB and use chunked uploads for files larger than 500MB.  \n",
    " \n",
    "### 4.2 **Security**  \n",
    "- Enforce HTTPS and TLS for secure data transfer.  \n",
    "- Reject password-protected or encrypted zip files.  \n",
    "- Use role-based access control (RBAC) for cloud storage operations.  \n",
    " \n",
    "### 4.3 **Scalability**  \n",
    "- Use scalable cloud storage (e.g., AWS S3).  \n",
    "- Employ load balancing for backend APIs to handle high traffic.  \n",
    "- Use Redis for task queue management to ensure fault tolerance.  \n",
    " \n",
    "---\n",
    " \n",
    "## 5. **Constraints and Dependencies**  \n",
    " \n",
    "### 5.1 **Constraints**  \n",
    "1. Maximum dataset size: 10GB.  \n",
    "2. Supported image formats: `.jpg`, `.png`, and `.bmp`.  \n",
    "3. No support for password-protected zip files.  \n",
    " \n",
    "### 5.2 **Dependencies**  \n",
    "1. Cloud storage provider (AWS S3 or equivalent).  \n",
    "2. Python libraries for file validation (`Pillow`, `zipfile`).  \n",
    "3. WebSocket or SSE for real-time communication.  \n",
    " \n",
    "---\n",
    " \n",
    "## 6. **Edge Case Handling**  \n",
    " \n",
    "### **Classification Dataset**  \n",
    "1. Empty sub-folders: Log warnings and allow upload.  \n",
    "2. Corrupted files: Reject the dataset.  \n",
    " \n",
    "### **Segmentation Dataset**  \n",
    "1. Missing masks: Reject the dataset.  \n",
    "2. Empty \"mask\" folder: Reject the dataset.  \n",
    " \n",
    "### **General Cases**  \n",
    "1. Unsupported formats: Reject the dataset.  \n",
    "2. Mixed formats: Reject the dataset.  \n",
    "3. Cloud storage failure: Retry upload up to 3 times with exponential backoff.  \n",
    " \n",
    "---\n",
    " \n",
    "## 7. **Monitoring and Logging**  \n",
    " \n",
    "- Use Prometheus and Grafana for performance monitoring.  \n",
    "- Log all errors, warnings, and progress updates to CloudWatch or ELK Stack.  \n",
    "- Set up alerts for critical failures using Slack or PagerDuty.  \n",
    "''' \n",
    "\n",
    "# Setup initial message\n",
    "messages = [HumanMessage(content=Requirements)]\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Run the workflow\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    # Model initialization\n",
    "    model = AzureChatOpenAI(\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "        api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "    )\n",
    "    code_gen = CodeGenerator(\n",
    "        model=model, \n",
    "        system_developer=developer_prompt,\n",
    "        system_validator=validator_prompt,\n",
    "        system_corrector=corrector_prompt,\n",
    "        checkpointer=checkpointer\n",
    "    )\n",
    "    \n",
    "    # Stream results\n",
    "    thread = []\n",
    "    for event in code_gen.graph.stream(\n",
    "        {\"messages\": messages},\n",
    "        config={\"configurable\": {\n",
    "            \"thread_id\": \"thread_1\",\n",
    "            \"checkpoint_ns\": \"code_generation\"\n",
    "        }}\n",
    "    ):\n",
    "        for key, value in event.items():\n",
    "            if key == 'messages' and value:\n",
    "                # Extract only the code part if it's from developer or corrector\n",
    "                if hasattr(value[0], 'content'):\n",
    "                    content = value[0].content\n",
    "                    if \"```python\" in content or \"```\" in content:\n",
    "                        code = code_gen.extract_code(content)\n",
    "                        print(code)\n",
    "                    else:\n",
    "                        print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:49:03,520 - __main__ - INFO - [1357048344.py:236] - Using columns: User Story = 'userstory', Tech Spec = 'tech spec'\n",
      "2025-03-06 01:49:03,521 - __main__ - INFO - [1357048344.py:260] - Successfully extracted 7 tech specs from Excel file\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:49:04,411 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_141 (1/7)\n",
      "2025-03-06 01:49:04,413 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:49:29,844 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:49:29,846 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_141/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:49:30,283 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:49:30,283 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 36.000000 seconds\n",
      "2025-03-06 01:50:17,324 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:50:17,325 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_141/validation_results.json\n",
      "2025-03-06 01:50:17,326 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_141/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:50:17,971 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:50:17,972 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 51.000000 seconds\n",
      "2025-03-06 01:51:27,866 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:51:27,867 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_141/attempt_correction/code.py\n",
      "2025-03-06 01:51:27,868 - __main__ - INFO - [1357048344.py:545] - Successfully processed tech spec for user story ID: US_141\n",
      "2025-03-06 01:51:27,868 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 01:51:27,868 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_142 (2/7)\n",
      "2025-03-06 01:51:27,870 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:51:28,301 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:51:28,302 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 43.000000 seconds\n",
      "2025-03-06 01:52:29,747 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:52:29,748 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_142/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:52:30,174 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:52:30,175 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 43.000000 seconds\n",
      "2025-03-06 01:53:28,043 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:53:28,045 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_142/validation_results.json\n",
      "2025-03-06 01:53:28,045 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_142/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:53:28,687 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:53:28,688 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 46.000000 seconds\n",
      "2025-03-06 01:54:34,148 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:54:34,150 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_142/attempt_correction/code.py\n",
      "2025-03-06 01:54:34,151 - __main__ - INFO - [1357048344.py:545] - Successfully processed tech spec for user story ID: US_142\n",
      "2025-03-06 01:54:34,151 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 01:54:34,151 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_143 (3/7)\n",
      "2025-03-06 01:54:34,153 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:54:34,577 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:54:34,577 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 42.000000 seconds\n",
      "2025-03-06 01:55:49,683 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:55:49,685 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_143/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:55:50,119 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:55:50,119 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 28.000000 seconds\n",
      "2025-03-06 01:56:29,711 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:56:29,713 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_143/validation_results.json\n",
      "2025-03-06 01:56:29,714 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_143/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:56:30,357 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:56:30,357 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 01:57:31,742 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:57:31,743 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 01:58:33,224 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:58:33,229 - __main__ - ERROR - [1357048344.py:553] - Error processing tech spec for user story ID US_143: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "2025-03-06 01:58:33,229 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_144 (4/7)\n",
      "2025-03-06 01:58:33,231 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:58:49,820 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:58:49,851 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_144/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:58:50,284 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:58:50,284 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 44.000000 seconds\n",
      "2025-03-06 01:59:45,115 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 01:59:45,116 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_144/validation_results.json\n",
      "2025-03-06 01:59:45,117 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_144/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 01:59:45,762 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 01:59:45,763 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 50.000000 seconds\n",
      "2025-03-06 02:01:12,025 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:01:12,026 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_144/attempt_correction/code.py\n",
      "2025-03-06 02:01:12,027 - __main__ - INFO - [1357048344.py:545] - Successfully processed tech spec for user story ID: US_144\n",
      "2025-03-06 02:01:12,028 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-06 02:01:12,028 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_145 (5/7)\n",
      "2025-03-06 02:01:12,030 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:01:12,455 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:01:12,455 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 25.000000 seconds\n",
      "2025-03-06 02:02:17,754 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:02:17,756 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_145/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:02:18,194 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:02:18,194 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 21.000000 seconds\n",
      "2025-03-06 02:02:50,975 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:02:50,977 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_145/validation_results.json\n",
      "2025-03-06 02:02:50,977 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_145/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:02:51,621 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:02:51,622 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 49.000000 seconds\n",
      "2025-03-06 02:04:20,131 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:04:20,132 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_145/attempt_correction/code.py\n",
      "2025-03-06 02:04:20,133 - __main__ - INFO - [1357048344.py:545] - Successfully processed tech spec for user story ID: US_145\n",
      "2025-03-06 02:04:20,134 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 02:04:20,134 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_146 (6/7)\n",
      "2025-03-06 02:04:20,135 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:04:20,577 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:04:20,578 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 22.000000 seconds\n",
      "2025-03-06 02:05:02,926 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:05:02,928 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_146/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:05:03,355 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:05:03,355 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 41.000000 seconds\n",
      "2025-03-06 02:05:57,859 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:05:57,860 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_146/validation_results.json\n",
      "2025-03-06 02:05:57,861 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_146/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:05:58,599 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:05:58,599 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 47.000000 seconds\n",
      "2025-03-06 02:07:19,203 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:07:19,205 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_146/attempt_correction/code.py\n",
      "2025-03-06 02:07:19,206 - __main__ - INFO - [1357048344.py:545] - Successfully processed tech spec for user story ID: US_146\n",
      "2025-03-06 02:07:19,206 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 02:07:19,206 - __main__ - INFO - [1357048344.py:525] - Processing tech spec for user story ID: US_147 (7/7)\n",
      "2025-03-06 02:07:19,208 - __main__ - INFO - [1357048344.py:356] - Processing user story ID: US_147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "developer - User Story ID: US_147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:07:19,632 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:07:19,633 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 28.000000 seconds\n",
      "2025-03-06 02:08:16,254 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:08:16,255 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_147/attempt_initial/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate - User Story ID: US_147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:08:16,684 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:08:16,684 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 33.000000 seconds\n",
      "2025-03-06 02:09:01,327 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 02:09:01,329 - __main__ - INFO - [1357048344.py:428] - Saved validation results to code_generation_20250306_014903/US_147/validation_results.json\n",
      "2025-03-06 02:09:01,329 - __main__ - INFO - [1357048344.py:349] - Saved code attempt to code_generation_20250306_014903/US_147/attempt_validated_fail/code.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction - User Story ID: US_147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:09:01,976 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:09:01,977 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 02:10:03,461 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:10:03,462 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 02:11:04,827 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 02:11:04,835 - __main__ - ERROR - [1357048344.py:553] - Error processing tech spec for user story ID US_147: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "2025-03-06 02:11:04,835 - __main__ - INFO - [1357048344.py:556] - Completed processing all tech specs. Output directory: code_generation_20250306_014903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processing complete. Output directory: code_generation_20250306_014903\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from IPython.display import Image, display\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Azure OpenAI configuration\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "\n",
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: str\n",
    "    validation_status: bool\n",
    "    error_messages: list[str]\n",
    "    is_valid: bool  # Using is_valid to match the notebook's conditional edge structure\n",
    "    user_story_id: str  # Added to track user story ID for folder naming\n",
    "\n",
    "# Define prompts\n",
    "developer_prompt = \"\"\"\n",
    "Role: Python Developer\n",
    "Task: Generate complete, production-ready Python code based on the requirements specification.\n",
    "\n",
    "Requirements:\n",
    "{requirements}\n",
    "\n",
    "Your code must include:\n",
    "1. All necessary imports and dependencies\n",
    "2. Complete implementation with:\n",
    "   - Well-structured classes and functions\n",
    "   - Configuration management (using dataclasses or similar)\n",
    "   - Comprehensive error handling and validation\n",
    "   - Type hints throughout\n",
    "   - Logging with appropriate levels\n",
    "   - Unit tests where applicable\n",
    "3. Clear documentation:\n",
    "   - Module docstrings\n",
    "   - Function/method docstrings with parameters and return values\n",
    "   - Inline comments for complex logic\n",
    "\n",
    "Focus on implementing EVERY aspect mentioned in the requirements. Do not leave any required functionality unimplemented.\n",
    "\n",
    "## Output Format\n",
    "Your response should be the complete, production-ready Python code without surrounding explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "\n",
    "# Your Python code here\n",
    "\"\"\"\n",
    "\n",
    "validator_prompt = \"\"\"\n",
    "Role: Senior Code Reviewer\n",
    "Task: Perform a thorough validation of the provided Python code against the requirements.\n",
    "\n",
    "Requirements:\n",
    "{Requirements}\n",
    "\n",
    "Validation Process:\n",
    "1. Carefully compare the code against EACH requirement in the specification\n",
    "2. For each requirement, determine if it has been fully, partially, or not implemented\n",
    "3. Identify any missing functionality, edge cases, or requirements\n",
    "4. Evaluate code quality, error handling, security, and performance\n",
    "\n",
    "Validation Checklist:\n",
    "1. Code Completeness:\n",
    "   - All imports and dependencies present\n",
    "   - Full implementation of required functionality (check EACH requirement)\n",
    "   - No placeholder code or TODOs\n",
    "\n",
    "2. Code Quality:\n",
    "   - Follows PEP 8 standards\n",
    "   - Clear variable/function naming\n",
    "   - Appropriate modularization\n",
    "   - Avoids code duplication\n",
    "   - Maintainable architecture\n",
    "\n",
    "3. Technical Implementation:\n",
    "   - Proper error handling with specific exceptions\n",
    "   - Complete type annotations\n",
    "   - Correct algorithm implementation\n",
    "   - Efficient resource usage\n",
    "   - Security considerations addressed\n",
    "\n",
    "4. Documentation:\n",
    "   - Comprehensive docstrings\n",
    "   - Clear inline comments where needed\n",
    "\n",
    "## Output Format\n",
    "Return your validation report as a structured JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"validation_report\": {{\n",
    "    \"overall_assessment\": \"Pass/Fail\",\n",
    "    \"issues_found\": [\n",
    "      \"Issue 1 description\",\n",
    "      \"Issue 2 description\",\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"suggested_improvements\": [\n",
    "      {{\n",
    "        \"description\": \"Improvement 1\",\n",
    "        \"priority\": \"high/medium/low\"\n",
    "      }},\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"implementation_vs_requirements\": {{\n",
    "      \"match\": true/false,\n",
    "      \"details\": [\n",
    "        {{\n",
    "          \"requirement_section\": \"Requirement name/section\",\n",
    "          \"status\": \"Implemented/Partially Implemented/Not Implemented\",\n",
    "          \"notes\": \"Notes about implementation\"\n",
    "        }},\n",
    "        \"...\"\n",
    "      ]\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Be strict in your assessment. If ANY requirement is not fully implemented, the overall assessment should be \"Fail\".\n",
    "\"\"\"\n",
    "\n",
    "corrector_prompt = \"\"\"\n",
    "Role: Senior Python Developer\n",
    "Task: Refactor and fix the code based on the validation feedback.\n",
    "Original Requirements:\n",
    "{requirements}\n",
    "Validation Feedback:\n",
    "{ValidationFeedback}\n",
    "Correction Instructions:\n",
    "\n",
    "Address ALL issues identified in the validation feedback\n",
    "Pay particular attention to any requirements marked as \"Not Implemented\" or \"Partially Implemented\"\n",
    "Maintain the original architectural approach unless fundamentally flawed\n",
    "Ensure complete implementation of ALL requirements from the original specification\n",
    "Add or improve:\n",
    "\n",
    "Error handling for all edge cases\n",
    "Type hints throughout the codebase\n",
    "Documentation (docstrings and comments)\n",
    "Logging for important operations\n",
    "Performance optimizations where possible\n",
    "\n",
    "Important: Make sure you implement EVERY feature mentioned in the requirements that was flagged as missing or incomplete in the validation feedback.\n",
    "Output Format\n",
    "Your response should be the complete, corrected, production-ready Python code without explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "Your corrected Python code here\n",
    "\"\"\"\n",
    "\n",
    "def extract_user_story_id(user_story_text):\n",
    "    \"\"\"\n",
    "    Extract user story ID from the text that contains 'User Story ID: XXX'\n",
    "    \n",
    "    Args:\n",
    "        user_story_text (str): The full user story text\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted user story ID or 'unknown_id' if not found\n",
    "    \"\"\"\n",
    "    # Look for \"User Story ID: XXX\" pattern\n",
    "    match = re.search(r'User\\s+Story\\s+ID\\s*:\\s*(\\d+)', user_story_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"US_{match.group(1)}\"\n",
    "    \n",
    "    # Alternative pattern - look for \"userstory1\" or similar patterns at the start of a line\n",
    "    match = re.search(r'^(?:(?:user)?story|us)(\\d+)', user_story_text, re.IGNORECASE | re.MULTILINE)\n",
    "    if match:\n",
    "        return f\"US_{match.group(1)}\"\n",
    "    \n",
    "    # If no ID is found, generate a fallback ID based on a hash of the content\n",
    "    logger.warning(\"No user story ID found in text, using fallback ID\")\n",
    "    import hashlib\n",
    "    hash_id = hashlib.md5(user_story_text.encode()).hexdigest()[:8]\n",
    "    return f\"Unknown_ID_{hash_id}\"\n",
    "\n",
    "def read_tech_specs_from_excel(excel_file_path):\n",
    "    \"\"\"\n",
    "    Read technical specifications from Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'user_story_id': ID of the user story\n",
    "        - 'tech_spec': Technical specification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Find the user story column and tech spec column\n",
    "        user_story_col = None\n",
    "        tech_spec_col = None\n",
    "        \n",
    "        # Determine column names - assuming first row has column headers\n",
    "        col_names = df.columns.tolist()\n",
    "        \n",
    "        # Find user story column\n",
    "        for col in col_names:\n",
    "            if 'user' in str(col).lower() and 'story' in str(col).lower():\n",
    "                user_story_col = col\n",
    "                break\n",
    "        \n",
    "        # Find tech spec column\n",
    "        for col in col_names:\n",
    "            if ('tech' in str(col).lower() and 'spec' in str(col).lower()) or 'requirement' in str(col).lower():\n",
    "                tech_spec_col = col\n",
    "                break\n",
    "        \n",
    "        # If we didn't find the right columns, default to the first two\n",
    "        if user_story_col is None and len(col_names) > 0:\n",
    "            user_story_col = col_names[0]\n",
    "        \n",
    "        if tech_spec_col is None and len(col_names) > 1:\n",
    "            tech_spec_col = col_names[1]\n",
    "        \n",
    "        logger.info(f\"Using columns: User Story = '{user_story_col}', Tech Spec = '{tech_spec_col}'\")\n",
    "        \n",
    "        # Extract tech specs\n",
    "        tech_specs = []\n",
    "        \n",
    "        # Skip the first row if it's empty (which appears to be the case)\n",
    "        start_row = 1 if df.iloc[0].isna().all() else 0\n",
    "        \n",
    "        for idx, row in df.iloc[start_row:].iterrows():\n",
    "            if pd.isna(row[user_story_col]) or pd.isna(row[tech_spec_col]):\n",
    "                logger.warning(f\"Skipping row {idx} due to missing data\")\n",
    "                continue\n",
    "                \n",
    "            user_story_text = str(row[user_story_col])\n",
    "            tech_spec_text = str(row[tech_spec_col])\n",
    "            \n",
    "            # Extract user story ID using the helper function\n",
    "            user_story_id = extract_user_story_id(user_story_text)\n",
    "            \n",
    "            tech_specs.append({\n",
    "                'user_story_id': user_story_id,\n",
    "                'tech_spec': tech_spec_text\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(tech_specs)} tech specs from Excel file\")\n",
    "        return tech_specs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading Excel file: {e}\")\n",
    "        raise\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model, base_output_dir=None, system_developer=\"\", system_validator=\"\", system_corrector=\"\"):\n",
    "        self.system_developer = system_developer\n",
    "        self.system_validator = system_validator\n",
    "        self.system_corrector = system_corrector\n",
    "        \n",
    "        # Create output base directory\n",
    "        self.base_output_dir = base_output_dir or f\"code_generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.base_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        \n",
    "        # Add conditional edges (matching notebook pattern)\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\", \n",
    "            lambda state: state[\"is_valid\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        graph.add_edge(\"correction\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile()\n",
    "        self.model = model\n",
    "        \n",
    "        # Try to display graph visualization if possible\n",
    "        try:\n",
    "            display(Image(self.graph.get_graph().draw_mermaid_png()))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error displaying graph: {e}\")\n",
    "            pass\n",
    "\n",
    "    def get_output_dir(self, user_story_id):\n",
    "        \"\"\"Create and return a user story specific output directory\"\"\"\n",
    "        # Create user story specific directory if it doesn't exist\n",
    "        user_story_dir = os.path.join(self.base_output_dir, user_story_id)\n",
    "        os.makedirs(user_story_dir, exist_ok=True)\n",
    "        return user_story_dir\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        \"\"\"Extract code from between triple backticks or triple single quotes\"\"\"\n",
    "        pattern = r\"```(?:python)?\\\\s*(.*?)```\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        # Try with triple single quotes\n",
    "        pattern = r\"'''(?:python)?\\\\s*(.*?)'''\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        return text  # Return original if no code blocks found\n",
    "\n",
    "    def save_code_attempt(self, code: str, user_story_id: str, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt and return directory path\"\"\"\n",
    "        # Get user story specific output directory\n",
    "        output_dir = self.get_output_dir(user_story_id)\n",
    "        \n",
    "        attempt_dir = os.path.join(output_dir, f\"attempt_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        # Save code\n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved code attempt to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def developer(self, state: CodeGenerationState):\n",
    "        \"\"\"Generate initial code\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        logger.info(f\"Processing user story ID: {user_story_id}\")\n",
    "        print(f\"developer - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_developer:\n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_developer.format(\n",
    "                requirements=messages[0].content,  # Changed from Requirements to requirements\n",
    "                TechnicalSpecifications=messages[0].content\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        \n",
    "        # Extract code from response\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save code\n",
    "        self.save_code_attempt(code_only, user_story_id)\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'validation_status': None,\n",
    "            'error_messages': [],\n",
    "            'is_valid': False,\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState):\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        messages = state.get('messages', [])\n",
    "        current_code = state.get('current_code', '')\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        \n",
    "        print(f\"validate - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_validator:\n",
    "            original_message = state[\"messages\"][0].content if state[\"messages\"] else \"\"\n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_validator.format(\n",
    "                Requirements=original_message,\n",
    "                TechnicalSpecifications=original_message\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\").lower()\n",
    "        \n",
    "        # Attempt to determine if validation passed by extracting JSON\n",
    "        is_valid = False\n",
    "        try:\n",
    "            # Try to extract JSON from the message\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                is_valid = (validation_json.get(\"validation_report\", {}).get(\"overall_assessment\", \"\").lower() == \"pass\")\n",
    "        except:\n",
    "            # Fallback to the original logic if JSON extraction fails\n",
    "            is_valid = \"pass\" in response_text and \"correctly implements\" in response_text\n",
    "        \n",
    "        # Save validation results to JSON if possible\n",
    "        try:\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                output_dir = self.get_output_dir(user_story_id)\n",
    "                json_path = os.path.join(output_dir, \"validation_results.json\")\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(validation_json, f, indent=2)\n",
    "                logger.info(f\"Saved validation results to {json_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save validation results: {e}\")\n",
    "        \n",
    "        if is_valid:\n",
    "            self.save_code_attempt(current_code, user_story_id, \"validated_pass\")\n",
    "        else:\n",
    "            self.save_code_attempt(current_code, user_story_id, \"validated_fail\")\n",
    "            \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': current_code,\n",
    "            'is_valid': is_valid,\n",
    "            'error_messages': [] if is_valid else [\"Validation failed\"],\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState):\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        \n",
    "        print(f\"correction - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_corrector:\n",
    "            # Get original requirements from the first human message in the chain\n",
    "            original_requirements = \"\"\n",
    "            for msg in state['messages']:\n",
    "                if isinstance(msg, HumanMessage) and msg.content:\n",
    "                    original_requirements = msg.content\n",
    "                    break\n",
    "            \n",
    "            # Get validation feedback from the most recent message\n",
    "            validation_feedback = messages[0].content if messages else \"\"\n",
    "            \n",
    "            # Note: Using exact case from the prompt template\n",
    "            formatted_prompt = self.system_corrector.format(\n",
    "                requirements=original_requirements,  # Changed from Requirements to requirements\n",
    "                ValidationFeedback=validation_feedback\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save corrected code\n",
    "        self.save_code_attempt(code_only, user_story_id, \"correction\")\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'is_valid': False,\n",
    "            'error_messages': [],\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "def process_tech_specs(excel_file_path=\"tech.xlsx\"):\n",
    "    \"\"\"\n",
    "    Process tech specs from an Excel file\n",
    "    \n",
    "    Args:\n",
    "        excel_file_path: Path to Excel file with tech specs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read tech specs from Excel\n",
    "        tech_specs = read_tech_specs_from_excel(excel_file_path)\n",
    "        \n",
    "        if not tech_specs:\n",
    "            logger.error(\"No tech specs found in Excel file\")\n",
    "            return\n",
    "        \n",
    "        # Model initialization\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "        )\n",
    "        \n",
    "        # Create a base output directory\n",
    "        base_output_dir = f\"code_generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Initialize code generator with the base directory\n",
    "        code_gen = CodeGenerator(\n",
    "            model=model, \n",
    "            base_output_dir=base_output_dir,\n",
    "            system_developer=developer_prompt,\n",
    "            system_validator=validator_prompt,\n",
    "            system_corrector=corrector_prompt\n",
    "        )\n",
    "        \n",
    "        # Process each tech spec\n",
    "        for idx, spec in enumerate(tech_specs):\n",
    "            user_story_id = spec['user_story_id']\n",
    "            tech_spec = spec['tech_spec']\n",
    "            \n",
    "            logger.info(f\"Processing tech spec for user story ID: {user_story_id} ({idx+1}/{len(tech_specs)})\")\n",
    "            \n",
    "            # Setup initial message\n",
    "            messages = [HumanMessage(content=tech_spec)]\n",
    "            \n",
    "            # Set up the input state\n",
    "            initial_state = {\n",
    "                \"messages\": messages,\n",
    "                \"current_code\": \"\",\n",
    "                \"validation_status\": None,\n",
    "                \"error_messages\": [],\n",
    "                \"is_valid\": False,\n",
    "                \"user_story_id\": user_story_id\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Run the graph\n",
    "                result = code_gen.graph.invoke(initial_state)\n",
    "                \n",
    "                # Log success\n",
    "                logger.info(f\"Successfully processed tech spec for user story ID: {user_story_id}\")\n",
    "                \n",
    "                # Extract final code\n",
    "                if 'current_code' in result and result['current_code']:\n",
    "                    final_status = \"final_corrected\" if not result.get('is_valid', False) else \"final_validated\"\n",
    "                    code_gen.save_code_attempt(result['current_code'], user_story_id, final_status)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing tech spec for user story ID {user_story_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        logger.info(f\"Completed processing all tech specs. Output directory: {base_output_dir}\")\n",
    "        return base_output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_tech_specs: {e}\")\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    output_dir = process_tech_specs(\"tech.xlsx\")\n",
    "    print(f\"All processing complete. Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:12:32,416 - __main__ - INFO - [234747294.py:318] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-06 15:12:32,416 - root - INFO - [234747294.py:56] - Using 'final_corrected' code for US_145\n",
      "2025-03-06 15:12:32,416 - root - INFO - [234747294.py:56] - Using 'initial' code for US_147\n",
      "2025-03-06 15:12:32,416 - root - INFO - [234747294.py:56] - Using 'final_corrected' code for US_146\n",
      "2025-03-06 15:12:32,416 - root - INFO - [234747294.py:56] - Using 'final_corrected' code for US_141\n",
      "2025-03-06 15:12:32,416 - root - INFO - [234747294.py:56] - Using 'final_corrected' code for US_142\n",
      "2025-03-06 15:12:32,417 - root - INFO - [234747294.py:56] - Using 'final_corrected' code for US_144\n",
      "2025-03-06 15:12:32,417 - root - INFO - [234747294.py:56] - Using 'initial' code for US_143\n",
      "2025-03-06 15:12:32,417 - __main__ - INFO - [234747294.py:322] - Found 7 code files to combine\n",
      "2025-03-06 15:12:32,418 - __main__ - INFO - [234747294.py:338] - Successfully combined code files into: /home/airangers/Desktop/shivani/SDLC/combined_solution_20250306_151232.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined solution created at: /home/airangers/Desktop/shivani/SDLC/combined_solution_20250306_151232.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code Combiner Agent\n",
    "\n",
    "This script finds the latest code_generation folder, extracts the corrected code from each\n",
    "user story subfolder, and combines them into a single cohesive Python file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_latest_code_generation_folder():\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_corrected_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all corrected code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logging.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logging.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "class CodeSection:\n",
    "    \"\"\"Base class for code sections.\"\"\"\n",
    "    def __init__(self, content, module_name):\n",
    "        self.content = content\n",
    "        self.module_name = module_name\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"# From {self.module_name}\\n{self.content}\"\n",
    "\n",
    "class ImportSection(CodeSection):\n",
    "    \"\"\"Section for import statements.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ClassSection(CodeSection):\n",
    "    \"\"\"Section for class definitions.\"\"\"\n",
    "    def __init__(self, name, content, module_name):\n",
    "        super().__init__(content, module_name)\n",
    "        self.name = name\n",
    "\n",
    "class FunctionSection(CodeSection):\n",
    "    \"\"\"Section for function definitions.\"\"\"\n",
    "    def __init__(self, name, content, module_name):\n",
    "        super().__init__(content, module_name)\n",
    "        self.name = name\n",
    "\n",
    "class VariableSection(CodeSection):\n",
    "    \"\"\"Section for global variable definitions.\"\"\"\n",
    "    def __init__(self, name, content, module_name):\n",
    "        super().__init__(content, module_name)\n",
    "        self.name = name\n",
    "\n",
    "class OtherSection(CodeSection):\n",
    "    \"\"\"Section for other code.\"\"\"\n",
    "    pass\n",
    "\n",
    "class CodeAnalyzer:\n",
    "    \"\"\"Analyzes Python code files to extract components.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.imports = []\n",
    "        self.classes = []\n",
    "        self.functions = []\n",
    "        self.variables = []\n",
    "        self.other_code = []\n",
    "\n",
    "    def analyze_file(self, file_path, module_name):\n",
    "        \"\"\"Analyze a Python file and extract its components.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Split the file into lines for processing\n",
    "        lines = content.splitlines()\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Import statements\n",
    "            if line.startswith('import ') or line.startswith('from '):\n",
    "                self.imports.append(ImportSection(line, module_name))\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Class definitions\n",
    "            if line.startswith('class '):\n",
    "                class_name = line.split('class ')[1].split('(')[0].strip().split(':')[0].strip()\n",
    "                class_lines = [lines[i]]\n",
    "                i += 1\n",
    "                \n",
    "                # Get indentation level of the class body\n",
    "                indent_level = None\n",
    "                while i < len(lines):\n",
    "                    if lines[i].strip():  # Non-empty line\n",
    "                        current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                        if indent_level is None:\n",
    "                            indent_level = current_indent\n",
    "                            if indent_level == 0:  # No indentation means we're out of the class\n",
    "                                break\n",
    "                        elif current_indent < indent_level:  # We've gone back to a lower indentation\n",
    "                            break\n",
    "                    \n",
    "                    class_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                self.classes.append(ClassSection(class_name, '\\n'.join(class_lines), module_name))\n",
    "                continue\n",
    "            \n",
    "            # Function definitions\n",
    "            if line.startswith('def '):\n",
    "                function_name = line.split('def ')[1].split('(')[0].strip()\n",
    "                function_lines = [lines[i]]\n",
    "                i += 1\n",
    "                \n",
    "                # Get indentation level of the function body\n",
    "                indent_level = None\n",
    "                while i < len(lines):\n",
    "                    if lines[i].strip():  # Non-empty line\n",
    "                        current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                        if indent_level is None:\n",
    "                            indent_level = current_indent\n",
    "                            if indent_level == 0:  # No indentation means we're out of the function\n",
    "                                break\n",
    "                        elif current_indent < indent_level:  # We've gone back to a lower indentation\n",
    "                            break\n",
    "                    \n",
    "                    function_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                self.functions.append(FunctionSection(function_name, '\\n'.join(function_lines), module_name))\n",
    "                continue\n",
    "            \n",
    "            # Global variable assignments\n",
    "            var_match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_]*)\\s*=', line)\n",
    "            if var_match and not line.startswith(' ') and not line.startswith('\\t'):\n",
    "                var_name = var_match.group(1)\n",
    "                self.variables.append(VariableSection(var_name, line, module_name))\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Other code\n",
    "            if not (line.startswith(' ') or line.startswith('\\t')):\n",
    "                # Check if it's an if __name__ == \"__main__\" block\n",
    "                if line.startswith('if __name__ == ') and ('\"__main__\"' in line or \"'__main__'\" in line):\n",
    "                    # Skip the whole block\n",
    "                    i += 1\n",
    "                    indent_level = None\n",
    "                    while i < len(lines):\n",
    "                        if lines[i].strip():  # Non-empty line\n",
    "                            current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                            if indent_level is None:\n",
    "                                indent_level = current_indent\n",
    "                                if indent_level == 0:  # No indentation means we're out of the block\n",
    "                                    break\n",
    "                            elif current_indent < indent_level:  # We've gone back to a lower indentation\n",
    "                                break\n",
    "                        i += 1\n",
    "                    continue\n",
    "                \n",
    "                self.other_code.append(OtherSection(line, module_name))\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # If we get here, just move to the next line\n",
    "            i += 1\n",
    "\n",
    "class CodeCombiner:\n",
    "    \"\"\"Combines analyzed code sections into a single file.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = CodeAnalyzer()\n",
    "    \n",
    "    def process_files(self, code_files):\n",
    "        \"\"\"Process all code files and extract their components.\"\"\"\n",
    "        for module_name, file_path in code_files:\n",
    "            try:\n",
    "                self.analyzer.analyze_file(file_path, module_name)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    def get_combined_code(self):\n",
    "        \"\"\"Generate the combined code content.\"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Add file header\n",
    "        sections.append('\"\"\"')\n",
    "        sections.append('Combined Solution')\n",
    "        sections.append('This file was automatically generated by combining multiple code files.')\n",
    "        sections.append(f'Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        sections.append('\"\"\"')\n",
    "        sections.append(\"\")\n",
    "        \n",
    "        # Add imports\n",
    "        unique_imports = set()\n",
    "        import_lines = []\n",
    "        for imp in self.analyzer.imports:\n",
    "            if imp.content not in unique_imports:\n",
    "                unique_imports.add(imp.content)\n",
    "                import_lines.append(imp.content)\n",
    "        \n",
    "        sections.append(\"# Imports\")\n",
    "        sections.extend(sorted(import_lines))\n",
    "        sections.append(\"\")\n",
    "        \n",
    "        # Add global variables\n",
    "        if self.analyzer.variables:\n",
    "            sections.append(\"# Global variables\")\n",
    "            for var in self.analyzer.variables:\n",
    "                sections.append(str(var))\n",
    "            sections.append(\"\")\n",
    "        \n",
    "        # Add classes\n",
    "        if self.analyzer.classes:\n",
    "            sections.append(\"# Classes\")\n",
    "            for cls in self.analyzer.classes:\n",
    "                sections.append(str(cls))\n",
    "                sections.append(\"\")\n",
    "        \n",
    "        # Add functions\n",
    "        if self.analyzer.functions:\n",
    "            sections.append(\"# Functions\")\n",
    "            for func in self.analyzer.functions:\n",
    "                sections.append(str(func))\n",
    "                sections.append(\"\")\n",
    "        \n",
    "        # Add other code\n",
    "        if self.analyzer.other_code:\n",
    "            sections.append(\"# Other code\")\n",
    "            for code in self.analyzer.other_code:\n",
    "                sections.append(str(code))\n",
    "            sections.append(\"\")\n",
    "        \n",
    "        # Add main block\n",
    "        sections.append(\"# Main execution\")\n",
    "        sections.append('if __name__ == \"__main__\":')\n",
    "        sections.append('    # Combined execution of all modules')\n",
    "        sections.append('    try:')\n",
    "        sections.append('        print(\"Running combined solution...\")')\n",
    "        \n",
    "        # Check if we have main functions\n",
    "        main_function_names = [func.name for func in self.analyzer.functions \n",
    "                             if func.name in ('main', 'process_tech_specs')]\n",
    "        if main_function_names:\n",
    "            for func_name in main_function_names:\n",
    "                if func_name == 'process_tech_specs':\n",
    "                    sections.append(f'        {func_name}(\"tech.xlsx\")')\n",
    "                else:\n",
    "                    sections.append(f'        {func_name}()')\n",
    "        \n",
    "        sections.append('    except Exception as e:')\n",
    "        sections.append('        logging.error(f\"Error running combined solution: {e}\")')\n",
    "        sections.append('        raise')\n",
    "        \n",
    "        return '\\n'.join(sections)\n",
    "\n",
    "def combine_code_agent():\n",
    "    \"\"\"\n",
    "    Agent that combines all corrected code files from the latest code_generation folder\n",
    "    into a single cohesive file that works flawlessly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Find the latest code_generation folder\n",
    "        latest_folder = find_latest_code_generation_folder()\n",
    "        logger.info(f\"Found latest code generation folder: {latest_folder}\")\n",
    "        \n",
    "        # Find all corrected code files\n",
    "        code_files = find_corrected_code_files(latest_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to combine\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to combine\")\n",
    "            return\n",
    "        \n",
    "        # Combine the code files\n",
    "        combiner = CodeCombiner()\n",
    "        combiner.process_files(code_files)\n",
    "        combined_code = combiner.get_combined_code()\n",
    "        \n",
    "        # Write the combined code\n",
    "        output_file = os.path.join(os.path.dirname(latest_folder), f\"combined_solution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(combined_code)\n",
    "        \n",
    "        logger.info(f\"Successfully combined code files into: {output_file}\")\n",
    "        print(f\"Combined solution created at: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining code files: {e}\")\n",
    "        raise\n",
    "\n",
    "# If this script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    combine_code_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:16:21,149 - __main__ - INFO - [3095127349.py:720] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-06 15:16:21,149 - __main__ - INFO - [3095127349.py:300] - Using 'final_corrected' code for US_145\n",
      "2025-03-06 15:16:21,150 - __main__ - INFO - [3095127349.py:300] - Using 'initial' code for US_147\n",
      "2025-03-06 15:16:21,150 - __main__ - INFO - [3095127349.py:300] - Using 'final_corrected' code for US_146\n",
      "2025-03-06 15:16:21,150 - __main__ - INFO - [3095127349.py:300] - Using 'final_corrected' code for US_141\n",
      "2025-03-06 15:16:21,150 - __main__ - INFO - [3095127349.py:300] - Using 'final_corrected' code for US_142\n",
      "2025-03-06 15:16:21,150 - __main__ - INFO - [3095127349.py:300] - Using 'final_corrected' code for US_144\n",
      "2025-03-06 15:16:21,151 - __main__ - INFO - [3095127349.py:300] - Using 'initial' code for US_143\n",
      "2025-03-06 15:16:21,151 - __main__ - INFO - [3095127349.py:724] - Found 7 code files to combine\n",
      "2025-03-06 15:16:21,151 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,151 - __main__ - ERROR - [3095127349.py:353] - Syntax error in file /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 15:16:21,151 - __main__ - INFO - [3095127349.py:362] - Using fallback parser for /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,152 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_147/attempt_initial/code.py\n",
      "2025-03-06 15:16:21,154 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,154 - __main__ - ERROR - [3095127349.py:353] - Syntax error in file /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 15:16:21,154 - __main__ - INFO - [3095127349.py:362] - Using fallback parser for /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,155 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,156 - __main__ - ERROR - [3095127349.py:353] - Syntax error in file /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py: parameter without a default follows parameter with a default (<unknown>, line 163)\n",
      "2025-03-06 15:16:21,156 - __main__ - INFO - [3095127349.py:362] - Using fallback parser for /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,156 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,156 - __main__ - ERROR - [3095127349.py:353] - Syntax error in file /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 15:16:21,156 - __main__ - INFO - [3095127349.py:362] - Using fallback parser for /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,157 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-06 15:16:21,159 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: List in US_144\n",
      "2025-03-06 15:16:21,159 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: Dict in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: Image in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: os in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: logging in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: ThreadPoolExecutor in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: ValidationResult in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: logger in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: app in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: SUPPORTED_FORMATS in US_144\n",
      "2025-03-06 15:16:21,160 - __main__ - INFO - [3095127349.py:326] - Analyzing file: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_143/attempt_initial/code.py\n",
      "2025-03-06 15:16:21,162 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: FastAPI in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: HTTPException in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: List in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: Dict in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: BaseModel in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: asyncio in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: os in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: hashlib in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: logging in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: Image in US_143\n",
      "2025-03-06 15:16:21,163 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: pydicom in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: json in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: datetime in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: LOG_FILE in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: app in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: retrieved_images in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: format in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: patient_id in US_143\n",
      "2025-03-06 15:16:21,164 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: headers in US_143\n",
      "2025-03-06 15:16:21,165 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: response in US_143\n",
      "2025-03-06 15:16:21,165 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: file_path in US_143\n",
      "2025-03-06 15:16:21,165 - __main__ - WARNING - [3095127349.py:349] - Duplicate entity name: format in US_143\n",
      "2025-03-06 15:16:21,165 - __main__ - INFO - [3095127349.py:477] - Building dependency graph\n",
      "2025-03-06 15:16:21,166 - __main__ - INFO - [3095127349.py:605] - Generating combined code\n",
      "2025-03-06 15:16:21,166 - __main__ - INFO - [3095127349.py:556] - Resolving naming conflicts\n",
      "2025-03-06 15:16:21,166 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: ValidationResult appears 2 times\n",
      "2025-03-06 15:16:21,166 - __main__ - INFO - [3095127349.py:601] - Renamed ValidationResult to ValidationResult_141 in US_141\n",
      "2025-03-06 15:16:21,166 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: main appears 2 times\n",
      "2025-03-06 15:16:21,166 - __main__ - INFO - [3095127349.py:601] - Renamed main to main_142 in US_142\n",
      "2025-03-06 15:16:21,167 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: app appears 5 times\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed app to app_145 in US_145\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed app to app_141 in US_141\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed app to app_144 in US_144\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed app to app_143 in US_143\n",
      "2025-03-06 15:16:21,167 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: logger appears 3 times\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed logger to logger_144 in US_144\n",
      "2025-03-06 15:16:21,167 - __main__ - INFO - [3095127349.py:601] - Renamed logger to logger_142 in US_142\n",
      "2025-03-06 15:16:21,167 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: level appears 2 times\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed level to level_142 in US_142\n",
      "2025-03-06 15:16:21,168 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: format appears 4 times\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed format to format_143 in US_143\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed format to format_143 in US_143\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed format to format_142 in US_142\n",
      "2025-03-06 15:16:21,168 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: handlers appears 2 times\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed handlers to handlers_142 in US_142\n",
      "2025-03-06 15:16:21,168 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: temp_dir appears 2 times\n",
      "2025-03-06 15:16:21,168 - __main__ - INFO - [3095127349.py:601] - Renamed temp_dir to temp_dir_141 in US_141\n",
      "2025-03-06 15:16:21,168 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: LOG_FILE appears 2 times\n",
      "2025-03-06 15:16:21,169 - __main__ - INFO - [3095127349.py:601] - Renamed LOG_FILE to LOG_FILE_143 in US_143\n",
      "2025-03-06 15:16:21,169 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: SUPPORTED_FORMATS appears 2 times\n",
      "2025-03-06 15:16:21,169 - __main__ - INFO - [3095127349.py:601] - Renamed SUPPORTED_FORMATS to SUPPORTED_FORMATS_144 in US_144\n",
      "2025-03-06 15:16:21,169 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: patient_id appears 2 times\n",
      "2025-03-06 15:16:21,169 - __main__ - INFO - [3095127349.py:601] - Renamed patient_id to patient_id_143 in US_143\n",
      "2025-03-06 15:16:21,169 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: retrieved_images appears 2 times\n",
      "2025-03-06 15:16:21,169 - __main__ - INFO - [3095127349.py:601] - Renamed retrieved_images to retrieved_images_143 in US_143\n",
      "2025-03-06 15:16:21,169 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: headers appears 2 times\n",
      "2025-03-06 15:16:21,170 - __main__ - INFO - [3095127349.py:601] - Renamed headers to headers_143 in US_143\n",
      "2025-03-06 15:16:21,170 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: response appears 2 times\n",
      "2025-03-06 15:16:21,170 - __main__ - INFO - [3095127349.py:601] - Renamed response to response_143 in US_143\n",
      "2025-03-06 15:16:21,170 - __main__ - WARNING - [3095127349.py:566] - Conflict detected: file_path appears 2 times\n",
      "2025-03-06 15:16:21,170 - __main__ - INFO - [3095127349.py:601] - Renamed file_path to file_path_143 in US_143\n",
      "2025-03-06 15:16:21,170 - __main__ - INFO - [3095127349.py:512] - Sorting entities topologically\n",
      "2025-03-06 15:16:21,171 - __main__ - INFO - [3095127349.py:749] - Successfully created intelligent combined solution: /home/airangers/Desktop/shivani/SDLC/intelligent_solution_20250306_151621.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intelligent combined solution created at: /home/airangers/Desktop/shivani/SDLC/intelligent_solution_20250306_151621.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Intelligent Code Combiner Agent\n",
    "\n",
    "This script finds the latest code_generation folder, analyzes the corrected code from each\n",
    "user story subfolder, and intelligently combines them into a single cohesive Python file\n",
    "based on code dependencies and relationships.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import ast\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Set, Tuple, Any, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CodeEntity:\n",
    "    \"\"\"Base class for code entities (imports, classes, functions, variables).\"\"\"\n",
    "    def __init__(self, name: str, source_code: str, module_name: str, line_no: int):\n",
    "        self.name = name\n",
    "        self.source_code = source_code\n",
    "        self.module_name = module_name\n",
    "        self.line_no = line_no\n",
    "        self.dependencies = set()  # Names of other entities this entity depends on\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"# From {self.module_name}\\n{self.source_code}\"\n",
    "    \n",
    "    def add_dependency(self, name: str):\n",
    "        \"\"\"Add a dependency to this entity.\"\"\"\n",
    "        if name != self.name:  # Avoid self-dependency\n",
    "            self.dependencies.add(name)\n",
    "\n",
    "class ImportEntity(CodeEntity):\n",
    "    \"\"\"Class for import statements.\"\"\"\n",
    "    def __init__(self, name: str, source_code: str, module_name: str, line_no: int):\n",
    "        super().__init__(name, source_code, module_name, line_no)\n",
    "        self.imported_names = set()  # Names that are imported\n",
    "        \n",
    "    def add_imported_name(self, name: str):\n",
    "        \"\"\"Add a name that is imported by this import statement.\"\"\"\n",
    "        self.imported_names.add(name)\n",
    "\n",
    "class ClassEntity(CodeEntity):\n",
    "    \"\"\"Class for class definitions.\"\"\"\n",
    "    def __init__(self, name: str, source_code: str, module_name: str, line_no: int):\n",
    "        super().__init__(name, source_code, module_name, line_no)\n",
    "        self.methods = set()  # Method names defined in this class\n",
    "        self.superclasses = set()  # Names of superclasses\n",
    "        \n",
    "    def add_method(self, method_name: str):\n",
    "        \"\"\"Add a method name to this class.\"\"\"\n",
    "        self.methods.add(method_name)\n",
    "        \n",
    "    def add_superclass(self, superclass_name: str):\n",
    "        \"\"\"Add a superclass name to this class.\"\"\"\n",
    "        self.superclasses.add(superclass_name)\n",
    "        self.add_dependency(superclass_name)\n",
    "\n",
    "class FunctionEntity(CodeEntity):\n",
    "    \"\"\"Class for function definitions.\"\"\"\n",
    "    def __init__(self, name: str, source_code: str, module_name: str, line_no: int):\n",
    "        super().__init__(name, source_code, module_name, line_no)\n",
    "        self.calls = set()  # Function calls made by this function\n",
    "        self.parameters = set()  # Parameter names\n",
    "        self.return_values = set()  # Return value expressions\n",
    "        \n",
    "    def add_call(self, function_name: str):\n",
    "        \"\"\"Add a function call made by this function.\"\"\"\n",
    "        self.calls.add(function_name)\n",
    "        self.add_dependency(function_name)\n",
    "        \n",
    "    def add_parameter(self, param_name: str):\n",
    "        \"\"\"Add a parameter name to this function.\"\"\"\n",
    "        self.parameters.add(param_name)\n",
    "        \n",
    "    def add_return_value(self, return_expr: str):\n",
    "        \"\"\"Add a return value expression to this function.\"\"\"\n",
    "        self.return_values.add(return_expr)\n",
    "\n",
    "class VariableEntity(CodeEntity):\n",
    "    \"\"\"Class for global variable definitions.\"\"\"\n",
    "    def __init__(self, name: str, source_code: str, module_name: str, line_no: int):\n",
    "        super().__init__(name, source_code, module_name, line_no)\n",
    "        self.value = None  # Value expression\n",
    "        \n",
    "    def set_value(self, value: str):\n",
    "        \"\"\"Set the value expression of this variable.\"\"\"\n",
    "        self.value = value\n",
    "\n",
    "class OtherEntity(CodeEntity):\n",
    "    \"\"\"Class for other code snippets.\"\"\"\n",
    "    pass\n",
    "\n",
    "class DependencyVisitor(ast.NodeVisitor):\n",
    "    \"\"\"AST visitor to extract dependencies from code.\"\"\"\n",
    "    \n",
    "    def __init__(self, module_name: str):\n",
    "        self.module_name = module_name\n",
    "        self.imports = []\n",
    "        self.classes = []\n",
    "        self.functions = []\n",
    "        self.variables = []\n",
    "        self.other_code = []\n",
    "        \n",
    "        # Track names that are in scope\n",
    "        self.defined_names = set()\n",
    "        # Track the current class or function being visited\n",
    "        self.current_context = None\n",
    "        # Track used names to determine dependencies\n",
    "        self.used_names = defaultdict(set)\n",
    "        \n",
    "    def visit_Import(self, node):\n",
    "        \"\"\"Visit an import statement.\"\"\"\n",
    "        for name in node.names:\n",
    "            alias = name.asname or name.name\n",
    "            source_code = ast.unparse(node)\n",
    "            import_entity = ImportEntity(alias, source_code, self.module_name, node.lineno)\n",
    "            import_entity.add_imported_name(name.name)\n",
    "            self.imports.append(import_entity)\n",
    "            self.defined_names.add(alias)\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def visit_ImportFrom(self, node):\n",
    "        \"\"\"Visit a from ... import ... statement.\"\"\"\n",
    "        source_code = ast.unparse(node)\n",
    "        module_name = node.module if node.module else \"\"\n",
    "        \n",
    "        for name in node.names:\n",
    "            alias = name.asname or name.name\n",
    "            import_entity = ImportEntity(alias, source_code, self.module_name, node.lineno)\n",
    "            import_entity.add_imported_name(f\"{module_name}.{name.name}\" if module_name else name.name)\n",
    "            self.imports.append(import_entity)\n",
    "            self.defined_names.add(alias)\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def visit_ClassDef(self, node):\n",
    "        \"\"\"Visit a class definition.\"\"\"\n",
    "        # Get the source code for the class\n",
    "        source_code = ast.unparse(node)\n",
    "        class_entity = ClassEntity(node.name, source_code, self.module_name, node.lineno)\n",
    "        \n",
    "        # Track superclasses\n",
    "        for base in node.bases:\n",
    "            if isinstance(base, ast.Name):\n",
    "                class_entity.add_superclass(base.id)\n",
    "                \n",
    "        # Save the previous context and set the current context to this class\n",
    "        prev_context = self.current_context\n",
    "        self.current_context = class_entity\n",
    "        \n",
    "        # Visit child nodes\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "        # Restore the previous context\n",
    "        self.current_context = prev_context\n",
    "        \n",
    "        self.classes.append(class_entity)\n",
    "        self.defined_names.add(node.name)\n",
    "        \n",
    "    def visit_FunctionDef(self, node):\n",
    "        \"\"\"Visit a function definition.\"\"\"\n",
    "        # Get the source code for the function\n",
    "        source_code = ast.unparse(node)\n",
    "        function_entity = FunctionEntity(node.name, source_code, self.module_name, node.lineno)\n",
    "        \n",
    "        # Add function parameters\n",
    "        for arg in node.args.args:\n",
    "            function_entity.add_parameter(arg.arg)\n",
    "            \n",
    "        # Check if this is a method of a class\n",
    "        if isinstance(self.current_context, ClassEntity):\n",
    "            self.current_context.add_method(node.name)\n",
    "            # For class methods, we don't add them to the global namespace\n",
    "        else:\n",
    "            self.functions.append(function_entity)\n",
    "            self.defined_names.add(node.name)\n",
    "            \n",
    "        # Save the previous context and set the current context to this function\n",
    "        prev_context = self.current_context\n",
    "        self.current_context = function_entity\n",
    "            \n",
    "        # Visit child nodes\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "        # Restore the previous context\n",
    "        self.current_context = prev_context\n",
    "        \n",
    "    def visit_Assign(self, node):\n",
    "        \"\"\"Visit an assignment statement.\"\"\"\n",
    "        # Only process global variable assignments\n",
    "        if not isinstance(self.current_context, (ClassEntity, FunctionEntity)):\n",
    "            # Get the target names\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "                    source_code = ast.unparse(node)\n",
    "                    var_entity = VariableEntity(target.id, source_code, self.module_name, node.lineno)\n",
    "                    self.variables.append(var_entity)\n",
    "                    self.defined_names.add(target.id)\n",
    "        \n",
    "        # Visit child nodes to find dependencies\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def visit_Call(self, node):\n",
    "        \"\"\"Visit a function call.\"\"\"\n",
    "        # Extract the function being called\n",
    "        if isinstance(node.func, ast.Name):\n",
    "            func_name = node.func.id\n",
    "            # Record the dependency in the current context\n",
    "            if isinstance(self.current_context, (FunctionEntity, ClassEntity)):\n",
    "                self.used_names[self.current_context.name].add(func_name)\n",
    "                if isinstance(self.current_context, FunctionEntity):\n",
    "                    self.current_context.add_call(func_name)\n",
    "        elif isinstance(node.func, ast.Attribute):\n",
    "            # Handle method calls like obj.method()\n",
    "            if isinstance(node.func.value, ast.Name):\n",
    "                # Record the use of the object\n",
    "                if isinstance(self.current_context, (FunctionEntity, ClassEntity)):\n",
    "                    self.used_names[self.current_context.name].add(node.func.value.id)\n",
    "        \n",
    "        # Visit child nodes\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def visit_Name(self, node):\n",
    "        \"\"\"Visit a name (variable use).\"\"\"\n",
    "        # Record name usage in the current context\n",
    "        if isinstance(self.current_context, (FunctionEntity, ClassEntity)):\n",
    "            # Only record names that are loaded (used), not stored or deleted\n",
    "            if isinstance(node.ctx, ast.Load):\n",
    "                self.used_names[self.current_context.name].add(node.id)\n",
    "                \n",
    "        # Visit child nodes\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "    def process_dependencies(self):\n",
    "        \"\"\"Process the collected information to establish dependencies.\"\"\"\n",
    "        # Add dependencies to functions based on name usage\n",
    "        for entity in self.functions:\n",
    "            for used_name in self.used_names.get(entity.name, set()):\n",
    "                if used_name in self.defined_names and used_name != entity.name:\n",
    "                    entity.add_dependency(used_name)\n",
    "                    \n",
    "        # Add dependencies to classes based on name usage\n",
    "        for entity in self.classes:\n",
    "            for used_name in self.used_names.get(entity.name, set()):\n",
    "                if used_name in self.defined_names and used_name != entity.name:\n",
    "                    entity.add_dependency(used_name)\n",
    "\n",
    "\n",
    "def find_latest_code_generation_folder():\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logger.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logger.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "class IntelligentCodeCombiner:\n",
    "    \"\"\"Intelligently analyzes and combines Python code files.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.all_imports = []  # List of ImportEntity objects\n",
    "        self.all_classes = []  # List of ClassEntity objects\n",
    "        self.all_functions = []  # List of FunctionEntity objects\n",
    "        self.all_variables = []  # List of VariableEntity objects\n",
    "        self.all_other = []  # List of OtherEntity objects\n",
    "        \n",
    "        # Maps for quick lookup\n",
    "        self.entity_map = {}  # name -> entity\n",
    "        \n",
    "        # Dependency graph\n",
    "        self.dependency_graph = nx.DiGraph()\n",
    "        \n",
    "    def analyze_file(self, file_path: str, module_name: str):\n",
    "        \"\"\"Analyze a Python file and extract its components with dependency information.\"\"\"\n",
    "        logger.info(f\"Analyzing file: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                code = f.read()\n",
    "            \n",
    "            try:\n",
    "                # Parse the AST\n",
    "                tree = ast.parse(code)\n",
    "                \n",
    "                # Visit the AST to extract entities and dependencies\n",
    "                visitor = DependencyVisitor(module_name)\n",
    "                visitor.visit(tree)\n",
    "                visitor.process_dependencies()\n",
    "                \n",
    "                # Collect the extracted entities\n",
    "                self.all_imports.extend(visitor.imports)\n",
    "                self.all_classes.extend(visitor.classes)\n",
    "                self.all_functions.extend(visitor.functions)\n",
    "                self.all_variables.extend(visitor.variables)\n",
    "                \n",
    "                # Update the entity map\n",
    "                for entity in visitor.imports + visitor.classes + visitor.functions + visitor.variables:\n",
    "                    if entity.name in self.entity_map:\n",
    "                        logger.warning(f\"Duplicate entity name: {entity.name} in {module_name}\")\n",
    "                    self.entity_map[entity.name] = entity\n",
    "                \n",
    "            except SyntaxError as e:\n",
    "                logger.error(f\"Syntax error in file {file_path}: {e}\")\n",
    "                # Fallback to a simpler parsing approach for files with syntax errors\n",
    "                self._simple_parse_fallback(code, file_path, module_name)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing file {file_path}: {e}\")\n",
    "    \n",
    "    def _simple_parse_fallback(self, code: str, file_path: str, module_name: str):\n",
    "        \"\"\"Fallback method for parsing files that have syntax errors.\"\"\"\n",
    "        logger.info(f\"Using fallback parser for {file_path}\")\n",
    "        lines = code.splitlines()\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Extract imports\n",
    "            if line.startswith('import ') or line.startswith('from '):\n",
    "                import_entity = ImportEntity(\n",
    "                    \"import_\" + str(i),  # Generate a placeholder name\n",
    "                    line,\n",
    "                    module_name,\n",
    "                    i + 1\n",
    "                )\n",
    "                self.all_imports.append(import_entity)\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Extract class definitions\n",
    "            if line.startswith('class '):\n",
    "                class_name = line.split('class ')[1].split('(')[0].strip().split(':')[0].strip()\n",
    "                # Collect all lines of the class\n",
    "                class_lines = [lines[i]]\n",
    "                i += 1\n",
    "                \n",
    "                # Get indentation level of the class body\n",
    "                indent_level = None\n",
    "                while i < len(lines):\n",
    "                    if lines[i].strip():  # Non-empty line\n",
    "                        current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                        if indent_level is None:\n",
    "                            indent_level = current_indent\n",
    "                            if indent_level == 0:  # No indentation means we're out of the class\n",
    "                                break\n",
    "                        elif current_indent < indent_level:  # We've gone back to a lower indentation\n",
    "                            break\n",
    "                    \n",
    "                    class_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                class_entity = ClassEntity(\n",
    "                    class_name,\n",
    "                    '\\n'.join(class_lines),\n",
    "                    module_name,\n",
    "                    i + 1\n",
    "                )\n",
    "                self.all_classes.append(class_entity)\n",
    "                self.entity_map[class_name] = class_entity\n",
    "                continue\n",
    "                \n",
    "            # Extract function definitions\n",
    "            if line.startswith('def '):\n",
    "                function_name = line.split('def ')[1].split('(')[0].strip()\n",
    "                # Collect all lines of the function\n",
    "                function_lines = [lines[i]]\n",
    "                i += 1\n",
    "                \n",
    "                # Get indentation level of the function body\n",
    "                indent_level = None\n",
    "                while i < len(lines):\n",
    "                    if lines[i].strip():  # Non-empty line\n",
    "                        current_indent = len(lines[i]) - len(lines[i].lstrip())\n",
    "                        if indent_level is None:\n",
    "                            indent_level = current_indent\n",
    "                            if indent_level == 0:  # No indentation means we're out of the function\n",
    "                                break\n",
    "                        elif current_indent < indent_level:  # We've gone back to a lower indentation\n",
    "                            break\n",
    "                    \n",
    "                    function_lines.append(lines[i])\n",
    "                    i += 1\n",
    "                \n",
    "                function_entity = FunctionEntity(\n",
    "                    function_name,\n",
    "                    '\\n'.join(function_lines),\n",
    "                    module_name,\n",
    "                    i + 1\n",
    "                )\n",
    "                self.all_functions.append(function_entity)\n",
    "                self.entity_map[function_name] = function_entity\n",
    "                continue\n",
    "                \n",
    "            # Global variable assignments\n",
    "            var_match = re.match(r'^([a-zA-Z_][a-zA-Z0-9_]*)\\s*=', line)\n",
    "            if var_match and not line.startswith(' ') and not line.startswith('\\t'):\n",
    "                var_name = var_match.group(1)\n",
    "                var_entity = VariableEntity(\n",
    "                    var_name,\n",
    "                    line,\n",
    "                    module_name,\n",
    "                    i + 1\n",
    "                )\n",
    "                self.all_variables.append(var_entity)\n",
    "                self.entity_map[var_name] = var_entity\n",
    "                i += 1\n",
    "                continue\n",
    "                \n",
    "            # Other code\n",
    "            other_entity = OtherEntity(\n",
    "                f\"other_{i}\",\n",
    "                line,\n",
    "                module_name,\n",
    "                i + 1\n",
    "            )\n",
    "            self.all_other.append(other_entity)\n",
    "            i += 1\n",
    "    \n",
    "    def build_dependency_graph(self):\n",
    "        \"\"\"Build a dependency graph from the analyzed code entities.\"\"\"\n",
    "        logger.info(\"Building dependency graph\")\n",
    "        \n",
    "        # Add nodes to the graph\n",
    "        for entity in list(self.entity_map.values()):\n",
    "            self.dependency_graph.add_node(entity.name, entity=entity)\n",
    "        \n",
    "        # Add edges based on dependencies\n",
    "        for entity in list(self.entity_map.values()):\n",
    "            for dep_name in entity.dependencies:\n",
    "                if dep_name in self.entity_map:\n",
    "                    self.dependency_graph.add_edge(dep_name, entity.name)\n",
    "        \n",
    "        # Special handling for imports: they should come first\n",
    "        for import_entity in self.all_imports:\n",
    "            # Create a unique identifier for this import statement\n",
    "            import_id = f\"import_{import_entity.line_no}_{import_entity.module_name}\"\n",
    "            \n",
    "            # Add it to the graph if not already there\n",
    "            if import_id not in self.dependency_graph:\n",
    "                self.dependency_graph.add_node(import_id, entity=import_entity)\n",
    "            \n",
    "            # Add edges from imports to any entities that might use them\n",
    "            for entity_name, entity in self.entity_map.items():\n",
    "                # Skip imports themselves\n",
    "                if isinstance(entity, ImportEntity):\n",
    "                    continue\n",
    "                    \n",
    "                # If the import brings in a name that's used in the entity's dependencies,\n",
    "                # add an edge from the import to the entity\n",
    "                for imported_name in import_entity.imported_names:\n",
    "                    if imported_name in entity.dependencies:\n",
    "                        self.dependency_graph.add_edge(import_id, entity_name)\n",
    "    \n",
    "    def sort_entities_topologically(self):\n",
    "        \"\"\"Sort the entities topologically based on dependencies.\"\"\"\n",
    "        logger.info(\"Sorting entities topologically\")\n",
    "        \n",
    "        # Make sure the graph is built\n",
    "        if not self.dependency_graph.nodes():\n",
    "            self.build_dependency_graph()\n",
    "        \n",
    "        # Try to sort topologically\n",
    "        try:\n",
    "            sorted_names = list(nx.topological_sort(self.dependency_graph))\n",
    "            # Reverse the order so that dependencies come before dependents\n",
    "            sorted_names.reverse()\n",
    "            \n",
    "            # Create the sorted entity list\n",
    "            sorted_entities = []\n",
    "            for name in sorted_names:\n",
    "                if name in self.dependency_graph.nodes:\n",
    "                    entity = self.dependency_graph.nodes[name]['entity']\n",
    "                    sorted_entities.append(entity)\n",
    "            \n",
    "            return sorted_entities\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            # If there's a cycle, we can't sort topologically\n",
    "            logger.warning(\"Dependency graph has cycles. Using a heuristic sorting approach.\")\n",
    "            return self._heuristic_sort()\n",
    "    \n",
    "    def _heuristic_sort(self):\n",
    "        \"\"\"Use a heuristic approach to sort entities when topological sort fails.\"\"\"\n",
    "        # Define a priority order for entity types\n",
    "        # Imports first, then variables, classes, functions, and other code\n",
    "        sorted_entities = self.all_imports + self.all_variables + self.all_classes + self.all_functions + self.all_other\n",
    "        \n",
    "        # Try to identify entry points (functions that are not called by others)\n",
    "        entry_points = [entity for entity in self.all_functions if not any(entity.name in dep_entity.dependencies for dep_entity in self.all_functions)]\n",
    "        \n",
    "        # Move entry points to the end\n",
    "        for entry in entry_points:\n",
    "            if entry in sorted_entities:\n",
    "                sorted_entities.remove(entry)\n",
    "                sorted_entities.append(entry)\n",
    "        \n",
    "        return sorted_entities\n",
    "    \n",
    "    def resolve_conflicts(self):\n",
    "        \"\"\"Resolve naming conflicts between entities.\"\"\"\n",
    "        logger.info(\"Resolving naming conflicts\")\n",
    "        \n",
    "        # Count occurrences of each name\n",
    "        name_counts = Counter([entity.name for entity in \n",
    "                             self.all_classes + self.all_functions + self.all_variables])\n",
    "        \n",
    "        # Find names with multiple occurrences\n",
    "        conflicts = {name: count for name, count in name_counts.items() if count > 1}\n",
    "        \n",
    "        for name, count in conflicts.items():\n",
    "            logger.warning(f\"Conflict detected: {name} appears {count} times\")\n",
    "            \n",
    "            # Get all entities with this name\n",
    "            entities = [entity for entity in \n",
    "                      self.all_classes + self.all_functions + self.all_variables \n",
    "                      if entity.name == name]\n",
    "            \n",
    "            # Rename all but the most complex entity (assuming more complex = more important)\n",
    "            # We measure complexity by source code length as a simple heuristic\n",
    "            entities.sort(key=lambda e: len(e.source_code), reverse=True)\n",
    "            \n",
    "            # Keep the most complex entity as is\n",
    "            kept_entity = entities[0]\n",
    "            \n",
    "            # Rename the rest\n",
    "            for i, entity in enumerate(entities[1:], 1):\n",
    "                new_name = f\"{name}_{entity.module_name.replace('US_', '')}\"\n",
    "                \n",
    "                # Update the source code with the new name\n",
    "                entity.source_code = entity.source_code.replace(\n",
    "                    f\"def {name}(\", f\"def {new_name}(\", 1)\n",
    "                entity.source_code = entity.source_code.replace(\n",
    "                    f\"class {name}(\", f\"class {new_name}(\", 1)\n",
    "                entity.source_code = entity.source_code.replace(\n",
    "                    f\"{name} =\", f\"{new_name} =\", 1)\n",
    "                \n",
    "                # Update the entity name\n",
    "                old_name = entity.name\n",
    "                entity.name = new_name\n",
    "                \n",
    "                # Update the entity map\n",
    "                if old_name in self.entity_map:\n",
    "                    del self.entity_map[old_name]\n",
    "                self.entity_map[new_name] = entity\n",
    "                \n",
    "                logger.info(f\"Renamed {old_name} to {new_name} in {entity.module_name}\")\n",
    "    \n",
    "    def generate_combined_code(self):\n",
    "        \"\"\"Generate the combined code based on the dependency analysis.\"\"\"\n",
    "        logger.info(\"Generating combined code\")\n",
    "        \n",
    "        # Resolve naming conflicts\n",
    "        self.resolve_conflicts()\n",
    "        \n",
    "        # Sort entities\n",
    "        sorted_entities = self.sort_entities_topologically()\n",
    "        \n",
    "        # Prepare the output sections\n",
    "        sections = []\n",
    "        \n",
    "        # Add file header\n",
    "        sections.append('\"\"\"')\n",
    "        sections.append('Intelligently Combined Solution')\n",
    "        sections.append('This file was automatically generated by the Intelligent Code Combiner Agent.')\n",
    "        sections.append(f'Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "        sections.append('\"\"\"')\n",
    "        sections.append(\"\")\n",
    "        \n",
    "        # Add imports (deduplicated)\n",
    "        sections.append(\"# Imports\")\n",
    "        added_imports = set()\n",
    "        for entity in sorted_entities:\n",
    "            if isinstance(entity, ImportEntity):\n",
    "                # Deduplicate imports\n",
    "                if entity.source_code not in added_imports:\n",
    "                    added_imports.add(entity.source_code)\n",
    "                    sections.append(entity.source_code)\n",
    "        sections.append(\"\")\n",
    "        \n",
    "        # Add global variables\n",
    "        variable_entities = [e for e in sorted_entities if isinstance(e, VariableEntity)]\n",
    "        if variable_entities:\n",
    "            sections.append(\"# Global variables\")\n",
    "            for entity in variable_entities:\n",
    "                sections.append(f\"# From {entity.module_name}\")\n",
    "                sections.append(entity.source_code)\n",
    "            sections.append(\"\")\n",
    "        \n",
    "        # Add classes\n",
    "        class_entities = [e for e in sorted_entities if isinstance(e, ClassEntity)]\n",
    "        if class_entities:\n",
    "            sections.append(\"# Classes\")\n",
    "            for entity in class_entities:\n",
    "                sections.append(f\"# From {entity.module_name}\")\n",
    "                sections.append(entity.source_code)\n",
    "                sections.append(\"\")\n",
    "        \n",
    "        # Add functions\n",
    "        function_entities = [e for e in sorted_entities if isinstance(e, FunctionEntity)]\n",
    "        if function_entities:\n",
    "            sections.append(\"# Functions\")\n",
    "            for entity in function_entities:\n",
    "                sections.append(f\"# From {entity.module_name}\")\n",
    "                sections.append(entity.source_code)\n",
    "                sections.append(\"\")\n",
    "        \n",
    "        # Add other code\n",
    "        other_entities = [e for e in sorted_entities if isinstance(e, OtherEntity)]\n",
    "        if other_entities:\n",
    "            sections.append(\"# Other code\")\n",
    "            for entity in other_entities:\n",
    "                if \"if __name__ == '__main__'\" not in entity.source_code and 'if __name__ == \"__main__\"' not in entity.source_code:\n",
    "                    sections.append(f\"# From {entity.module_name}\")\n",
    "                    sections.append(entity.source_code)\n",
    "            sections.append(\"\")\n",
    "        \n",
    "        # Identify main functions for the execution block\n",
    "        main_function_candidates = [\n",
    "            entity.name for entity in function_entities\n",
    "            if entity.name in ('main', 'process_tech_specs', 'run', 'execute')\n",
    "        ]\n",
    "        \n",
    "        # Add main execution block\n",
    "        sections.append(\"# Main execution\")\n",
    "        sections.append('if __name__ == \"__main__\":')\n",
    "        sections.append('    # Intelligent execution of combined solution')\n",
    "        sections.append('    try:')\n",
    "        sections.append('        print(\"Running intelligently combined solution...\")')\n",
    "        \n",
    "        if main_function_candidates:\n",
    "            for func_name in main_function_candidates:\n",
    "                if func_name == 'process_tech_specs':\n",
    "                    sections.append(f'        {func_name}(\"tech.xlsx\")')\n",
    "                else:\n",
    "                    sections.append(f'        {func_name}()')\n",
    "        else:\n",
    "            # If no main functions found, look for other potential entry points\n",
    "            entry_points = []\n",
    "            for entity in function_entities:\n",
    "                # Count how many times this function is called by others\n",
    "                call_count = sum(1 for other in function_entities if entity.name in other.calls)\n",
    "                if call_count == 0 and entity.name not in ('__init__', 'setup', 'configure'):\n",
    "                    entry_points.append(entity.name)\n",
    "            \n",
    "            if entry_points:\n",
    "                sections.append('        # No explicit main function found, calling potential entry points')\n",
    "                for func_name in entry_points[:3]:  # Limit to top 3 potential entry points\n",
    "                    sections.append(f'        # {func_name}()')\n",
    "        \n",
    "        sections.append('    except Exception as e:')\n",
    "        sections.append('        logging.error(f\"Error running combined solution: {e}\")')\n",
    "        sections.append('        import traceback')\n",
    "        sections.append('        traceback.print_exc()')\n",
    "        \n",
    "        return '\\n'.join(sections)\n",
    "\n",
    "def intelligent_code_combiner_agent():\n",
    "    \"\"\"\n",
    "    Agent that intelligently combines all code files from the latest code_generation folder\n",
    "    into a single cohesive file that works flawlessly, taking into account code dependencies.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find the latest code_generation folder\n",
    "        latest_folder = find_latest_code_generation_folder()\n",
    "        logger.info(f\"Found latest code generation folder: {latest_folder}\")\n",
    "        \n",
    "        # Find all code files\n",
    "        code_files = find_code_files(latest_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to combine\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to combine\")\n",
    "            return\n",
    "        \n",
    "        # Initialize the intelligent code combiner\n",
    "        combiner = IntelligentCodeCombiner()\n",
    "        \n",
    "        # Analyze each file\n",
    "        for module_name, file_path in code_files:\n",
    "            combiner.analyze_file(file_path, module_name)\n",
    "        \n",
    "        # Build the dependency graph\n",
    "        combiner.build_dependency_graph()\n",
    "        \n",
    "        # Generate the combined code\n",
    "        combined_code = combiner.generate_combined_code()\n",
    "        \n",
    "        # Write the combined code\n",
    "        output_file = os.path.join(os.path.dirname(latest_folder), \n",
    "                                 f\"intelligent_solution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(combined_code)\n",
    "        \n",
    "        logger.info(f\"Successfully created intelligent combined solution: {output_file}\")\n",
    "        print(f\"Intelligent combined solution created at: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in intelligent code combiner: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    intelligent_code_combiner_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:25:49,182 - __main__ - INFO - [1217235215.py:55] - Azure OpenAI configuration verified\n",
      "2025-03-06 15:25:49,188 - __main__ - INFO - [1217235215.py:268] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'final_corrected' code for US_145\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'initial' code for US_147\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'final_corrected' code for US_146\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'final_corrected' code for US_141\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'final_corrected' code for US_142\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'final_corrected' code for US_144\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:99] - Using 'initial' code for US_143\n",
      "2025-03-06 15:25:49,189 - __main__ - INFO - [1217235215.py:272] - Found 7 code files to combine\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 8992 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 7244 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_147/attempt_initial/code.py\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 7360 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 7881 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 7626 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 15:25:49,190 - __main__ - INFO - [1217235215.py:116] - Read 7327 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-06 15:25:49,191 - __main__ - INFO - [1217235215.py:116] - Read 8568 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_143/attempt_initial/code.py\n",
      "2025-03-06 15:25:49,198 - __main__ - INFO - [1217235215.py:161] - Split code into 1 chunks for processing\n",
      "2025-03-06 15:25:49,198 - __main__ - INFO - [1217235215.py:285] - Using LLM to analyze and combine code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:25:50,658 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 15:25:50,659 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 15:26:52,306 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 15:26:52,306 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 60.000000 seconds\n",
      "2025-03-06 15:27:53,776 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 15:27:53,778 - __main__ - ERROR - [1217235215.py:313] - Error in LLM code combiner: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_260768/1217235215.py\", line 286, in llm_code_combiner_agent\n",
      "    combined_code = analyze_and_combine_with_llm(model, code_chunks)\n",
      "  File \"/tmp/ipykernel_260768/1217235215.py\", line 201, in analyze_and_combine_with_llm\n",
      "    response = model.invoke([system_message, human_message])\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 284, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 860, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 690, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py\", line 925, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_openai/chat_models/base.py\", line 717, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/resources/chat/completions.py\", line 859, in create\n",
      "    return self._post(\n",
      "           ~~~~~~~~~~^\n",
      "        \"/chat/completions\",\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<40 lines>...\n",
      "        stream_cls=Stream[ChatCompletionChunk],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1283, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 960, in request\n",
      "    return self._request(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        cast_to=cast_to,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        retries_taken=retries_taken,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1049, in _request\n",
      "    return self._retry_request(\n",
      "           ~~~~~~~~~~~~~~~~~~~^\n",
      "        input_options,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        stream_cls=stream_cls,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1098, in _retry_request\n",
      "    return self._request(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        options=options,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        stream_cls=stream_cls,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1049, in _request\n",
      "    return self._retry_request(\n",
      "           ~~~~~~~~~~~~~~~~~~~^\n",
      "        input_options,\n",
      "        ^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        stream_cls=stream_cls,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1098, in _retry_request\n",
      "    return self._request(\n",
      "           ~~~~~~~~~~~~~^\n",
      "        options=options,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        stream_cls=stream_cls,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py\", line 1064, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 319\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[43mllm_code_combiner_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 286\u001b[0m, in \u001b[0;36mllm_code_combiner_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Use the LLM to analyze and combine the code\u001b[39;00m\n\u001b[1;32m    285\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing LLM to analyze and combine code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 286\u001b[0m combined_code \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_and_combine_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Extract code from LLM response if needed\u001b[39;00m\n\u001b[1;32m    289\u001b[0m combined_code \u001b[38;5;241m=\u001b[39m extract_code_from_llm_response(combined_code)\n",
      "Cell \u001b[0;32mIn[7], line 201\u001b[0m, in \u001b[0;36manalyze_and_combine_with_llm\u001b[0;34m(model, code_chunks)\u001b[0m\n\u001b[1;32m    198\u001b[0m system_message \u001b[38;5;241m=\u001b[39m SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a Python expert who specializes in analyzing and combining code modules.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m human_message \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m--> 201\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuman_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m combined_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_code\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:717\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1048\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1049\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1067\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1068\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1073\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM-Powered Code Combiner Agent\n",
    "\n",
    "This script uses GPT-4o via Azure OpenAI to intelligently combine code files\n",
    "from the latest code generation folder into a single cohesive file.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "import textwrap\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import tiktoken\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure Azure OpenAI environment variables are set\n",
    "def check_openai_config():\n",
    "    \"\"\"Check if Azure OpenAI config is set in environment variables.\"\"\"\n",
    "    required_vars = [\n",
    "        \"AZURE_OPENAI_API_KEY\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"AZURE_OPENAI_API_VERSION\",\n",
    "        \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"\n",
    "    ]\n",
    "    \n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        # Try to read from the existing code\n",
    "        try:\n",
    "            with open(__file__, 'r') as f:\n",
    "                content = f.read()\n",
    "                for var in missing:\n",
    "                    import re\n",
    "                    match = re.search(rf'os\\.environ\\[\"{var}\"\\]\\s*=\\s*\"([^\"]+)\"', content)\n",
    "                    if match:\n",
    "                        os.environ[var] = match.group(1)\n",
    "                        logger.info(f\"Set {var} from source code\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not read OpenAI config from source: {e}\")\n",
    "    \n",
    "    # Verify all variables are set\n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing Azure OpenAI configuration: {', '.join(missing)}\")\n",
    "    \n",
    "    logger.info(\"Azure OpenAI configuration verified\")\n",
    "\n",
    "def find_latest_code_generation_folder():\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logger.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logger.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "def read_code_files(code_files):\n",
    "    \"\"\"Read code files and return a dictionary mapping module names to code content.\"\"\"\n",
    "    code_contents = {}\n",
    "    \n",
    "    for module_name, file_path in code_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                code_contents[module_name] = content\n",
    "                logger.info(f\"Read {len(content)} bytes from {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return code_contents\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count the number of tokens in a text string.\"\"\"\n",
    "    try:\n",
    "        encoder = tiktoken.encoding_for_model(\"gpt-4o-2024-05-13\")\n",
    "        return len(encoder.encode(text))\n",
    "    except:\n",
    "        # Fallback to a rough approximation if tiktoken is not available\n",
    "        return len(text) // 4\n",
    "\n",
    "def chunk_code_for_llm(code_contents: Dict[str, str], max_tokens: int = 120000):\n",
    "    \"\"\"\n",
    "    Chunk code contents to fit within token limits.\n",
    "    Returns a list of chunks, where each chunk is a dictionary of module names to code contents.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = {}\n",
    "    current_tokens = 0\n",
    "    \n",
    "    # Sort by size to try to pack more efficiently\n",
    "    modules = sorted(code_contents.items(), key=lambda x: len(x[1]))\n",
    "    \n",
    "    for module_name, content in modules:\n",
    "        # Count tokens for this module\n",
    "        module_tokens = count_tokens(f\"Module {module_name}:\\n{content}\")\n",
    "        \n",
    "        # If adding this module would exceed the limit, start a new chunk\n",
    "        if current_tokens + module_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = {}\n",
    "            current_tokens = 0\n",
    "        \n",
    "        # Add module to current chunk\n",
    "        current_chunk[module_name] = content\n",
    "        current_tokens += module_tokens\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    logger.info(f\"Split code into {len(chunks)} chunks for processing\")\n",
    "    return chunks\n",
    "\n",
    "def prepare_prompt_for_llm(code_chunk: Dict[str, str]) -> str:\n",
    "    \"\"\"Prepare a prompt for the LLM to combine code.\"\"\"\n",
    "    modules_text = []\n",
    "    \n",
    "    for module_name, content in code_chunk.items():\n",
    "        modules_text.append(f\"MODULE: {module_name}\\n```python\\n{content}\\n```\\n\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "I have multiple Python modules that were generated separately but need to be combined into a single cohesive file.\n",
    "Please analyze these modules and combine them intelligently, considering dependencies and avoiding duplication.\n",
    "\n",
    "Here are the modules:\n",
    "\n",
    "{\"\".join(modules_text)}\n",
    "\n",
    "Please combine these modules into a single Python file, making sure to:\n",
    "1. Arrange code in the correct order (imports first, then classes, functions, and main execution)\n",
    "2. Remove duplicate imports, functions, or classes\n",
    "3. Resolve any naming conflicts\n",
    "4. Maintain all functionality from the original modules\n",
    "5. Ensure dependencies between components are preserved\n",
    "6. Include a main execution block that calls the appropriate entry points\n",
    "7. Make the combined code as clean and readable as possible\n",
    "8. Add helpful comments to explain the combined structure\n",
    "\n",
    "Return only the final combined Python code without explanation or other text.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def analyze_and_combine_with_llm(model, code_chunks: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Use the LLM to analyze and combine code chunks.\"\"\"\n",
    "    if len(code_chunks) == 1:\n",
    "        # If there's only one chunk, process it directly\n",
    "        prompt = prepare_prompt_for_llm(code_chunks[0])\n",
    "        system_message = SystemMessage(content=\"You are a Python expert who specializes in analyzing and combining code modules.\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        response = model.invoke([system_message, human_message])\n",
    "        combined_code = response.content\n",
    "        \n",
    "        return combined_code\n",
    "    else:\n",
    "        # If there are multiple chunks, process them iteratively\n",
    "        intermediate_results = []\n",
    "        \n",
    "        # First pass: process each chunk separately\n",
    "        for i, chunk in enumerate(code_chunks):\n",
    "            logger.info(f\"Processing chunk {i+1}/{len(code_chunks)}\")\n",
    "            prompt = prepare_prompt_for_llm(chunk)\n",
    "            system_message = SystemMessage(content=\"You are a Python expert who specializes in analyzing and combining code modules.\")\n",
    "            human_message = HumanMessage(content=prompt)\n",
    "            \n",
    "            response = model.invoke([system_message, human_message])\n",
    "            intermediate_results.append(response.content)\n",
    "        \n",
    "        # Second pass: combine the intermediate results\n",
    "        final_modules = {f\"Combined_Chunk_{i}\": code for i, code in enumerate(intermediate_results)}\n",
    "        final_prompt = prepare_prompt_for_llm(final_modules)\n",
    "        \n",
    "        system_message = SystemMessage(content=\"You are a Python expert who specializes in analyzing and combining code modules.\")\n",
    "        human_message = HumanMessage(content=final_prompt)\n",
    "        \n",
    "        response = model.invoke([system_message, human_message])\n",
    "        combined_code = response.content\n",
    "        \n",
    "        return combined_code\n",
    "\n",
    "def extract_code_from_llm_response(response: str) -> str:\n",
    "    \"\"\"Extract code from LLM response, removing any markdown code blocks.\"\"\"\n",
    "    # Check if the response is wrapped in code blocks\n",
    "    if response.startswith(\"```python\") and response.endswith(\"```\"):\n",
    "        # Extract code between the markers\n",
    "        code = response.split(\"```python\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "        return code\n",
    "    elif response.startswith(\"```\") and response.endswith(\"```\"):\n",
    "        # Extract code between the markers\n",
    "        code = response.split(\"```\", 2)[1]\n",
    "        if code.startswith(\"python\"):\n",
    "            code = code[6:]\n",
    "        return code.strip()\n",
    "    \n",
    "    # If not wrapped in code blocks, return as is\n",
    "    return response\n",
    "\n",
    "def llm_code_combiner_agent():\n",
    "    \"\"\"\n",
    "    Agent that uses LLM to intelligently combine all code files from the latest \n",
    "    code_generation folder into a single cohesive file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check Azure OpenAI configuration\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            temperature=0.2  # Lower temperature for more deterministic code combination\n",
    "        )\n",
    "        \n",
    "        # Find the latest code_generation folder\n",
    "        latest_folder = find_latest_code_generation_folder()\n",
    "        logger.info(f\"Found latest code generation folder: {latest_folder}\")\n",
    "        \n",
    "        # Find all code files\n",
    "        code_files = find_code_files(latest_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to combine\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to combine\")\n",
    "            return\n",
    "        \n",
    "        # Read all code files\n",
    "        code_contents = read_code_files(code_files)\n",
    "        \n",
    "        # Chunk code for processing by the LLM\n",
    "        code_chunks = chunk_code_for_llm(code_contents)\n",
    "        \n",
    "        # Use the LLM to analyze and combine the code\n",
    "        logger.info(\"Using LLM to analyze and combine code\")\n",
    "        combined_code = analyze_and_combine_with_llm(model, code_chunks)\n",
    "        \n",
    "        # Extract code from LLM response if needed\n",
    "        combined_code = extract_code_from_llm_response(combined_code)\n",
    "        \n",
    "        # Add a header comment\n",
    "        header = f'''\"\"\"\n",
    "LLM-Combined Solution\n",
    "This file was automatically generated by the LLM Code Combiner Agent.\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "        combined_code = header + combined_code\n",
    "        \n",
    "        # Write the combined code\n",
    "        output_file = os.path.join(os.path.dirname(latest_folder), \n",
    "                                 f\"llm_combined_solution_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(combined_code)\n",
    "        \n",
    "        logger.info(f\"Successfully created LLM-combined solution: {output_file}\")\n",
    "        print(f\"LLM-combined solution created at: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in LLM code combiner: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm_code_combiner_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 17:47:05,463 - __main__ - INFO - [2594179916.py:40] - Looking for OpenAI configuration in current environment...\n",
      "2025-03-06 17:47:05,464 - __main__ - INFO - [2594179916.py:60] - Azure OpenAI configuration verified\n",
      "2025-03-06 17:47:05,484 - __main__ - INFO - [2594179916.py:589] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-06 17:47:05,485 - __main__ - INFO - [2594179916.py:104] - Using 'final_corrected' code for US_145\n",
      "2025-03-06 17:47:05,485 - __main__ - INFO - [2594179916.py:104] - Using 'initial' code for US_147\n",
      "2025-03-06 17:47:05,486 - __main__ - INFO - [2594179916.py:104] - Using 'final_corrected' code for US_146\n",
      "2025-03-06 17:47:05,486 - __main__ - INFO - [2594179916.py:104] - Using 'final_corrected' code for US_141\n",
      "2025-03-06 17:47:05,486 - __main__ - INFO - [2594179916.py:104] - Using 'final_corrected' code for US_142\n",
      "2025-03-06 17:47:05,486 - __main__ - INFO - [2594179916.py:104] - Using 'final_corrected' code for US_144\n",
      "2025-03-06 17:47:05,486 - __main__ - INFO - [2594179916.py:104] - Using 'initial' code for US_143\n",
      "2025-03-06 17:47:05,487 - __main__ - INFO - [2594179916.py:598] - Found 7 code files to integrate\n",
      "2025-03-06 17:47:05,487 - __main__ - INFO - [2594179916.py:121] - Read 8992 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 17:47:05,487 - __main__ - INFO - [2594179916.py:121] - Read 7244 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_147/attempt_initial/code.py\n",
      "2025-03-06 17:47:05,487 - __main__ - INFO - [2594179916.py:121] - Read 7360 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 17:47:05,488 - __main__ - INFO - [2594179916.py:121] - Read 7881 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 17:47:05,488 - __main__ - INFO - [2594179916.py:121] - Read 7626 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 17:47:05,488 - __main__ - INFO - [2594179916.py:121] - Read 7327 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-06 17:47:05,488 - __main__ - INFO - [2594179916.py:121] - Read 8568 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_143/attempt_initial/code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_145 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_145_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_147 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_147_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_146 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_146_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_141 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_141_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_142 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_142_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_144 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_144_code.py\n",
      "2025-03-06 17:47:05,489 - __main__ - INFO - [2594179916.py:567] - Saved module US_143 to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/US_143_code.py\n",
      "2025-03-06 17:47:05,490 - __main__ - ERROR - [2594179916.py:286] - Syntax error in module US_145: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 17:47:05,490 - __main__ - INFO - [2594179916.py:291] - Using LLM to extract API documentation for US_145\n",
      "2025-03-06 17:47:39,245 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 17:47:39,252 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_145\n",
      "/tmp/ipykernel_302908/2594179916.py:137: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_302908/2594179916.py:138: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.body[0].value.s.strip()\n",
      "2025-03-06 17:47:39,254 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_147\n",
      "2025-03-06 17:47:39,254 - __main__ - ERROR - [2594179916.py:286] - Syntax error in module US_146: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 17:47:39,255 - __main__ - INFO - [2594179916.py:291] - Using LLM to extract API documentation for US_146\n",
      "2025-03-06 17:48:09,800 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 17:48:09,802 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_146\n",
      "2025-03-06 17:48:09,803 - __main__ - ERROR - [2594179916.py:286] - Syntax error in module US_141: parameter without a default follows parameter with a default (<unknown>, line 163)\n",
      "2025-03-06 17:48:09,803 - __main__ - INFO - [2594179916.py:291] - Using LLM to extract API documentation for US_141\n",
      "2025-03-06 17:48:39,893 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 17:48:39,894 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_141\n",
      "2025-03-06 17:48:39,894 - __main__ - ERROR - [2594179916.py:286] - Syntax error in module US_142: invalid syntax (<unknown>, line 1)\n",
      "2025-03-06 17:48:39,894 - __main__ - INFO - [2594179916.py:291] - Using LLM to extract API documentation for US_142\n",
      "2025-03-06 17:48:59,068 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 17:48:59,069 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_142\n",
      "2025-03-06 17:48:59,071 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_144\n",
      "/tmp/ipykernel_302908/2594179916.py:137: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_302908/2594179916.py:138: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.body[0].value.s.strip()\n",
      "2025-03-06 17:48:59,071 - __main__ - INFO - [2594179916.py:391] - Generated API documentation for US_143\n",
      "2025-03-06 17:48:59,073 - __main__ - INFO - [2594179916.py:618] - Saved API documentation to /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/api_documentation.json\n",
      "2025-03-06 17:48:59,073 - __main__ - INFO - [2594179916.py:535] - Sending API documentation to LLM for integration\n",
      "2025-03-06 17:48:59,505 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-06 17:48:59,506 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 11.000000 seconds\n",
      "2025-03-06 17:49:38,730 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 17:49:38,732 - __main__ - INFO - [2594179916.py:642] - Successfully created integrated solution: /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705/integrated_solution.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrated solution created at: /home/airangers/Desktop/shivani/SDLC/integrated_solution_20250306_174705\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "API Documentation-Based Code Integrator\n",
    "\n",
    "This script generates API documentation for code modules and uses that documentation\n",
    "to guide the LLM in creating an integrated solution.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import textwrap\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure Azure OpenAI environment variables are set\n",
    "def check_openai_config():\n",
    "    \"\"\"Check if Azure OpenAI config is set in environment variables.\"\"\"\n",
    "    required_vars = [\n",
    "        \"AZURE_OPENAI_API_KEY\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"AZURE_OPENAI_API_VERSION\",\n",
    "        \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"\n",
    "    ]\n",
    "    \n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        # Check if we can find them in the current file\n",
    "        logger.info(\"Looking for OpenAI configuration in current environment...\")\n",
    "        \n",
    "        # Try to use values that might be in the environment from previous execution\n",
    "        if \"AZURE_OPENAI_API_KEY\" not in os.environ and '0bf3daeba1814d03b5d62e1da4077478' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_ENDPOINT\" not in os.environ and 'https://openaisk123.openai.azure.com/' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_VERSION\" not in os.environ and '2024-08-01-preview' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\" not in os.environ and 'gpt-4o' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "    \n",
    "    # Verify all variables are set\n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing Azure OpenAI configuration: {', '.join(missing)}\")\n",
    "    \n",
    "    logger.info(\"Azure OpenAI configuration verified\")\n",
    "\n",
    "def find_latest_code_generation_folder():\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logger.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logger.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "def read_code_files(code_files):\n",
    "    \"\"\"Read code files and return a dictionary mapping module names to code content.\"\"\"\n",
    "    code_contents = {}\n",
    "    \n",
    "    for module_name, file_path in code_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                code_contents[module_name] = content\n",
    "                logger.info(f\"Read {len(content)} bytes from {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return code_contents\n",
    "\n",
    "class APIDocGenerator:\n",
    "    \"\"\"Class to generate API documentation for Python code.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def extract_docstring(self, node):\n",
    "        \"\"\"Extract docstring from an AST node.\"\"\"\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):\n",
    "            if (node.body and isinstance(node.body[0], ast.Expr) and \n",
    "                isinstance(node.body[0].value, ast.Str)):\n",
    "                return node.body[0].value.s.strip()\n",
    "        return None\n",
    "    \n",
    "    def extract_function_info(self, func_node):\n",
    "        \"\"\"Extract information about a function from its AST node.\"\"\"\n",
    "        name = func_node.name\n",
    "        docstring = self.extract_docstring(func_node) or \"No documentation available.\"\n",
    "        \n",
    "        params = []\n",
    "        returns = None\n",
    "        \n",
    "        # Extract parameters\n",
    "        for arg in func_node.args.args:\n",
    "            param_name = arg.arg\n",
    "            param_type = None\n",
    "            \n",
    "            # Check for type annotation\n",
    "            if arg.annotation:\n",
    "                param_type = ast.unparse(arg.annotation)\n",
    "            \n",
    "            params.append({\n",
    "                \"name\": param_name,\n",
    "                \"type\": param_type,\n",
    "                \"description\": \"Parameter description not available.\"\n",
    "            })\n",
    "        \n",
    "        # Extract return type from annotation if available\n",
    "        if func_node.returns:\n",
    "            returns = ast.unparse(func_node.returns)\n",
    "        \n",
    "        # Look for parameter and return descriptions in docstring\n",
    "        if docstring:\n",
    "            # Parse parameters from docstring\n",
    "            param_matches = re.findall(r'(?:Args|Parameters):\\s*(.*?)(?:\\n\\n|\\Z)', docstring, re.DOTALL)\n",
    "            if param_matches:\n",
    "                param_section = param_matches[0]\n",
    "                param_descriptions = re.findall(r'(\\w+)(?:\\s+\\((.*?)\\))?\\s*:\\s*(.*?)(?=\\n\\s*\\w+\\s*:|$)', param_section, re.DOTALL)\n",
    "                \n",
    "                for param_name, param_type, param_desc in param_descriptions:\n",
    "                    for param in params:\n",
    "                        if param[\"name\"] == param_name:\n",
    "                            param[\"description\"] = param_desc.strip()\n",
    "                            if not param[\"type\"] and param_type:\n",
    "                                param[\"type\"] = param_type\n",
    "            \n",
    "            # Parse return from docstring\n",
    "            returns_matches = re.findall(r'(?:Returns|Return):\\s*(.*?)(?:\\n\\n|\\Z)', docstring, re.DOTALL)\n",
    "            if returns_matches and not returns:\n",
    "                return_section = returns_matches[0].strip()\n",
    "                # Check if there's a type specified\n",
    "                return_type_match = re.match(r'(?:.*?)\\s*\\((.*?)\\)\\s*:', return_section)\n",
    "                if return_type_match:\n",
    "                    returns = return_type_match.group(1)\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"docstring\": docstring,\n",
    "            \"parameters\": params,\n",
    "            \"returns\": returns\n",
    "        }\n",
    "    \n",
    "    def extract_class_info(self, class_node):\n",
    "        \"\"\"Extract information about a class from its AST node.\"\"\"\n",
    "        name = class_node.name\n",
    "        docstring = self.extract_docstring(class_node) or \"No documentation available.\"\n",
    "        \n",
    "        # Get base classes\n",
    "        bases = []\n",
    "        for base in class_node.bases:\n",
    "            if isinstance(base, ast.Name):\n",
    "                bases.append(base.id)\n",
    "            else:\n",
    "                bases.append(ast.unparse(base))\n",
    "        \n",
    "        # Get methods\n",
    "        methods = []\n",
    "        for node in class_node.body:\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                methods.append(self.extract_function_info(node))\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"docstring\": docstring,\n",
    "            \"bases\": bases,\n",
    "            \"methods\": methods\n",
    "        }\n",
    "    \n",
    "    def extract_global_vars(self, module_node):\n",
    "        \"\"\"Extract global variables from a module.\"\"\"\n",
    "        variables = []\n",
    "        \n",
    "        for node in module_node.body:\n",
    "            if isinstance(node, ast.Assign) and isinstance(node.targets[0], ast.Name):\n",
    "                var_name = node.targets[0].id\n",
    "                var_value = ast.unparse(node.value)\n",
    "                variables.append({\n",
    "                    \"name\": var_name,\n",
    "                    \"value\": var_value\n",
    "                })\n",
    "                \n",
    "        return variables\n",
    "    \n",
    "    def extract_imports(self, module_node):\n",
    "        \"\"\"Extract imports from a module.\"\"\"\n",
    "        imports = []\n",
    "        \n",
    "        for node in module_node.body:\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"module\": name.name,\n",
    "                        \"alias\": name.asname\n",
    "                    })\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                module = node.module or \"\"\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"module\": module,\n",
    "                        \"name\": name.name,\n",
    "                        \"alias\": name.asname\n",
    "                    })\n",
    "                    \n",
    "        return imports\n",
    "    \n",
    "    def generate_module_api_doc(self, module_name, code):\n",
    "        \"\"\"Generate API documentation for a module.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            module_doc = {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": self.extract_docstring(tree) or \"No module documentation available.\",\n",
    "                \"imports\": self.extract_imports(tree),\n",
    "                \"global_vars\": self.extract_global_vars(tree),\n",
    "                \"functions\": [],\n",
    "                \"classes\": []\n",
    "            }\n",
    "            \n",
    "            # Extract functions and classes\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    module_doc[\"functions\"].append(self.extract_function_info(node))\n",
    "                elif isinstance(node, ast.ClassDef):\n",
    "                    module_doc[\"classes\"].append(self.extract_class_info(node))\n",
    "            \n",
    "            return module_doc\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"Syntax error in module {module_name}: {e}\")\n",
    "            return self._fallback_api_doc_generation(module_name, code)\n",
    "    \n",
    "    def _fallback_api_doc_generation(self, module_name, code):\n",
    "        \"\"\"Use the LLM as a fallback for API doc generation when parsing fails.\"\"\"\n",
    "        logger.info(f\"Using LLM to extract API documentation for {module_name}\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate a detailed API documentation for the following Python code module.\n",
    "Extract all functions, classes, methods, and their parameters, return types, and docstrings.\n",
    "Format the response as a JSON object with the structure shown in the example.\n",
    "\n",
    "Example structure:\n",
    "```json\n",
    "{{\n",
    "  \"name\": \"module_name\",\n",
    "  \"docstring\": \"Module docstring\",\n",
    "  \"imports\": [\n",
    "    {{\"module\": \"os\", \"alias\": null}},\n",
    "    {{\"module\": \"pandas\", \"alias\": \"pd\"}}\n",
    "  ],\n",
    "  \"global_vars\": [\n",
    "    {{\"name\": \"logger\", \"value\": \"logging.getLogger(__name__)\"}}\n",
    "  ],\n",
    "  \"functions\": [\n",
    "    {{\n",
    "      \"name\": \"function_name\",\n",
    "      \"docstring\": \"Function docstring\",\n",
    "      \"parameters\": [\n",
    "        {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "      ],\n",
    "      \"returns\": \"str\"\n",
    "    }}\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {{\n",
    "      \"name\": \"ClassName\",\n",
    "      \"docstring\": \"Class docstring\",\n",
    "      \"bases\": [\"BaseClass\"],\n",
    "      \"methods\": [\n",
    "        {{\n",
    "          \"name\": \"method_name\",\n",
    "          \"docstring\": \"Method docstring\",\n",
    "          \"parameters\": [\n",
    "            {{\"name\": \"self\", \"type\": null, \"description\": \"Instance reference\"}},\n",
    "            {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "          ],\n",
    "          \"returns\": \"bool\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Here's the code to document:\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Extract as much detail as possible. If you can't determine something exactly (like a type), make your best guess based on context.\n",
    "\"\"\"\n",
    "\n",
    "        system_message = SystemMessage(content=\"You are a Python expert who specializes in extracting API documentation from code.\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke([system_message, human_message])\n",
    "            content = response.content\n",
    "            \n",
    "            # Try to extract JSON from the response\n",
    "            json_match = re.search(r'```json\\s*(.*?)\\s*```', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                # If no JSON code block, try to find any JSON object\n",
    "                json_match = re.search(r'({[\\s\\S]*})', content)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(1)\n",
    "                else:\n",
    "                    json_str = content\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting API doc from LLM for {module_name}: {e}\")\n",
    "            # Return a minimal structure\n",
    "            return {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": \"Documentation extraction failed.\",\n",
    "                \"imports\": [],\n",
    "                \"global_vars\": [],\n",
    "                \"functions\": [],\n",
    "                \"classes\": []\n",
    "            }\n",
    "    \n",
    "    def generate_all_module_docs(self, code_contents):\n",
    "        \"\"\"Generate API documentation for all modules.\"\"\"\n",
    "        module_docs = {}\n",
    "        \n",
    "        for module_name, code in code_contents.items():\n",
    "            try:\n",
    "                module_doc = self.generate_module_api_doc(module_name, code)\n",
    "                module_docs[module_name] = module_doc\n",
    "                logger.info(f\"Generated API documentation for {module_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating API doc for {module_name}: {e}\")\n",
    "        \n",
    "        return module_docs\n",
    "    \n",
    "    def format_api_docs_for_llm(self, module_docs):\n",
    "        \"\"\"Format API documentation for use in LLM prompt.\"\"\"\n",
    "        formatted_docs = []\n",
    "        \n",
    "        for module_name, doc in module_docs.items():\n",
    "            module_text = [f\"MODULE: {module_name}\"]\n",
    "            \n",
    "            # Add module docstring\n",
    "            module_text.append(f\"Description: {doc['docstring']}\")\n",
    "            module_text.append(\"\")\n",
    "            \n",
    "            # Add imports\n",
    "            if doc[\"imports\"]:\n",
    "                module_text.append(\"Imports:\")\n",
    "                for imp in doc[\"imports\"]:\n",
    "                    if \"name\" in imp:\n",
    "                        from_txt = f\"from {imp['module']} \" if imp['module'] else \"from \"\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  {from_txt}import {imp['name']}{as_txt}\")\n",
    "                    else:\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  import {imp['module']}{as_txt}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add global variables\n",
    "            if doc[\"global_vars\"]:\n",
    "                module_text.append(\"Global Variables:\")\n",
    "                for var in doc[\"global_vars\"]:\n",
    "                    module_text.append(f\"  {var['name']} = {var['value']}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add classes\n",
    "            if doc[\"classes\"]:\n",
    "                module_text.append(\"Classes:\")\n",
    "                for cls in doc[\"classes\"]:\n",
    "                    bases = f\"({', '.join(cls['bases'])})\" if cls['bases'] else \"\"\n",
    "                    module_text.append(f\"  class {cls['name']}{bases}:\")\n",
    "                    module_text.append(f\"    \\\"{cls['docstring']}\\\"\")\n",
    "                    module_text.append(\"\")\n",
    "                    \n",
    "                    if cls[\"methods\"]:\n",
    "                        module_text.append(\"    Methods:\")\n",
    "                        for method in cls[\"methods\"]:\n",
    "                            params = []\n",
    "                            for p in method[\"parameters\"]:\n",
    "                                param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                                params.append(f\"{p['name']}{param_type}\")\n",
    "                            \n",
    "                            returns = f\" -> {method['returns']}\" if method['returns'] else \"\"\n",
    "                            module_text.append(f\"      def {method['name']}({', '.join(params)}){returns}:\")\n",
    "                            module_text.append(f\"        \\\"{method['docstring']}\\\"\")\n",
    "                            \n",
    "                            # Add parameter descriptions\n",
    "                            if any(p[\"description\"] != \"Parameter description not available.\" for p in method[\"parameters\"]):\n",
    "                                module_text.append(\"        Parameters:\")\n",
    "                                for p in method[\"parameters\"]:\n",
    "                                    if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                        module_text.append(f\"          {p['name']}: {p['description']}\")\n",
    "                            \n",
    "                            module_text.append(\"\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            # Add functions\n",
    "            if doc[\"functions\"]:\n",
    "                module_text.append(\"Functions:\")\n",
    "                for func in doc[\"functions\"]:\n",
    "                    params = []\n",
    "                    for p in func[\"parameters\"]:\n",
    "                        param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                        params.append(f\"{p['name']}{param_type}\")\n",
    "                    \n",
    "                    returns = f\" -> {func['returns']}\" if func['returns'] else \"\"\n",
    "                    module_text.append(f\"  def {func['name']}({', '.join(params)}){returns}:\")\n",
    "                    module_text.append(f\"    \\\"{func['docstring']}\\\"\")\n",
    "                    \n",
    "                    # Add parameter descriptions\n",
    "                    if any(p[\"description\"] != \"Parameter description not available.\" for p in func[\"parameters\"]):\n",
    "                        module_text.append(\"    Parameters:\")\n",
    "                        for p in func[\"parameters\"]:\n",
    "                            if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                module_text.append(f\"      {p['name']}: {p['description']}\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            formatted_docs.append(\"\\n\".join(module_text))\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs) + \"\\n\"\n",
    "\n",
    "class CodeIntegrator:\n",
    "    \"\"\"Class to integrate code modules based on API documentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, doc_generator):\n",
    "        self.model = model\n",
    "        self.doc_generator = doc_generator\n",
    "    \n",
    "    def create_integration_prompt(self, api_docs):\n",
    "        \"\"\"Create a prompt for the LLM to integrate the code.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "I have multiple Python modules that need to be integrated into a cohesive solution.\n",
    "Below is the API documentation for each module.\n",
    "\n",
    "{api_docs}\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Create a single integrated Python file that can use ALL these modules\n",
    "2. Design the integrated solution to call the appropriate functions from each module\n",
    "3. Ensure proper sequencing and dependencies between modules\n",
    "4. Maintain all original functionality while avoiding code duplication\n",
    "5. Include a main execution block that coordinates the overall flow\n",
    "6. Use proper imports and references to make the solution work seamlessly\n",
    "7. Write clear comments to explain how the integration works\n",
    "\n",
    "The solution should act as a coordinator that calls functions from the modules above\n",
    "rather than reimplementing the functionality. Assume each module will be available\n",
    "as a separate file that can be imported.\n",
    "\n",
    "Format your response as a single Python file with all necessary imports, functions, \n",
    "and a main execution block. Add helpful comments to explain your integration strategy.\n",
    "\n",
    "Return only the final integrated Python code without explanation or other text.\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_integrated_code(self, module_docs):\n",
    "        \"\"\"Generate integrated code based on API documentation.\"\"\"\n",
    "        # Format API docs for the LLM\n",
    "        api_docs_text = self.doc_generator.format_api_docs_for_llm(module_docs)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = self.create_integration_prompt(api_docs_text)\n",
    "        \n",
    "        # Send to LLM\n",
    "        system_message = SystemMessage(content=\"\"\"You are a Python expert who specializes in integrating multiple code modules \n",
    "into cohesive solutions. You excel at understanding module dependencies and creating orchestration code.\"\"\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        logger.info(\"Sending API documentation to LLM for integration\")\n",
    "        response = self.model.invoke([system_message, human_message])\n",
    "        \n",
    "        # Extract code from response\n",
    "        content = response.content\n",
    "        \n",
    "        # Check if the response is wrapped in code blocks\n",
    "        if \"```python\" in content and content.rstrip().endswith(\"```\"):\n",
    "            # Extract code between the markers\n",
    "            code = content.split(\"```python\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "            return code\n",
    "        elif \"```\" in content and content.rstrip().endswith(\"```\"):\n",
    "            # Extract code between the markers\n",
    "            code = content.split(\"```\", 2)[1]\n",
    "            if code.startswith(\"python\"):\n",
    "                code = code[6:]\n",
    "            return code.strip()\n",
    "        \n",
    "        # If not wrapped in code blocks, return as is\n",
    "        return content\n",
    "\n",
    "def save_modules_with_proper_names(output_dir, code_contents):\n",
    "    \"\"\"Save individual modules with proper names based on user story IDs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for module_name, code in code_contents.items():\n",
    "        file_name = f\"{module_name}_code.py\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved module {module_name} to {file_path}\")\n",
    "\n",
    "def api_based_code_integrator():\n",
    "    \"\"\"\n",
    "    Agent that generates API documentation for code modules and uses that\n",
    "    to create an integrated solution.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check Azure OpenAI configuration\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            temperature=0.1  # Low temperature for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Find the latest code_generation folder\n",
    "        latest_folder = find_latest_code_generation_folder()\n",
    "        logger.info(f\"Found latest code generation folder: {latest_folder}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_dir = os.path.join(os.path.dirname(latest_folder), f\"integrated_solution_{timestamp}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all code files\n",
    "        code_files = find_code_files(latest_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to integrate\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to integrate\")\n",
    "            return\n",
    "        \n",
    "        # Read all code files\n",
    "        code_contents = read_code_files(code_files)\n",
    "        \n",
    "        # Save modules with proper names\n",
    "        save_modules_with_proper_names(output_dir, code_contents)\n",
    "        \n",
    "        # Generate API documentation\n",
    "        doc_generator = APIDocGenerator(model)\n",
    "        module_docs = doc_generator.generate_all_module_docs(code_contents)\n",
    "        \n",
    "        # Save API documentation\n",
    "        api_docs_path = os.path.join(output_dir, \"api_documentation.json\")\n",
    "        with open(api_docs_path, 'w') as f:\n",
    "            json.dump(module_docs, f, indent=2)\n",
    "        logger.info(f\"Saved API documentation to {api_docs_path}\")\n",
    "        \n",
    "        # Generate integrated code\n",
    "        integrator = CodeIntegrator(model, doc_generator)\n",
    "        integrated_code = integrator.generate_integrated_code(module_docs)\n",
    "        \n",
    "        # Add header\n",
    "        header = f'''\"\"\"\n",
    "API Documentation-Based Integrated Solution\n",
    "This file was automatically generated by the API-Based Code Integrator.\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "This code serves as an integration layer that coordinates all the individual modules.\n",
    "Each module's code is stored in separate files named by their user story IDs.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "        integrated_code = header + integrated_code\n",
    "        \n",
    "        # Save integrated code\n",
    "        integrated_code_path = os.path.join(output_dir, \"integrated_solution.py\")\n",
    "        with open(integrated_code_path, 'w') as f:\n",
    "            f.write(integrated_code)\n",
    "        \n",
    "        logger.info(f\"Successfully created integrated solution: {integrated_code_path}\")\n",
    "        print(f\"Integrated solution created at: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in API-based code integrator: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_based_code_integrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 18:15:08,802 - __main__ - INFO - [1066117537.py:61] - Azure OpenAI configuration verified\n",
      "2025-03-06 18:15:08,809 - __main__ - INFO - [1066117537.py:755] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'final_corrected' code for US_145\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'initial' code for US_147\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'final_corrected' code for US_146\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'final_corrected' code for US_141\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'final_corrected' code for US_142\n",
      "2025-03-06 18:15:08,810 - __main__ - INFO - [1066117537.py:105] - Using 'final_corrected' code for US_144\n",
      "2025-03-06 18:15:08,811 - __main__ - INFO - [1066117537.py:105] - Using 'initial' code for US_143\n",
      "2025-03-06 18:15:08,811 - __main__ - INFO - [1066117537.py:764] - Found 7 code files to integrate\n",
      "2025-03-06 18:15:08,811 - __main__ - INFO - [1066117537.py:122] - Read 8979 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-06 18:15:08,811 - __main__ - INFO - [1066117537.py:122] - Read 7244 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_147/attempt_initial/code.py\n",
      "2025-03-06 18:15:08,811 - __main__ - INFO - [1066117537.py:122] - Read 7347 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-06 18:15:08,812 - __main__ - INFO - [1066117537.py:122] - Read 7881 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-06 18:15:08,812 - __main__ - INFO - [1066117537.py:122] - Read 7613 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-06 18:15:08,812 - __main__ - INFO - [1066117537.py:122] - Read 7327 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-06 18:15:08,812 - __main__ - INFO - [1066117537.py:122] - Read 8568 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_143/attempt_initial/code.py\n",
      "2025-03-06 18:15:08,812 - __main__ - INFO - [1066117537.py:655] - Saved module US_145 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_145_code.py\n",
      "2025-03-06 18:15:08,813 - __main__ - INFO - [1066117537.py:655] - Saved module US_147 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_147_code.py\n",
      "2025-03-06 18:15:08,813 - __main__ - INFO - [1066117537.py:655] - Saved module US_146 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_146_code.py\n",
      "2025-03-06 18:15:08,813 - __main__ - INFO - [1066117537.py:655] - Saved module US_141 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_141_code.py\n",
      "2025-03-06 18:15:08,814 - __main__ - INFO - [1066117537.py:655] - Saved module US_142 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_142_code.py\n",
      "2025-03-06 18:15:08,814 - __main__ - INFO - [1066117537.py:655] - Saved module US_144 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_144_code.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 18:15:08,814 - __main__ - INFO - [1066117537.py:655] - Saved module US_143 to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/US_143_code.py\n",
      "/tmp/ipykernel_302908/1066117537.py:138: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_302908/1066117537.py:139: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.body[0].value.s.strip()\n",
      "2025-03-06 18:15:08,866 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_145\n",
      "2025-03-06 18:15:08,867 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_147\n",
      "2025-03-06 18:15:08,868 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_146\n",
      "2025-03-06 18:15:08,869 - __main__ - ERROR - [1066117537.py:287] - Syntax error in module US_141: parameter without a default follows parameter with a default (<unknown>, line 163)\n",
      "2025-03-06 18:15:08,869 - __main__ - INFO - [1066117537.py:292] - Using LLM to extract API documentation for US_141\n",
      "2025-03-06 18:15:41,086 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 18:15:41,088 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_141\n",
      "/tmp/ipykernel_302908/1066117537.py:138: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_302908/1066117537.py:139: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return node.body[0].value.s.strip()\n",
      "2025-03-06 18:15:41,089 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_142\n",
      "2025-03-06 18:15:41,089 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_144\n",
      "2025-03-06 18:15:41,090 - __main__ - INFO - [1066117537.py:392] - Generated API documentation for US_143\n",
      "2025-03-06 18:15:41,091 - __main__ - INFO - [1066117537.py:784] - Saved API documentation to /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/api_documentation.json\n",
      "2025-03-06 18:15:41,092 - __main__ - INFO - [1066117537.py:595] - Sending API documentation to LLM for integration\n",
      "2025-03-06 18:16:00,968 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 18:16:00,970 - __main__ - INFO - [1066117537.py:642] - Created __init__.py file at /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/__init__.py\n",
      "2025-03-06 18:16:00,971 - __main__ - INFO - [1066117537.py:719] - Created README.md file at /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/README.md\n",
      "2025-03-06 18:16:00,971 - __main__ - INFO - [1066117537.py:686] - Created setup.py file at /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/setup.py\n",
      "2025-03-06 18:16:00,971 - __main__ - INFO - [1066117537.py:817] - Successfully created improved integrated solution: /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508/integrated_solution.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved integrated solution created at: /home/airangers/Desktop/shivani/SDLC/improved_integrated_solution_20250306_181508\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved API Documentation-Based Code Integrator\n",
    "\n",
    "This script generates API documentation for code modules and uses that documentation\n",
    "to guide the LLM in creating an integrated solution with proper module imports.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "import textwrap\n",
    "import shutil\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure Azure OpenAI environment variables are set\n",
    "def check_openai_config():\n",
    "    \"\"\"Check if Azure OpenAI config is set in environment variables.\"\"\"\n",
    "    required_vars = [\n",
    "        \"AZURE_OPENAI_API_KEY\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"AZURE_OPENAI_API_VERSION\",\n",
    "        \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"\n",
    "    ]\n",
    "    \n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        # Check if we can find them in the current file\n",
    "        logger.info(\"Looking for OpenAI configuration in current environment...\")\n",
    "        \n",
    "        # Try to use values that might be in the environment from previous execution\n",
    "        if \"AZURE_OPENAI_API_KEY\" not in os.environ and '0bf3daeba1814d03b5d62e1da4077478' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_ENDPOINT\" not in os.environ and 'https://openaisk123.openai.azure.com/' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_VERSION\" not in os.environ and '2024-08-01-preview' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\" not in os.environ and 'gpt-4o' not in os.environ.values():\n",
    "            os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "    \n",
    "    # Verify all variables are set\n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing Azure OpenAI configuration: {', '.join(missing)}\")\n",
    "    \n",
    "    logger.info(\"Azure OpenAI configuration verified\")\n",
    "\n",
    "def find_latest_code_generation_folder():\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    base_dir = os.getcwd()  # Current working directory\n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logger.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logger.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "def read_code_files(code_files):\n",
    "    \"\"\"Read code files and return a dictionary mapping module names to code content.\"\"\"\n",
    "    code_contents = {}\n",
    "    \n",
    "    for module_name, file_path in code_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                code_contents[module_name] = content\n",
    "                logger.info(f\"Read {len(content)} bytes from {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return code_contents\n",
    "\n",
    "class APIDocGenerator:\n",
    "    \"\"\"Class to generate API documentation for Python code.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def extract_docstring(self, node):\n",
    "        \"\"\"Extract docstring from an AST node.\"\"\"\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):\n",
    "            if (node.body and isinstance(node.body[0], ast.Expr) and \n",
    "                isinstance(node.body[0].value, ast.Str)):\n",
    "                return node.body[0].value.s.strip()\n",
    "        return None\n",
    "    \n",
    "    def extract_function_info(self, func_node):\n",
    "        \"\"\"Extract information about a function from its AST node.\"\"\"\n",
    "        name = func_node.name\n",
    "        docstring = self.extract_docstring(func_node) or \"No documentation available.\"\n",
    "        \n",
    "        params = []\n",
    "        returns = None\n",
    "        \n",
    "        # Extract parameters\n",
    "        for arg in func_node.args.args:\n",
    "            param_name = arg.arg\n",
    "            param_type = None\n",
    "            \n",
    "            # Check for type annotation\n",
    "            if arg.annotation:\n",
    "                param_type = ast.unparse(arg.annotation)\n",
    "            \n",
    "            params.append({\n",
    "                \"name\": param_name,\n",
    "                \"type\": param_type,\n",
    "                \"description\": \"Parameter description not available.\"\n",
    "            })\n",
    "        \n",
    "        # Extract return type from annotation if available\n",
    "        if func_node.returns:\n",
    "            returns = ast.unparse(func_node.returns)\n",
    "        \n",
    "        # Look for parameter and return descriptions in docstring\n",
    "        if docstring:\n",
    "            # Parse parameters from docstring\n",
    "            param_matches = re.findall(r'(?:Args|Parameters):\\s*(.*?)(?:\\n\\n|\\Z)', docstring, re.DOTALL)\n",
    "            if param_matches:\n",
    "                param_section = param_matches[0]\n",
    "                param_descriptions = re.findall(r'(\\w+)(?:\\s+\\((.*?)\\))?\\s*:\\s*(.*?)(?=\\n\\s*\\w+\\s*:|$)', param_section, re.DOTALL)\n",
    "                \n",
    "                for param_name, param_type, param_desc in param_descriptions:\n",
    "                    for param in params:\n",
    "                        if param[\"name\"] == param_name:\n",
    "                            param[\"description\"] = param_desc.strip()\n",
    "                            if not param[\"type\"] and param_type:\n",
    "                                param[\"type\"] = param_type\n",
    "            \n",
    "            # Parse return from docstring\n",
    "            returns_matches = re.findall(r'(?:Returns|Return):\\s*(.*?)(?:\\n\\n|\\Z)', docstring, re.DOTALL)\n",
    "            if returns_matches and not returns:\n",
    "                return_section = returns_matches[0].strip()\n",
    "                # Check if there's a type specified\n",
    "                return_type_match = re.match(r'(?:.*?)\\s*\\((.*?)\\)\\s*:', return_section)\n",
    "                if return_type_match:\n",
    "                    returns = return_type_match.group(1)\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"docstring\": docstring,\n",
    "            \"parameters\": params,\n",
    "            \"returns\": returns\n",
    "        }\n",
    "    \n",
    "    def extract_class_info(self, class_node):\n",
    "        \"\"\"Extract information about a class from its AST node.\"\"\"\n",
    "        name = class_node.name\n",
    "        docstring = self.extract_docstring(class_node) or \"No documentation available.\"\n",
    "        \n",
    "        # Get base classes\n",
    "        bases = []\n",
    "        for base in class_node.bases:\n",
    "            if isinstance(base, ast.Name):\n",
    "                bases.append(base.id)\n",
    "            else:\n",
    "                bases.append(ast.unparse(base))\n",
    "        \n",
    "        # Get methods\n",
    "        methods = []\n",
    "        for node in class_node.body:\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                methods.append(self.extract_function_info(node))\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"docstring\": docstring,\n",
    "            \"bases\": bases,\n",
    "            \"methods\": methods\n",
    "        }\n",
    "    \n",
    "    def extract_global_vars(self, module_node):\n",
    "        \"\"\"Extract global variables from a module.\"\"\"\n",
    "        variables = []\n",
    "        \n",
    "        for node in module_node.body:\n",
    "            if isinstance(node, ast.Assign) and isinstance(node.targets[0], ast.Name):\n",
    "                var_name = node.targets[0].id\n",
    "                var_value = ast.unparse(node.value)\n",
    "                variables.append({\n",
    "                    \"name\": var_name,\n",
    "                    \"value\": var_value\n",
    "                })\n",
    "                \n",
    "        return variables\n",
    "    \n",
    "    def extract_imports(self, module_node):\n",
    "        \"\"\"Extract imports from a module.\"\"\"\n",
    "        imports = []\n",
    "        \n",
    "        for node in module_node.body:\n",
    "            if isinstance(node, ast.Import):\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"module\": name.name,\n",
    "                        \"alias\": name.asname\n",
    "                    })\n",
    "            elif isinstance(node, ast.ImportFrom):\n",
    "                module = node.module or \"\"\n",
    "                for name in node.names:\n",
    "                    imports.append({\n",
    "                        \"module\": module,\n",
    "                        \"name\": name.name,\n",
    "                        \"alias\": name.asname\n",
    "                    })\n",
    "                    \n",
    "        return imports\n",
    "    \n",
    "    def generate_module_api_doc(self, module_name, code):\n",
    "        \"\"\"Generate API documentation for a module.\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            module_doc = {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": self.extract_docstring(tree) or \"No module documentation available.\",\n",
    "                \"imports\": self.extract_imports(tree),\n",
    "                \"global_vars\": self.extract_global_vars(tree),\n",
    "                \"functions\": [],\n",
    "                \"classes\": []\n",
    "            }\n",
    "            \n",
    "            # Extract functions and classes\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    module_doc[\"functions\"].append(self.extract_function_info(node))\n",
    "                elif isinstance(node, ast.ClassDef):\n",
    "                    module_doc[\"classes\"].append(self.extract_class_info(node))\n",
    "            \n",
    "            return module_doc\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"Syntax error in module {module_name}: {e}\")\n",
    "            return self._fallback_api_doc_generation(module_name, code)\n",
    "    \n",
    "    def _fallback_api_doc_generation(self, module_name, code):\n",
    "        \"\"\"Use the LLM as a fallback for API doc generation when parsing fails.\"\"\"\n",
    "        logger.info(f\"Using LLM to extract API documentation for {module_name}\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate a detailed API documentation for the following Python code module.\n",
    "Extract all functions, classes, methods, and their parameters, return types, and docstrings.\n",
    "Format the response as a JSON object with the structure shown in the example.\n",
    "\n",
    "Example structure:\n",
    "```json\n",
    "{{\n",
    "  \"name\": \"module_name\",\n",
    "  \"docstring\": \"Module docstring\",\n",
    "  \"imports\": [\n",
    "    {{\"module\": \"os\", \"alias\": null}},\n",
    "    {{\"module\": \"pandas\", \"alias\": \"pd\"}}\n",
    "  ],\n",
    "  \"global_vars\": [\n",
    "    {{\"name\": \"logger\", \"value\": \"logging.getLogger(__name__)\"}}\n",
    "  ],\n",
    "  \"functions\": [\n",
    "    {{\n",
    "      \"name\": \"function_name\",\n",
    "      \"docstring\": \"Function docstring\",\n",
    "      \"parameters\": [\n",
    "        {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "      ],\n",
    "      \"returns\": \"str\"\n",
    "    }}\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {{\n",
    "      \"name\": \"ClassName\",\n",
    "      \"docstring\": \"Class docstring\",\n",
    "      \"bases\": [\"BaseClass\"],\n",
    "      \"methods\": [\n",
    "        {{\n",
    "          \"name\": \"method_name\",\n",
    "          \"docstring\": \"Method docstring\",\n",
    "          \"parameters\": [\n",
    "            {{\"name\": \"self\", \"type\": null, \"description\": \"Instance reference\"}},\n",
    "            {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "          ],\n",
    "          \"returns\": \"bool\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "\n",
    "Here's the code to document:\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Extract as much detail as possible. If you can't determine something exactly (like a type), make your best guess based on context.\n",
    "\"\"\"\n",
    "\n",
    "        system_message = SystemMessage(content=\"You are a Python expert who specializes in extracting API documentation from code.\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke([system_message, human_message])\n",
    "            content = response.content\n",
    "            \n",
    "            # Try to extract JSON from the response\n",
    "            json_match = re.search(r'```json\\s*(.*?)\\s*```', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                # If no JSON code block, try to find any JSON object\n",
    "                json_match = re.search(r'({[\\s\\S]*})', content)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(1)\n",
    "                else:\n",
    "                    json_str = content\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting API doc from LLM for {module_name}: {e}\")\n",
    "            # Return a minimal structure\n",
    "            return {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": \"Documentation extraction failed.\",\n",
    "                \"imports\": [],\n",
    "                \"global_vars\": [],\n",
    "                \"functions\": [],\n",
    "                \"classes\": []\n",
    "            }\n",
    "    \n",
    "    def generate_all_module_docs(self, code_contents):\n",
    "        \"\"\"Generate API documentation for all modules.\"\"\"\n",
    "        module_docs = {}\n",
    "        \n",
    "        for module_name, code in code_contents.items():\n",
    "            try:\n",
    "                module_doc = self.generate_module_api_doc(module_name, code)\n",
    "                module_docs[module_name] = module_doc\n",
    "                logger.info(f\"Generated API documentation for {module_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating API doc for {module_name}: {e}\")\n",
    "        \n",
    "        return module_docs\n",
    "    \n",
    "    def format_api_docs_for_llm(self, module_docs):\n",
    "        \"\"\"Format API documentation for use in LLM prompt.\"\"\"\n",
    "        formatted_docs = []\n",
    "        \n",
    "        for module_name, doc in module_docs.items():\n",
    "            module_text = [f\"MODULE: {module_name}_code.py\"]\n",
    "            \n",
    "            # Add module docstring\n",
    "            module_text.append(f\"Description: {doc['docstring']}\")\n",
    "            module_text.append(\"\")\n",
    "            \n",
    "            # Add imports\n",
    "            if doc[\"imports\"]:\n",
    "                module_text.append(\"Imports:\")\n",
    "                for imp in doc[\"imports\"]:\n",
    "                    if \"name\" in imp:\n",
    "                        from_txt = f\"from {imp['module']} \" if imp['module'] else \"from \"\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  {from_txt}import {imp['name']}{as_txt}\")\n",
    "                    else:\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  import {imp['module']}{as_txt}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add global variables\n",
    "            if doc[\"global_vars\"]:\n",
    "                module_text.append(\"Global Variables:\")\n",
    "                for var in doc[\"global_vars\"]:\n",
    "                    module_text.append(f\"  {var['name']} = {var['value']}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add classes\n",
    "            if doc[\"classes\"]:\n",
    "                module_text.append(\"Classes:\")\n",
    "                for cls in doc[\"classes\"]:\n",
    "                    bases = f\"({', '.join(cls['bases'])})\" if cls['bases'] else \"\"\n",
    "                    module_text.append(f\"  class {cls['name']}{bases}:\")\n",
    "                    module_text.append(f\"    \\\"{cls['docstring']}\\\"\")\n",
    "                    module_text.append(\"\")\n",
    "                    \n",
    "                    if cls[\"methods\"]:\n",
    "                        module_text.append(\"    Methods:\")\n",
    "                        for method in cls[\"methods\"]:\n",
    "                            params = []\n",
    "                            for p in method[\"parameters\"]:\n",
    "                                param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                                params.append(f\"{p['name']}{param_type}\")\n",
    "                            \n",
    "                            returns = f\" -> {method['returns']}\" if method['returns'] else \"\"\n",
    "                            module_text.append(f\"      def {method['name']}({', '.join(params)}){returns}:\")\n",
    "                            module_text.append(f\"        \\\"{method['docstring']}\\\"\")\n",
    "                            \n",
    "                            # Add parameter descriptions\n",
    "                            if any(p[\"description\"] != \"Parameter description not available.\" for p in method[\"parameters\"]):\n",
    "                                module_text.append(\"        Parameters:\")\n",
    "                                for p in method[\"parameters\"]:\n",
    "                                    if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                        module_text.append(f\"          {p['name']}: {p['description']}\")\n",
    "                            \n",
    "                            module_text.append(\"\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            # Add functions\n",
    "            if doc[\"functions\"]:\n",
    "                module_text.append(\"Functions:\")\n",
    "                for func in doc[\"functions\"]:\n",
    "                    params = []\n",
    "                    for p in func[\"parameters\"]:\n",
    "                        param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                        params.append(f\"{p['name']}{param_type}\")\n",
    "                    \n",
    "                    returns = f\" -> {func['returns']}\" if func['returns'] else \"\"\n",
    "                    module_text.append(f\"  def {func['name']}({', '.join(params)}){returns}:\")\n",
    "                    module_text.append(f\"    \\\"{func['docstring']}\\\"\")\n",
    "                    \n",
    "                    # Add parameter descriptions\n",
    "                    if any(p[\"description\"] != \"Parameter description not available.\" for p in func[\"parameters\"]):\n",
    "                        module_text.append(\"    Parameters:\")\n",
    "                        for p in func[\"parameters\"]:\n",
    "                            if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                module_text.append(f\"      {p['name']}: {p['description']}\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            formatted_docs.append(\"\\n\".join(module_text))\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs) + \"\\n\"\n",
    "\n",
    "class CodeIntegrator:\n",
    "    \"\"\"Class to integrate code modules based on API documentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, doc_generator):\n",
    "        self.model = model\n",
    "        self.doc_generator = doc_generator\n",
    "    \n",
    "    def create_integration_prompt(self, api_docs):\n",
    "        \"\"\"Create a prompt for the LLM to integrate the code.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "I have multiple Python modules that need to be integrated into a cohesive solution.\n",
    "Below is the API documentation for each module. Each module is stored in a separate file\n",
    "with the naming pattern of \"US_XXX_code.py\" where XXX is the user story ID.\n",
    "\n",
    "{api_docs}\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Create a single integrated Python file that coordinates functionality from all these modules\n",
    "2. Design the integrated solution to import modules correctly using their filenames (e.g., \"import US_142_code\" NOT \"import US_142\")\n",
    "3. Create proper references to functions and classes from each module with correct module prefixes\n",
    "4. Make sure to import all necessary standard and third-party libraries needed by the solution\n",
    "5. Ensure proper sequencing and dependencies between modules\n",
    "6. Include a main execution block that coordinates the overall flow\n",
    "7. Write clear comments to explain how the integration works\n",
    "\n",
    "IMPORTANT: Each module should be imported using its full filename (e.g., \"import US_142_code\" not \"import US_142\").\n",
    "When referring to functions, classes, or variables from these modules, use the proper module prefix\n",
    "(e.g., \"US_142_code.process_file()\" not \"US_142.process_file()\").\n",
    "\n",
    "Format your response as a single Python file with all necessary imports, functions, \n",
    "and a main execution block. Add helpful comments to explain your integration strategy.\n",
    "\n",
    "Return only the final integrated Python code without explanation or other text.\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def validate_integrated_code(self, code, module_names):\n",
    "        \"\"\"Validate that the integrated code properly imports all modules with correct names.\"\"\"\n",
    "        # Check if modules are imported with _code suffix\n",
    "        proper_imports = True\n",
    "        module_import_checks = []\n",
    "        \n",
    "        for module_name in module_names:\n",
    "            module_import_name = f\"{module_name}_code\"\n",
    "            if f\"import {module_name}\" in code and f\"import {module_import_name}\" not in code:\n",
    "                proper_imports = False\n",
    "                module_import_checks.append((module_name, False))\n",
    "            else:\n",
    "                module_import_checks.append((module_name, True))\n",
    "        \n",
    "        # Check for any functions or classes referenced without proper module prefix\n",
    "        improper_references = []\n",
    "        \n",
    "        for module_name in module_names:\n",
    "            # Look for patterns like \"ModuleName.function\" instead of \"ModuleName_code.function\"\n",
    "            pattern = fr\"{module_name}\\.[a-zA-Z0-9_]+\"\n",
    "            matches = re.findall(pattern, code)\n",
    "            if matches:\n",
    "                improper_references.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            \"proper_imports\": proper_imports,\n",
    "            \"module_import_checks\": module_import_checks,\n",
    "            \"improper_references\": improper_references\n",
    "        }\n",
    "    \n",
    "    def fix_integrated_code(self, code, validation_result):\n",
    "        \"\"\"Fix issues with the integrated code based on validation results.\"\"\"\n",
    "        fixed_code = code\n",
    "        \n",
    "        # Fix improper imports\n",
    "        for module_name, is_proper in validation_result[\"module_import_checks\"]:\n",
    "            if not is_proper:\n",
    "                # Replace \"import ModuleName\" with \"import ModuleName_code\"\n",
    "                fixed_code = re.sub(\n",
    "                    fr\"import\\s+{module_name}(?!_code)\",\n",
    "                    f\"import {module_name}_code\",\n",
    "                    fixed_code\n",
    "                )\n",
    "                \n",
    "                # Replace \"from ModuleName import\" with \"from ModuleName_code import\"\n",
    "                fixed_code = re.sub(\n",
    "                    fr\"from\\s+{module_name}(?!_code)\\s+import\",\n",
    "                    f\"from {module_name}_code import\",\n",
    "                    fixed_code\n",
    "                )\n",
    "        \n",
    "        # Fix improper references\n",
    "        for ref in validation_result[\"improper_references\"]:\n",
    "            module_name = ref.split('.')[0]\n",
    "            fixed_code = fixed_code.replace(ref, ref.replace(f\"{module_name}.\", f\"{module_name}_code.\"))\n",
    "        \n",
    "        return fixed_code\n",
    "    \n",
    "    def generate_integrated_code(self, module_docs):\n",
    "        \"\"\"Generate integrated code based on API documentation.\"\"\"\n",
    "        # Format API docs for the LLM\n",
    "        api_docs_text = self.doc_generator.format_api_docs_for_llm(module_docs)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = self.create_integration_prompt(api_docs_text)\n",
    "        \n",
    "        # Send to LLM\n",
    "        system_message = SystemMessage(content=\"\"\"You are a Python expert who specializes in integrating multiple code modules \n",
    "into cohesive solutions. You excel at understanding module dependencies and creating orchestration code.\"\"\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        logger.info(\"Sending API documentation to LLM for integration\")\n",
    "        response = self.model.invoke([system_message, human_message])\n",
    "        \n",
    "        # Extract code from response\n",
    "        content = response.content\n",
    "        \n",
    "        # Check if the response is wrapped in code blocks\n",
    "        if \"```python\" in content and \"```\" in content.split(\"```python\", 1)[1]:\n",
    "            # Extract code between the markers\n",
    "            code = content.split(\"```python\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "        elif \"```\" in content and content.count(\"```\") >= 2:\n",
    "            # Extract code between the markers\n",
    "            parts = content.split(\"```\", 2)\n",
    "            code = parts[1]\n",
    "            if code.startswith(\"python\"):\n",
    "                code = code[6:]\n",
    "            code = code.strip()\n",
    "        else:\n",
    "            # If not wrapped in code blocks, return as is\n",
    "            code = content\n",
    "        \n",
    "        # Validate and fix the code\n",
    "        validation_result = self.validate_integrated_code(code, module_docs.keys())\n",
    "        \n",
    "        if not validation_result[\"proper_imports\"] or validation_result[\"improper_references\"]:\n",
    "            logger.info(\"Fixing issues in the integrated code\")\n",
    "            code = self.fix_integrated_code(code, validation_result)\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    def generate_init_file(self, output_dir, module_docs):\n",
    "        \"\"\"Generate an __init__.py file to make importing modules easier.\"\"\"\n",
    "        init_content = ['\"\"\"Package initialization file.\"\"\"\\n']\n",
    "        \n",
    "        # Add imports for all modules\n",
    "        for module_name in module_docs.keys():\n",
    "            # Import the module\n",
    "            init_content.append(f\"import {module_name}_code\")\n",
    "            \n",
    "            # Create shorter aliases for convenience\n",
    "            init_content.append(f\"{module_name} = {module_name}_code\")\n",
    "        \n",
    "        # Write the file\n",
    "        init_path = os.path.join(output_dir, \"__init__.py\")\n",
    "        with open(init_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(init_content))\n",
    "        \n",
    "        logger.info(f\"Created __init__.py file at {init_path}\")\n",
    "\n",
    "def save_modules_with_proper_names(output_dir, code_contents):\n",
    "    \"\"\"Save individual modules with proper names based on user story IDs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for module_name, code in code_contents.items():\n",
    "        file_name = f\"{module_name}_code.py\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved module {module_name} to {file_path}\")\n",
    "\n",
    "def create_setup_py(output_dir, module_name=\"integrated_solution\"):\n",
    "    \"\"\"Create a setup.py file to make the package installable.\"\"\"\n",
    "    setup_content = f'''\n",
    "\"\"\"\n",
    "Setup script for {module_name} package.\n",
    "\"\"\"\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"{module_name}\",\n",
    "    version=\"0.1.0\",\n",
    "    packages=find_packages(),\n",
    "    author=\"AI Code Generator\",\n",
    "    author_email=\"ai@example.com\",\n",
    "    description=\"Integrated solution generated from multiple modules\",\n",
    "    classifiers=[\n",
    "        \"Programming Language :: Python :: 3\",\n",
    "        \"License :: OSI Approved :: MIT License\",\n",
    "        \"Operating System :: OS Independent\",\n",
    "    ],\n",
    "    python_requires=\">=3.6\",\n",
    ")\n",
    "'''\n",
    "    \n",
    "    setup_path = os.path.join(output_dir, \"setup.py\")\n",
    "    with open(setup_path, 'w') as f:\n",
    "        f.write(setup_content)\n",
    "    \n",
    "    logger.info(f\"Created setup.py file at {setup_path}\")\n",
    "\n",
    "def create_readme(output_dir, module_docs):\n",
    "    \"\"\"Create a README.md file with information about the integrated solution.\"\"\"\n",
    "    readme_content = [\n",
    "        \"# Integrated Solution\",\n",
    "        \"\",\n",
    "        \"This is an automatically generated integrated solution that combines functionality from multiple modules.\",\n",
    "        \"\",\n",
    "        \"## Modules\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    for module_name, doc in module_docs.items():\n",
    "        readme_content.append(f\"### {module_name}_code\")\n",
    "        readme_content.append(f\"{doc['docstring']}\")\n",
    "        \n",
    "        if doc[\"functions\"]:\n",
    "            readme_content.append(\"\\nFunctions:\")\n",
    "            for func in doc[\"functions\"]:\n",
    "                readme_content.append(f\"- `{func['name']}`: {func['docstring'].split('.')[0]}\")\n",
    "        \n",
    "        if doc[\"classes\"]:\n",
    "            readme_content.append(\"\\nClasses:\")\n",
    "            for cls in doc[\"classes\"]:\n",
    "                readme_content.append(f\"- `{cls['name']}`: {cls['docstring'].split('.')[0]}\")\n",
    "        \n",
    "        readme_content.append(\"\")\n",
    "    \n",
    "    readme_path = os.path.join(output_dir, \"README.md\")\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(readme_content))\n",
    "    \n",
    "    logger.info(f\"Created README.md file at {readme_path}\")\n",
    "\n",
    "def create_package_structure(output_dir, package_name=\"integrated_solution\"):\n",
    "    \"\"\"Create a proper Python package structure.\"\"\"\n",
    "    # Create package directory\n",
    "    package_dir = os.path.join(output_dir, package_name)\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    \n",
    "    # Create an empty __init__.py in the package directory\n",
    "    with open(os.path.join(package_dir, \"__init__.py\"), 'w') as f:\n",
    "        f.write('\"\"\"Package initialization file.\"\"\"\\n')\n",
    "    \n",
    "    logger.info(f\"Created package structure at {package_dir}\")\n",
    "    \n",
    "    return package_dir\n",
    "\n",
    "def improved_api_based_code_integrator():\n",
    "    \"\"\"\n",
    "    Agent that generates API documentation for code modules and uses that\n",
    "    to create an integrated solution with proper module imports.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check Azure OpenAI configuration\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            temperature=0.1  # Low temperature for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Find the latest code_generation folder\n",
    "        latest_folder = find_latest_code_generation_folder()\n",
    "        logger.info(f\"Found latest code generation folder: {latest_folder}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_dir = os.path.join(os.path.dirname(latest_folder), f\"improved_integrated_solution_{timestamp}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all code files\n",
    "        code_files = find_code_files(latest_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to integrate\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to integrate\")\n",
    "            return\n",
    "        \n",
    "        # Read all code files\n",
    "        code_contents = read_code_files(code_files)\n",
    "        \n",
    "        # Save modules with proper names\n",
    "        save_modules_with_proper_names(output_dir, code_contents)\n",
    "        \n",
    "        # Generate API documentation\n",
    "        doc_generator = APIDocGenerator(model)\n",
    "        module_docs = doc_generator.generate_all_module_docs(code_contents)\n",
    "        \n",
    "        # Save API documentation\n",
    "        api_docs_path = os.path.join(output_dir, \"api_documentation.json\")\n",
    "        with open(api_docs_path, 'w') as f:\n",
    "            json.dump(module_docs, f, indent=2)\n",
    "        logger.info(f\"Saved API documentation to {api_docs_path}\")\n",
    "        \n",
    "        # Generate integrated code\n",
    "        integrator = CodeIntegrator(model, doc_generator)\n",
    "        integrated_code = integrator.generate_integrated_code(module_docs)\n",
    "        \n",
    "        # Add header\n",
    "        header = f'''\"\"\"\n",
    "Improved API Documentation-Based Integrated Solution\n",
    "This file was automatically generated by the Improved API-Based Code Integrator.\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "This code serves as an integration layer that coordinates all the individual modules.\n",
    "Each module's code is stored in separate files named by their user story IDs with \"_code.py\" suffix.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "        integrated_code = header + integrated_code\n",
    "        \n",
    "        # Save integrated code\n",
    "        integrated_code_path = os.path.join(output_dir, \"integrated_solution.py\")\n",
    "        with open(integrated_code_path, 'w') as f:\n",
    "            f.write(integrated_code)\n",
    "        \n",
    "        # Create __init__.py file\n",
    "        integrator.generate_init_file(output_dir, module_docs)\n",
    "        \n",
    "        # Create README.md\n",
    "        create_readme(output_dir, module_docs)\n",
    "        \n",
    "        # Create setup.py\n",
    "        create_setup_py(output_dir)\n",
    "        \n",
    "        logger.info(f\"Successfully created improved integrated solution: {integrated_code_path}\")\n",
    "        print(f\"Improved integrated solution created at: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in improved API-based code integrator: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    improved_api_based_code_integrator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 16:33:25,664 - __main__ - INFO - [3915235524.py:62] - Azure OpenAI configuration verified\n",
      "2025-03-07 16:33:25,671 - __main__ - INFO - [3915235524.py:1348] - Found latest code generation folder: /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903\n",
      "2025-03-07 16:33:25,672 - __main__ - INFO - [3915235524.py:106] - Using 'final_corrected' code for US_145\n",
      "2025-03-07 16:33:25,672 - __main__ - INFO - [3915235524.py:106] - Using 'initial' code for US_147\n",
      "2025-03-07 16:33:25,672 - __main__ - INFO - [3915235524.py:106] - Using 'final_corrected' code for US_146\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:106] - Using 'final_corrected' code for US_141\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:106] - Using 'final_corrected' code for US_142\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:106] - Using 'final_corrected' code for US_144\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:106] - Using 'initial' code for US_143\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:1357] - Found 7 code files to integrate\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:123] - Read 8979 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_145/attempt_final_corrected/code.py\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:123] - Read 7244 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_147/attempt_initial/code.py\n",
      "2025-03-07 16:33:25,673 - __main__ - INFO - [3915235524.py:123] - Read 7347 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_146/attempt_final_corrected/code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:123] - Read 7881 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_141/attempt_final_corrected/code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:123] - Read 7613 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_142/attempt_final_corrected/code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:123] - Read 7327 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_144/attempt_final_corrected/code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:123] - Read 8568 bytes from /home/airangers/Desktop/shivani/SDLC/code_generation_20250306_014903/US_143/attempt_initial/code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:1081] - Saved module US_145 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_145_code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:1081] - Saved module US_147 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_147_code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:1081] - Saved module US_146 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_146_code.py\n",
      "2025-03-07 16:33:25,674 - __main__ - INFO - [3915235524.py:1081] - Saved module US_141 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_141_code.py\n",
      "2025-03-07 16:33:25,675 - __main__ - INFO - [3915235524.py:1081] - Saved module US_142 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_142_code.py\n",
      "2025-03-07 16:33:25,675 - __main__ - INFO - [3915235524.py:1081] - Saved module US_144 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_144_code.py\n",
      "2025-03-07 16:33:25,675 - __main__ - INFO - [3915235524.py:1081] - Saved module US_143 to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/US_143_code.py\n",
      "/tmp/ipykernel_548766/3915235524.py:254: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_548766/3915235524.py:255: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  docstring = node.body[0].value.s.strip()\n",
      "2025-03-07 16:33:25,676 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_145\n",
      "2025-03-07 16:33:25,677 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_147\n",
      "/tmp/ipykernel_548766/3915235524.py:207: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_548766/3915235524.py:208: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  docstring = node.body[0].value.s.strip()\n",
      "2025-03-07 16:33:25,679 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_146\n",
      "2025-03-07 16:33:25,680 - __main__ - ERROR - [3915235524.py:462] - Syntax error in module US_141: parameter without a default follows parameter with a default (<unknown>, line 163)\n",
      "2025-03-07 16:33:25,680 - __main__ - INFO - [3915235524.py:493] - Using LLM to extract enhanced API documentation for US_141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 16:33:59,237 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-07 16:33:59,239 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_141\n",
      "/tmp/ipykernel_548766/3915235524.py:254: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  isinstance(node.body[0].value, ast.Str)):\n",
      "/tmp/ipykernel_548766/3915235524.py:255: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  docstring = node.body[0].value.s.strip()\n",
      "2025-03-07 16:33:59,242 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_142\n",
      "2025-03-07 16:33:59,245 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_144\n",
      "2025-03-07 16:33:59,248 - __main__ - INFO - [3915235524.py:630] - Generated enhanced API documentation for US_143\n",
      "2025-03-07 16:33:59,250 - __main__ - INFO - [3915235524.py:1377] - Saved enhanced API documentation to /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/enhanced_api_documentation.json\n",
      "2025-03-07 16:33:59,250 - __main__ - INFO - [3915235524.py:1209] - Created detailed relationship documentation at /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/RELATIONSHIPS.md\n",
      "2025-03-07 16:33:59,251 - __main__ - WARNING - [3915235524.py:1396] - matplotlib or networkx not available, skipping graph generation\n",
      "2025-03-07 16:33:59,251 - __main__ - INFO - [3915235524.py:998] - Sending enhanced API documentation to LLM for integration\n",
      "2025-03-07 16:33:59,920 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-03-07 16:33:59,920 - openai._base_client - INFO - [_base_client.py:1092] - Retrying request to /chat/completions in 27.000000 seconds\n",
      "2025-03-07 16:34:42,928 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://openaisk123.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-03-07 16:34:42,930 - __main__ - INFO - [3915235524.py:1068] - Created enhanced __init__.py file at /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/__init__.py\n",
      "2025-03-07 16:34:42,930 - __main__ - INFO - [3915235524.py:1326] - Created enhanced README.md file at /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/README.md\n",
      "2025-03-07 16:34:42,931 - __main__ - INFO - [3915235524.py:1240] - Created setup.py file at /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/setup.py\n",
      "2025-03-07 16:34:42,931 - __main__ - INFO - [3915235524.py:1432] - Successfully created relationship-enhanced integrated solution: /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325/integrated_solution.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship-enhanced integrated solution created at: /home/airangers/Desktop/shivani/SDLC/relationship_enhanced_solution_20250307_163325\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict, Annotated, List, Dict, Tuple, Optional, Set, Any\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyMessage, SystemMessage, HumanMessage, AIMessage\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Integrated Code Generator and Relationship-Enhanced Integrator\n",
    "\n",
    "This script provides an end-to-end solution that:\n",
    "1. Reads technical specifications from an Excel file\n",
    "2. Generates production-ready Python code for each specification\n",
    "3. Validates and corrects the generated code\n",
    "4. Analyzes the code to extract relationships between functions, classes, and modules\n",
    "5. Generates comprehensive API documentation with relationship details\n",
    "6. Creates an integrated solution that properly imports and coordinates all modules\n",
    "7. Produces supporting documentation and visualization of relationships\n",
    "\n",
    "The workflow is completely automated from specification to final integrated solution.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import textwrap\n",
    "import shutil\n",
    "from typing import TypedDict, Annotated, List, Dict, Tuple, Optional, Set, Any\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Azure OpenAI configuration - Used across both parts of the script\n",
    "def check_openai_config():\n",
    "    \"\"\"Check if Azure OpenAI config is set in environment variables.\"\"\"\n",
    "    required_vars = [\n",
    "        \"AZURE_OPENAI_API_KEY\",\n",
    "        \"AZURE_OPENAI_ENDPOINT\",\n",
    "        \"AZURE_OPENAI_API_VERSION\",\n",
    "        \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"\n",
    "    ]\n",
    "    \n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        # Check if we can set values explicitly\n",
    "        logger.info(\"Looking for OpenAI configuration in current environment...\")\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_KEY\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0bf3daeba1814d03b5d62e1da4077478\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_ENDPOINT\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://openaisk123.openai.azure.com/\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_API_VERSION\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "        \n",
    "        if \"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\" not in os.environ:\n",
    "            os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "    \n",
    "    # Verify all variables are set\n",
    "    missing = [var for var in required_vars if not os.environ.get(var)]\n",
    "    if missing:\n",
    "        raise EnvironmentError(f\"Missing Azure OpenAI configuration: {', '.join(missing)}\")\n",
    "    \n",
    "    logger.info(\"Azure OpenAI configuration verified\")\n",
    "\n",
    "#############################################################\n",
    "# PART 1: Code Generation from Technical Specifications\n",
    "#############################################################\n",
    "\n",
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: str\n",
    "    validation_status: bool\n",
    "    error_messages: list[str]\n",
    "    is_valid: bool\n",
    "    user_story_id: str\n",
    "\n",
    "# Define prompts for code generation\n",
    "developer_prompt = \"\"\"\n",
    "Role: Python Developer\n",
    "Task: Generate complete, production-ready Python code based on the requirements specification.\n",
    "\n",
    "Requirements:\n",
    "{requirements}\n",
    "\n",
    "Your code must include:\n",
    "1. All necessary imports and dependencies\n",
    "2. Complete implementation with:\n",
    "   - Well-structured classes and functions\n",
    "   - Configuration management (using dataclasses or similar)\n",
    "   - Comprehensive error handling and validation\n",
    "   - Type hints throughout\n",
    "   - Logging with appropriate levels\n",
    "   - Unit tests where applicable\n",
    "3. Clear documentation:\n",
    "   - Module docstrings\n",
    "   - Function/method docstrings with parameters and return values\n",
    "   - Inline comments for complex logic\n",
    "\n",
    "Focus on implementing EVERY aspect mentioned in the requirements. Do not leave any required functionality unimplemented.\n",
    "\n",
    "## Output Format\n",
    "Your response should be the complete, production-ready Python code without surrounding explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "\n",
    "# Your Python code here\n",
    "\"\"\"\n",
    "\n",
    "validator_prompt = \"\"\"\n",
    "Role: Senior Code Reviewer\n",
    "Task: Perform a thorough validation of the provided Python code against the requirements.\n",
    "\n",
    "Requirements:\n",
    "{Requirements}\n",
    "\n",
    "Validation Process:\n",
    "1. Carefully compare the code against EACH requirement in the specification\n",
    "2. For each requirement, determine if it has been fully, partially, or not implemented\n",
    "3. Identify any missing functionality, edge cases, or requirements\n",
    "4. Evaluate code quality, error handling, security, and performance\n",
    "\n",
    "Validation Checklist:\n",
    "1. Code Completeness:\n",
    "   - All imports and dependencies present\n",
    "   - Full implementation of required functionality (check EACH requirement)\n",
    "   - No placeholder code or TODOs\n",
    "\n",
    "2. Code Quality:\n",
    "   - Follows PEP 8 standards\n",
    "   - Clear variable/function naming\n",
    "   - Appropriate modularization\n",
    "   - Avoids code duplication\n",
    "   - Maintainable architecture\n",
    "\n",
    "3. Technical Implementation:\n",
    "   - Proper error handling with specific exceptions\n",
    "   - Complete type annotations\n",
    "   - Correct algorithm implementation\n",
    "   - Efficient resource usage\n",
    "   - Security considerations addressed\n",
    "\n",
    "4. Documentation:\n",
    "   - Comprehensive docstrings\n",
    "   - Clear inline comments where needed\n",
    "\n",
    "## Output Format\n",
    "Return your validation report as a structured JSON object with the following format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"validation_report\": {{\n",
    "    \"overall_assessment\": \"Pass/Fail\",\n",
    "    \"issues_found\": [\n",
    "      \"Issue 1 description\",\n",
    "      \"Issue 2 description\",\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"suggested_improvements\": [\n",
    "      {{\n",
    "        \"description\": \"Improvement 1\",\n",
    "        \"priority\": \"high/medium/low\"\n",
    "      }},\n",
    "      \"...\"\n",
    "    ],\n",
    "    \"implementation_vs_requirements\": {{\n",
    "      \"match\": true/false,\n",
    "      \"details\": [\n",
    "        {{\n",
    "          \"requirement_section\": \"Requirement name/section\",\n",
    "          \"status\": \"Implemented/Partially Implemented/Not Implemented\",\n",
    "          \"notes\": \"Notes about implementation\"\n",
    "        }},\n",
    "        \"...\"\n",
    "      ]\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Be strict in your assessment. If ANY requirement is not fully implemented, the overall assessment should be \"Fail\".\n",
    "\"\"\"\n",
    "\n",
    "corrector_prompt = \"\"\"\n",
    "Role: Senior Python Developer\n",
    "Task: Refactor and fix the code based on the validation feedback.\n",
    "Original Requirements:\n",
    "{requirements}\n",
    "Validation Feedback:\n",
    "{ValidationFeedback}\n",
    "Correction Instructions:\n",
    "\n",
    "Address ALL issues identified in the validation feedback\n",
    "Pay particular attention to any requirements marked as \"Not Implemented\" or \"Partially Implemented\"\n",
    "Maintain the original architectural approach unless fundamentally flawed\n",
    "Ensure complete implementation of ALL requirements from the original specification\n",
    "Add or improve:\n",
    "\n",
    "Error handling for all edge cases\n",
    "Type hints throughout the codebase\n",
    "Documentation (docstrings and comments)\n",
    "Logging for important operations\n",
    "Performance optimizations where possible\n",
    "\n",
    "Important: Make sure you implement EVERY feature mentioned in the requirements that was flagged as missing or incomplete in the validation feedback.\n",
    "Output Format\n",
    "Your response should be the complete, corrected, production-ready Python code without explanations.\n",
    "DO NOT enclose your code in triple backticks (``` or ''').\n",
    "Simply output the pure Python code directly:\n",
    "Your corrected Python code here\n",
    "\"\"\"\n",
    "\n",
    "def extract_user_story_id(user_story_text):\n",
    "    \"\"\"\n",
    "    Extract user story ID from the text that contains 'User Story ID: XXX'\n",
    "    \n",
    "    Args:\n",
    "        user_story_text (str): The full user story text\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted user story ID or 'unknown_id' if not found\n",
    "    \"\"\"\n",
    "    # Look for \"User Story ID: XXX\" pattern\n",
    "    match = re.search(r'User\\s+Story\\s+ID\\s*:\\s*(\\d+)', user_story_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"US_{match.group(1)}\"\n",
    "    \n",
    "    # Alternative pattern - look for \"userstory1\" or similar patterns at the start of a line\n",
    "    match = re.search(r'^(?:(?:user)?story|us)(\\d+)', user_story_text, re.IGNORECASE | re.MULTILINE)\n",
    "    if match:\n",
    "        return f\"US_{match.group(1)}\"\n",
    "    \n",
    "    # If no ID is found, generate a fallback ID based on a hash of the content\n",
    "    logger.warning(\"No user story ID found in text, using fallback ID\")\n",
    "    import hashlib\n",
    "    hash_id = hashlib.md5(user_story_text.encode()).hexdigest()[:8]\n",
    "    return f\"Unknown_ID_{hash_id}\"\n",
    "\n",
    "def read_tech_specs_from_excel(excel_file_path):\n",
    "    \"\"\"\n",
    "    Read technical specifications from Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'user_story_id': ID of the user story\n",
    "        - 'tech_spec': Technical specification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Find the user story column and tech spec column\n",
    "        user_story_col = None\n",
    "        tech_spec_col = None\n",
    "        \n",
    "        # Determine column names - assuming first row has column headers\n",
    "        col_names = df.columns.tolist()\n",
    "        \n",
    "        # Find user story column\n",
    "        for col in col_names:\n",
    "            if 'user' in str(col).lower() and 'story' in str(col).lower():\n",
    "                user_story_col = col\n",
    "                break\n",
    "        \n",
    "        # Find tech spec column\n",
    "        for col in col_names:\n",
    "            if ('tech' in str(col).lower() and 'spec' in str(col).lower()) or 'requirement' in str(col).lower():\n",
    "                tech_spec_col = col\n",
    "                break\n",
    "        \n",
    "        # If we didn't find the right columns, default to the first two\n",
    "        if user_story_col is None and len(col_names) > 0:\n",
    "            user_story_col = col_names[0]\n",
    "        \n",
    "        if tech_spec_col is None and len(col_names) > 1:\n",
    "            tech_spec_col = col_names[1]\n",
    "        \n",
    "        logger.info(f\"Using columns: User Story = '{user_story_col}', Tech Spec = '{tech_spec_col}'\")\n",
    "        \n",
    "        # Extract tech specs\n",
    "        tech_specs = []\n",
    "        \n",
    "        # Skip the first row if it's empty (which appears to be the case)\n",
    "        start_row = 1 if df.iloc[0].isna().all() else 0\n",
    "        \n",
    "        for idx, row in df.iloc[start_row:].iterrows():\n",
    "            if pd.isna(row[user_story_col]) or pd.isna(row[tech_spec_col]):\n",
    "                logger.warning(f\"Skipping row {idx} due to missing data\")\n",
    "                continue\n",
    "                \n",
    "            user_story_text = str(row[user_story_col])\n",
    "            tech_spec_text = str(row[tech_spec_col])\n",
    "            \n",
    "            # Extract user story ID using the helper function\n",
    "            user_story_id = extract_user_story_id(user_story_text)\n",
    "            \n",
    "            tech_specs.append({\n",
    "                'user_story_id': user_story_id,\n",
    "                'tech_spec': tech_spec_text\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(tech_specs)} tech specs from Excel file\")\n",
    "        return tech_specs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading Excel file: {e}\")\n",
    "        raise\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model, base_output_dir=None, system_developer=\"\", system_validator=\"\", system_corrector=\"\"):\n",
    "        self.system_developer = system_developer\n",
    "        self.system_validator = system_validator\n",
    "        self.system_corrector = system_corrector\n",
    "        \n",
    "        # Create output base directory\n",
    "        self.base_output_dir = base_output_dir or f\"code_generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.base_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        \n",
    "        # Add conditional edges (matching notebook pattern)\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\", \n",
    "            lambda state: state[\"is_valid\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        graph.add_edge(\"correction\", END)\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile()\n",
    "        self.model = model\n",
    "        \n",
    "        # Try to display graph visualization if possible (for notebook environments)\n",
    "        try:\n",
    "            from IPython.display import Image, display\n",
    "            display(Image(self.graph.get_graph().draw_mermaid_png()))\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Could not display graph: {e}\")\n",
    "            pass\n",
    "\n",
    "    def get_output_dir(self, user_story_id):\n",
    "        \"\"\"Create and return a user story specific output directory\"\"\"\n",
    "        # Create user story specific directory if it doesn't exist\n",
    "        user_story_dir = os.path.join(self.base_output_dir, user_story_id)\n",
    "        os.makedirs(user_story_dir, exist_ok=True)\n",
    "        return user_story_dir\n",
    "\n",
    "    def extract_code(self, text):\n",
    "        \"\"\"Extract code from between triple backticks or triple single quotes\"\"\"\n",
    "        pattern = r\"```(?:python)?\\\\s*(.*?)```\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        # Try with triple single quotes\n",
    "        pattern = r\"'''(?:python)?\\\\s*(.*?)'''\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[0].strip()\n",
    "            \n",
    "        return text  # Return original if no code blocks found\n",
    "\n",
    "    def save_code_attempt(self, code: str, user_story_id: str, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt and return directory path\"\"\"\n",
    "        # Get user story specific output directory\n",
    "        output_dir = self.get_output_dir(user_story_id)\n",
    "        \n",
    "        attempt_dir = os.path.join(output_dir, f\"attempt_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        # Save code\n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved code attempt to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def developer(self, state: CodeGenerationState):\n",
    "        \"\"\"Generate initial code\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        logger.info(f\"Processing user story ID: {user_story_id}\")\n",
    "        print(f\"developer - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_developer:\n",
    "            # Format the prompt\n",
    "            formatted_prompt = self.system_developer.format(\n",
    "                requirements=messages[0].content,\n",
    "                TechnicalSpecifications=messages[0].content\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        \n",
    "        # Extract code from response\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save code\n",
    "        self.save_code_attempt(code_only, user_story_id)\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'validation_status': None,\n",
    "            'error_messages': [],\n",
    "            'is_valid': False,\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState):\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        messages = state.get('messages', [])\n",
    "        current_code = state.get('current_code', '')\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        \n",
    "        print(f\"validate - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_validator:\n",
    "            original_message = state[\"messages\"][0].content if state[\"messages\"] else \"\"\n",
    "            # Format the prompt\n",
    "            formatted_prompt = self.system_validator.format(\n",
    "                Requirements=original_message,\n",
    "                TechnicalSpecifications=original_message\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\").lower()\n",
    "        \n",
    "        # Attempt to determine if validation passed by extracting JSON\n",
    "        is_valid = False\n",
    "        try:\n",
    "            # Try to extract JSON from the message\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                is_valid = (validation_json.get(\"validation_report\", {}).get(\"overall_assessment\", \"\").lower() == \"pass\")\n",
    "        except:\n",
    "            # Fallback to the original logic if JSON extraction fails\n",
    "            is_valid = \"pass\" in response_text and \"correctly implements\" in response_text\n",
    "        \n",
    "        # Save validation results to JSON if possible\n",
    "        try:\n",
    "            json_pattern = r\"```json\\s*(.*?)\\s*```\"\n",
    "            match = re.search(json_pattern, message.content, re.DOTALL)\n",
    "            if match:\n",
    "                validation_json = json.loads(match.group(1))\n",
    "                output_dir = self.get_output_dir(user_story_id)\n",
    "                json_path = os.path.join(output_dir, \"validation_results.json\")\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(validation_json, f, indent=2)\n",
    "                logger.info(f\"Saved validation results to {json_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save validation results: {e}\")\n",
    "        \n",
    "        if is_valid:\n",
    "            self.save_code_attempt(current_code, user_story_id, \"validated_pass\")\n",
    "        else:\n",
    "            self.save_code_attempt(current_code, user_story_id, \"validated_fail\")\n",
    "            \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': current_code,\n",
    "            'is_valid': is_valid,\n",
    "            'error_messages': [] if is_valid else [\"Validation failed\"],\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState):\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        messages = state['messages']\n",
    "        user_story_id = state.get('user_story_id', 'default_id')\n",
    "        \n",
    "        print(f\"correction - User Story ID: {user_story_id}\")\n",
    "        \n",
    "        if self.system_corrector:\n",
    "            # Get original requirements from the first human message in the chain\n",
    "            original_requirements = \"\"\n",
    "            for msg in state['messages']:\n",
    "                if isinstance(msg, HumanMessage) and msg.content:\n",
    "                    original_requirements = msg.content\n",
    "                    break\n",
    "            \n",
    "            # Get validation feedback from the most recent message\n",
    "            validation_feedback = messages[0].content if messages else \"\"\n",
    "            \n",
    "            # Format the prompt\n",
    "            formatted_prompt = self.system_corrector.format(\n",
    "                requirements=original_requirements,\n",
    "                ValidationFeedback=validation_feedback\n",
    "            )\n",
    "            messages = [SystemMessage(content=formatted_prompt)] + messages\n",
    "        \n",
    "        message = self.model.invoke(messages)\n",
    "        response_text = getattr(message, \"content\", \"\")\n",
    "        code_only = self.extract_code(response_text)\n",
    "        \n",
    "        # Save corrected code\n",
    "        self.save_code_attempt(code_only, user_story_id, \"correction\")\n",
    "        \n",
    "        return {\n",
    "            'messages': [message],\n",
    "            'current_code': code_only,\n",
    "            'is_valid': False,\n",
    "            'error_messages': [],\n",
    "            'user_story_id': user_story_id\n",
    "        }\n",
    "\n",
    "def process_tech_specs(excel_file_path=\"tech.xlsx\"):\n",
    "    \"\"\"\n",
    "    Process tech specs from an Excel file\n",
    "    \n",
    "    Args:\n",
    "        excel_file_path: Path to Excel file with tech specs\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the output directory with generated code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize OpenAI configuration\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Read tech specs from Excel\n",
    "        tech_specs = read_tech_specs_from_excel(excel_file_path)\n",
    "        \n",
    "        if not tech_specs:\n",
    "            logger.error(\"No tech specs found in Excel file\")\n",
    "            return None\n",
    "        \n",
    "        # Model initialization\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"]\n",
    "        )\n",
    "        \n",
    "        # Create a base output directory\n",
    "        base_output_dir = f\"code_generation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        # Initialize code generator with the base directory\n",
    "        code_gen = CodeGenerator(\n",
    "            model=model, \n",
    "            base_output_dir=base_output_dir,\n",
    "            system_developer=developer_prompt,\n",
    "            system_validator=validator_prompt,\n",
    "            system_corrector=corrector_prompt\n",
    "        )\n",
    "        \n",
    "        # Process each tech spec\n",
    "        for idx, spec in enumerate(tech_specs):\n",
    "            user_story_id = spec['user_story_id']\n",
    "            tech_spec = spec['tech_spec']\n",
    "            \n",
    "            logger.info(f\"Processing tech spec for user story ID: {user_story_id} ({idx+1}/{len(tech_specs)})\")\n",
    "            \n",
    "            # Setup initial message\n",
    "            messages = [HumanMessage(content=tech_spec)]\n",
    "            \n",
    "            # Set up the input state\n",
    "            initial_state = {\n",
    "                \"messages\": messages,\n",
    "                \"current_code\": \"\",\n",
    "                \"validation_status\": None,\n",
    "                \"error_messages\": [],\n",
    "                \"is_valid\": False,\n",
    "                \"user_story_id\": user_story_id\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Run the graph\n",
    "                result = code_gen.graph.invoke(initial_state)\n",
    "                \n",
    "                # Log success\n",
    "                logger.info(f\"Successfully processed tech spec for user story ID: {user_story_id}\")\n",
    "                \n",
    "                # Extract final code\n",
    "                if 'current_code' in result and result['current_code']:\n",
    "                    final_status = \"final_corrected\" if not result.get('is_valid', False) else \"final_validated\"\n",
    "                    code_gen.save_code_attempt(result['current_code'], user_story_id, final_status)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing tech spec for user story ID {user_story_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        logger.info(f\"Completed processing all tech specs. Output directory: {base_output_dir}\")\n",
    "        return base_output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process_tech_specs: {e}\")\n",
    "        raise\n",
    "\n",
    "#############################################################\n",
    "# PART 2: Code Integration with Relationship Enhancement\n",
    "#############################################################\n",
    "\n",
    "def find_latest_code_generation_folder(base_dir=None):\n",
    "    \"\"\"Find the latest code_generation folder based on creation time.\"\"\"\n",
    "    if base_dir is None:\n",
    "        base_dir = os.getcwd()  # Current working directory\n",
    "    \n",
    "    code_gen_folders = [d for d in os.listdir(base_dir) if d.startswith(\"code_generation_\") and os.path.isdir(os.path.join(base_dir, d))]\n",
    "    if not code_gen_folders:\n",
    "        raise FileNotFoundError(\"No code_generation folders found\")\n",
    "    \n",
    "    # Sort by creation time, most recent first\n",
    "    code_gen_folders.sort(key=lambda d: os.path.getctime(os.path.join(base_dir, d)), reverse=True)\n",
    "    return os.path.join(base_dir, code_gen_folders[0])\n",
    "\n",
    "def find_code_files(base_folder):\n",
    "    \"\"\"\n",
    "    Find all code files in subfolders.\n",
    "    Prioritize files in this order:\n",
    "    1. final_corrected\n",
    "    2. final_validated\n",
    "    3. correction\n",
    "    4. validated_pass\n",
    "    5. initial (fallback)\n",
    "    \"\"\"\n",
    "    code_files = []\n",
    "    \n",
    "    # Priority order for folder names\n",
    "    priority_folders = [\"final_corrected\", \"final_validated\", \"correction\", \"validated_pass\", \"initial\"]\n",
    "    \n",
    "    # First, get all user story folders\n",
    "    user_story_folders = [f for f in os.listdir(base_folder) \n",
    "                         if os.path.isdir(os.path.join(base_folder, f))]\n",
    "    \n",
    "    for user_folder in user_story_folders:\n",
    "        user_path = os.path.join(base_folder, user_folder)\n",
    "        \n",
    "        # Check each priority folder type\n",
    "        found = False\n",
    "        for priority in priority_folders:\n",
    "            attempt_path = os.path.join(user_path, f\"attempt_{priority}\")\n",
    "            code_file = os.path.join(attempt_path, \"code.py\")\n",
    "            \n",
    "            if os.path.exists(code_file):\n",
    "                code_files.append((user_folder, code_file))\n",
    "                found = True\n",
    "                logger.info(f\"Using '{priority}' code for {user_folder}\")\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            logger.warning(f\"No code files found at all for {user_folder}\")\n",
    "    \n",
    "    return code_files\n",
    "\n",
    "def read_code_files(code_files):\n",
    "    \"\"\"Read code files and return a dictionary mapping module names to code content.\"\"\"\n",
    "    code_contents = {}\n",
    "    \n",
    "    for module_name, file_path in code_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                code_contents[module_name] = content\n",
    "                logger.info(f\"Read {len(content)} bytes from {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return code_contents\n",
    "\n",
    "def get_docstring_summary(docstring):\n",
    "    \"\"\"Extract the first sentence of a docstring or return a default message.\"\"\"\n",
    "    if not docstring:\n",
    "        return \"No documentation available\"\n",
    "    \n",
    "    # Try to get the first sentence\n",
    "    if '.' in docstring:\n",
    "        return docstring.split('.')[0].strip()\n",
    "    \n",
    "    return docstring.strip()\n",
    "\n",
    "class RelationshipVisitor(ast.NodeVisitor):\n",
    "    \"\"\"AST visitor that extracts relationships between functions, classes, and variables.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.defined_names = set()  # All defined names in the module\n",
    "        self.function_calls = defaultdict(set)  # Mapping of function name to the set of function names it calls\n",
    "        self.class_instantiations = defaultdict(set)  # Mapping of function name to the set of class names it instantiates\n",
    "        self.attribute_accesses = defaultdict(set)  # Mapping of function/method name to the attributes it accesses\n",
    "        self.imports = []  # List of import statements\n",
    "        self.global_vars = []  # List of global variables\n",
    "        self.functions = []  # List of functions\n",
    "        self.classes = []  # List of classes\n",
    "        \n",
    "        # Track current context (function or class being processed)\n",
    "        self.current_function = None\n",
    "        self.current_class = None\n",
    "        self.current_method = None\n",
    "        \n",
    "        # Track known external names\n",
    "        self.external_modules = set()\n",
    "        \n",
    "    def visit_Import(self, node):\n",
    "        \"\"\"Process import statements.\"\"\"\n",
    "        for name in node.names:\n",
    "            import_name = name.name\n",
    "            alias = name.asname or import_name\n",
    "            self.imports.append({\n",
    "                \"module\": import_name,\n",
    "                \"alias\": name.asname\n",
    "            })\n",
    "            self.defined_names.add(alias)\n",
    "            self.external_modules.add(alias)\n",
    "        self.generic_visit(node)\n",
    "    \n",
    "    def visit_ImportFrom(self, node):\n",
    "        \"\"\"Process from ... import ... statements.\"\"\"\n",
    "        module = node.module or \"\"\n",
    "        for name in node.names:\n",
    "            import_name = name.name\n",
    "            alias = name.asname or import_name\n",
    "            self.imports.append({\n",
    "                \"module\": module,\n",
    "                \"name\": import_name,\n",
    "                \"alias\": name.asname\n",
    "            })\n",
    "            self.defined_names.add(alias)\n",
    "        self.generic_visit(node)\n",
    "    \n",
    "    def visit_ClassDef(self, node):\n",
    "        \"\"\"Process class definitions.\"\"\"\n",
    "        class_name = node.name\n",
    "        self.defined_names.add(class_name)\n",
    "        \n",
    "        # Extract base classes\n",
    "        bases = []\n",
    "        for base in node.bases:\n",
    "            if isinstance(base, ast.Name):\n",
    "                bases.append(base.id)\n",
    "            else:\n",
    "                try:\n",
    "                    bases.append(ast.unparse(base))\n",
    "                except:\n",
    "                    bases.append(str(base))\n",
    "        \n",
    "        # Extract docstring\n",
    "        docstring = None\n",
    "        if (node.body and isinstance(node.body[0], ast.Expr) and \n",
    "            isinstance(node.body[0].value, ast.Str)):\n",
    "            docstring = node.body[0].value.s.strip()\n",
    "        \n",
    "        # Save the current class context\n",
    "        prev_class = self.current_class\n",
    "        self.current_class = class_name\n",
    "        \n",
    "        # Process the class body\n",
    "        methods = []\n",
    "        for child in node.body:\n",
    "            if isinstance(child, ast.FunctionDef):\n",
    "                # This is a method\n",
    "                method_info = self.process_function(child, is_method=True)\n",
    "                if method_info:\n",
    "                    methods.append(method_info)\n",
    "        \n",
    "        # Add class info\n",
    "        self.classes.append({\n",
    "            \"name\": class_name,\n",
    "            \"docstring\": docstring or \"No documentation available.\",\n",
    "            \"bases\": bases,\n",
    "            \"methods\": methods,\n",
    "            \"relationships\": {\n",
    "                \"inherits_from\": bases,\n",
    "                \"used_by_functions\": [],  # Will be filled later\n",
    "                \"instantiated_by\": []  # Will be filled later\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Restore previous class context\n",
    "        self.current_class = prev_class\n",
    "    \n",
    "    def process_function(self, node, is_method=False):\n",
    "        \"\"\"Process function or method definition.\"\"\"\n",
    "        func_name = node.name\n",
    "        \n",
    "        # Skip if it's a special method (like __init__) - we'll still process its body though\n",
    "        skip_adding = False\n",
    "        if is_method and func_name.startswith('__') and func_name.endswith('__'):\n",
    "            skip_adding = True\n",
    "        \n",
    "        # For methods, the full name includes the class name\n",
    "        full_name = f\"{self.current_class}.{func_name}\" if is_method and self.current_class else func_name\n",
    "        \n",
    "        # Extract docstring\n",
    "        docstring = None\n",
    "        if (node.body and isinstance(node.body[0], ast.Expr) and \n",
    "            isinstance(node.body[0].value, ast.Str)):\n",
    "            docstring = node.body[0].value.s.strip()\n",
    "        \n",
    "        # Extract parameters\n",
    "        parameters = []\n",
    "        for arg in node.args.args:\n",
    "            param_name = arg.arg\n",
    "            param_type = None\n",
    "            if arg.annotation:\n",
    "                try:\n",
    "                    param_type = ast.unparse(arg.annotation)\n",
    "                except:\n",
    "                    param_type = str(arg.annotation)\n",
    "            \n",
    "            parameters.append({\n",
    "                \"name\": param_name,\n",
    "                \"type\": param_type,\n",
    "                \"description\": \"Parameter description not available.\"\n",
    "            })\n",
    "        \n",
    "        # Extract return type\n",
    "        returns = None\n",
    "        if node.returns:\n",
    "            try:\n",
    "                returns = ast.unparse(node.returns)\n",
    "            except:\n",
    "                returns = str(node.returns)\n",
    "        \n",
    "        # Save the current function context\n",
    "        prev_function = self.current_function\n",
    "        prev_method = self.current_method\n",
    "        \n",
    "        if is_method:\n",
    "            self.current_method = full_name\n",
    "        else:\n",
    "            self.current_function = full_name\n",
    "            self.defined_names.add(func_name)\n",
    "        \n",
    "        # Visit the function body to capture calls and relationships\n",
    "        self.generic_visit(node)\n",
    "        \n",
    "        # Create the function info object\n",
    "        func_info = {\n",
    "            \"name\": func_name,\n",
    "            \"docstring\": docstring or \"No documentation available.\",\n",
    "            \"parameters\": parameters,\n",
    "            \"returns\": returns,\n",
    "            \"relationships\": {\n",
    "                \"calls_functions\": list(self.function_calls.get(full_name, set())),\n",
    "                \"instantiates_classes\": list(self.class_instantiations.get(full_name, set())),\n",
    "                \"accesses_attributes\": list(self.attribute_accesses.get(full_name, set())),\n",
    "                \"called_by\": []  # Will be filled later\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Restore previous function context\n",
    "        self.current_function = prev_function\n",
    "        self.current_method = prev_method\n",
    "        \n",
    "        # Add to functions list if not a method or not a special method\n",
    "        if not skip_adding:\n",
    "            if not is_method:\n",
    "                self.functions.append(func_info)\n",
    "            return func_info\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def visit_FunctionDef(self, node):\n",
    "        \"\"\"Process function definitions.\"\"\"\n",
    "        self.process_function(node)\n",
    "    \n",
    "    def visit_Call(self, node):\n",
    "        \"\"\"Process function calls.\"\"\"\n",
    "        # Determine the current context\n",
    "        current_context = self.current_method if self.current_method else self.current_function\n",
    "        \n",
    "        if current_context:\n",
    "            # Function call\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                func_name = node.func.id\n",
    "                self.function_calls[current_context].add(func_name)\n",
    "            \n",
    "            # Method call (obj.method())\n",
    "            elif isinstance(node.func, ast.Attribute) and isinstance(node.func.value, ast.Name):\n",
    "                obj_name = node.func.value.id\n",
    "                method_name = node.func.attr\n",
    "                \n",
    "                # Could be a module.function() call\n",
    "                if obj_name in self.external_modules:\n",
    "                    full_call = f\"{obj_name}.{method_name}\"\n",
    "                else:\n",
    "                    # Could be a class instantiation (ClassName())\n",
    "                    for cls in self.classes:\n",
    "                        if cls[\"name\"] == obj_name:\n",
    "                            self.class_instantiations[current_context].add(obj_name)\n",
    "                            break\n",
    "                    \n",
    "                    full_call = f\"{obj_name}.{method_name}\"\n",
    "                \n",
    "                self.function_calls[current_context].add(full_call)\n",
    "                self.attribute_accesses[current_context].add(f\"{obj_name}.{method_name}\")\n",
    "        \n",
    "        self.generic_visit(node)\n",
    "    \n",
    "    def visit_Assign(self, node):\n",
    "        \"\"\"Process assignments.\"\"\"\n",
    "        # Only process global assignments\n",
    "        if not self.current_function and not self.current_method:\n",
    "            for target in node.targets:\n",
    "                if isinstance(target, ast.Name):\n",
    "                    var_name = target.id\n",
    "                    try:\n",
    "                        var_value = ast.unparse(node.value)\n",
    "                    except:\n",
    "                        var_value = str(node.value)\n",
    "                    \n",
    "                    # Check if it's a class instantiation\n",
    "                    if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):\n",
    "                        class_name = node.value.func.id\n",
    "                        # Check if it's one of our known classes\n",
    "                        for cls in self.classes:\n",
    "                            if cls[\"name\"] == class_name:\n",
    "                                self.class_instantiations[\"global\"].add(class_name)\n",
    "                                break\n",
    "                    \n",
    "                    self.global_vars.append({\n",
    "                        \"name\": var_name,\n",
    "                        \"value\": var_value\n",
    "                    })\n",
    "                    self.defined_names.add(var_name)\n",
    "        \n",
    "        self.generic_visit(node)\n",
    "    \n",
    "    def post_process(self):\n",
    "        \"\"\"Build reverse relationships after processing.\"\"\"\n",
    "        # For each function call, update the called_by relationship\n",
    "        for caller, callees in self.function_calls.items():\n",
    "            for callee in callees:\n",
    "                # Find the actual function record\n",
    "                for func in self.functions:\n",
    "                    if func[\"name\"] == callee:\n",
    "                        if caller not in func[\"relationships\"][\"called_by\"]:\n",
    "                            func[\"relationships\"][\"called_by\"].append(caller)\n",
    "        \n",
    "        # For each class instantiation, update the instantiated_by relationship\n",
    "        for instantiator, classes in self.class_instantiations.items():\n",
    "            for class_name in classes:\n",
    "                # Find the actual class record\n",
    "                for cls in self.classes:\n",
    "                    if cls[\"name\"] == class_name:\n",
    "                        if instantiator not in cls[\"relationships\"][\"instantiated_by\"]:\n",
    "                            cls[\"relationships\"][\"instantiated_by\"].append(instantiator)\n",
    "        \n",
    "        # For each class, update the used_by_functions relationship\n",
    "        for cls in self.classes:\n",
    "            class_name = cls[\"name\"]\n",
    "            for func in self.functions:\n",
    "                # If function instantiates this class\n",
    "                if class_name in func[\"relationships\"][\"instantiates_classes\"]:\n",
    "                    if func[\"name\"] not in cls[\"relationships\"][\"used_by_functions\"]:\n",
    "                        cls[\"relationships\"][\"used_by_functions\"].append(func[\"name\"])\n",
    "                \n",
    "                # If function accesses any attributes related to this class\n",
    "                for attr in func[\"relationships\"][\"accesses_attributes\"]:\n",
    "                    if attr.startswith(f\"{class_name}.\"):\n",
    "                        if func[\"name\"] not in cls[\"relationships\"][\"used_by_functions\"]:\n",
    "                            cls[\"relationships\"][\"used_by_functions\"].append(func[\"name\"])\n",
    "\n",
    "class EnhancedAPIDocGenerator:\n",
    "    \"\"\"Class to generate enhanced API documentation with relationship information.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def generate_module_api_doc(self, module_name, code):\n",
    "        \"\"\"Generate enhanced API documentation for a module.\"\"\"\n",
    "        try:\n",
    "            # Parse the AST\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Visit the AST to extract entities and relationships\n",
    "            visitor = RelationshipVisitor()\n",
    "            visitor.visit(tree)\n",
    "            visitor.post_process()\n",
    "            \n",
    "            # Extract module docstring\n",
    "            module_docstring = None\n",
    "            if (tree.body and isinstance(tree.body[0], ast.Expr) and \n",
    "                isinstance(tree.body[0].value, ast.Str)):\n",
    "                module_docstring = tree.body[0].value.s.strip()\n",
    "            \n",
    "            # Create module doc\n",
    "            module_doc = {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": module_docstring or \"No module documentation available.\",\n",
    "                \"imports\": visitor.imports,\n",
    "                \"global_vars\": visitor.global_vars,\n",
    "                \"functions\": visitor.functions,\n",
    "                \"classes\": visitor.classes,\n",
    "                \"relationships\": {\n",
    "                    \"dependencies\": self._analyze_module_dependencies(visitor),\n",
    "                    \"entry_points\": self._identify_entry_points(visitor)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return module_doc\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            logger.error(f\"Syntax error in module {module_name}: {e}\")\n",
    "            return self._fallback_api_doc_generation(module_name, code)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing module {module_name}: {e}\")\n",
    "            return self._fallback_api_doc_generation(module_name, code)\n",
    "    \n",
    "    def _analyze_module_dependencies(self, visitor):\n",
    "        \"\"\"Analyze module level dependencies.\"\"\"\n",
    "        dependencies = {\n",
    "            \"imports\": [imp.get(\"module\") for imp in visitor.imports if \"module\" in imp],\n",
    "            \"from_imports\": [f\"{imp.get('module')}.{imp.get('name')}\" for imp in visitor.imports if \"name\" in imp],\n",
    "        }\n",
    "        return dependencies\n",
    "    \n",
    "    def _identify_entry_points(self, visitor):\n",
    "        \"\"\"Identify potential entry points in the module.\"\"\"\n",
    "        # Entry points are functions that are not called by other functions\n",
    "        entry_points = []\n",
    "        \n",
    "        for func in visitor.functions:\n",
    "            if not func[\"relationships\"][\"called_by\"]:\n",
    "                # This function is not called by others\n",
    "                entry_points.append(func[\"name\"])\n",
    "        \n",
    "        # Also look for if __name__ == \"__main__\" block\n",
    "        # This is a simplification - in a real implementation, we'd need to parse the AST for this\n",
    "        \n",
    "        return entry_points\n",
    "    \n",
    "    def _fallback_api_doc_generation(self, module_name, code):\n",
    "        \"\"\"Use the LLM as a fallback for API doc generation when parsing fails.\"\"\"\n",
    "        logger.info(f\"Using LLM to extract enhanced API documentation for {module_name}\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate a detailed API documentation for the following Python code module.\n",
    "Extract all functions, classes, methods, and their parameters, return types, and docstrings.\n",
    "MOST IMPORTANTLY, also extract the relationships between functions and classes:\n",
    "- What functions call other functions\n",
    "- What functions instantiate classes\n",
    "- What classes inherit from other classes\n",
    "- What functions are entry points (not called by others)\n",
    "\n",
    "Format the response as a JSON object with the structure shown in the example.\n",
    "\n",
    "Example structure:\n",
    "```json\n",
    "{{\n",
    "  \"name\": \"module_name\",\n",
    "  \"docstring\": \"Module docstring\",\n",
    "  \"imports\": [\n",
    "    {{\"module\": \"os\", \"alias\": null}},\n",
    "    {{\"module\": \"pandas\", \"alias\": \"pd\"}}\n",
    "  ],\n",
    "  \"global_vars\": [\n",
    "    {{\"name\": \"logger\", \"value\": \"logging.getLogger(__name__)\"}}\n",
    "  ],\n",
    "  \"functions\": [\n",
    "    {{\n",
    "      \"name\": \"function_name\",\n",
    "      \"docstring\": \"Function docstring\",\n",
    "      \"parameters\": [\n",
    "        {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "      ],\n",
    "      \"returns\": \"str\",\n",
    "      \"relationships\": {{\n",
    "        \"calls_functions\": [\"other_function\", \"third_function\"],\n",
    "        \"instantiates_classes\": [\"SomeClass\"],\n",
    "        \"accesses_attributes\": [\"object.attribute\"],\n",
    "        \"called_by\": [\"main\"]\n",
    "      }}\n",
    "    }}\n",
    "  ],\n",
    "  \"classes\": [\n",
    "    {{\n",
    "      \"name\": \"ClassName\",\n",
    "      \"docstring\": \"Class docstring\",\n",
    "      \"bases\": [\"BaseClass\"],\n",
    "      \"methods\": [\n",
    "        {{\n",
    "          \"name\": \"method_name\",\n",
    "          \"docstring\": \"Method docstring\",\n",
    "          \"parameters\": [\n",
    "            {{\"name\": \"self\", \"type\": null, \"description\": \"Instance reference\"}},\n",
    "            {{\"name\": \"param1\", \"type\": \"str\", \"description\": \"Description of param1\"}}\n",
    "          ],\n",
    "          \"returns\": \"bool\",\n",
    "          \"relationships\": {{\n",
    "            \"calls_functions\": [\"some_function\"],\n",
    "            \"instantiates_classes\": [],\n",
    "            \"accesses_attributes\": [\"self.attribute\"],\n",
    "            \"called_by\": []\n",
    "          }}\n",
    "        }}\n",
    "      ],\n",
    "      \"relationships\": {{\n",
    "        \"inherits_from\": [\"BaseClass\"],\n",
    "        \"used_by_functions\": [\"function_name\"],\n",
    "        \"instantiated_by\": [\"function_name\"]\n",
    "      }}\n",
    "    }}\n",
    "  ],\n",
    "  \"relationships\": {{\n",
    "    \"dependencies\": {{\n",
    "      \"imports\": [\"os\", \"pandas\"],\n",
    "      \"from_imports\": [\"logging.getLogger\"]\n",
    "    }},\n",
    "    \"entry_points\": [\"main\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "Here's the code to document:\n",
    "\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "Focus especially on capturing the relationships between functions and classes to help understand how the code works together.\n",
    "\"\"\"\n",
    "\n",
    "        system_message = SystemMessage(content=\"You are a Python expert who specializes in extracting API documentation and code relationships from code.\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke([system_message, human_message])\n",
    "            content = response.content\n",
    "            \n",
    "            # Try to extract JSON from the response\n",
    "            json_match = re.search(r'```json\\s*(.*?)\\s*```', content, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                # If no JSON code block, try to find any JSON object\n",
    "                json_match = re.search(r'({[\\s\\S]*})', content)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(1)\n",
    "                else:\n",
    "                    json_str = content\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting API doc from LLM for {module_name}: {e}\")\n",
    "            # Return a minimal structure\n",
    "            return {\n",
    "                \"name\": module_name,\n",
    "                \"docstring\": \"Documentation extraction failed.\",\n",
    "                \"imports\": [],\n",
    "                \"global_vars\": [],\n",
    "                \"functions\": [],\n",
    "                \"classes\": [],\n",
    "                \"relationships\": {\n",
    "                    \"dependencies\": {\n",
    "                        \"imports\": [],\n",
    "                        \"from_imports\": []\n",
    "                    },\n",
    "                    \"entry_points\": []\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def generate_all_module_docs(self, code_contents):\n",
    "        \"\"\"Generate API documentation for all modules.\"\"\"\n",
    "        module_docs = {}\n",
    "        \n",
    "        for module_name, code in code_contents.items():\n",
    "            try:\n",
    "                module_doc = self.generate_module_api_doc(module_name, code)\n",
    "                module_docs[module_name] = module_doc\n",
    "                logger.info(f\"Generated enhanced API documentation for {module_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating API doc for {module_name}: {e}\")\n",
    "        \n",
    "        return module_docs\n",
    "    \n",
    "    def try_generate_dependency_graph(self, module_docs):\n",
    "        \"\"\"Try to generate a dependency graph visualization for all modules.\"\"\"\n",
    "        try:\n",
    "            # Check if matplotlib and networkx are available\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                import networkx as nx\n",
    "            except ImportError:\n",
    "                logger.warning(\"matplotlib or networkx not available, skipping graph generation\")\n",
    "                return None\n",
    "            \n",
    "            # Create a directed graph\n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Add nodes for each module\n",
    "            for module_name in module_docs.keys():\n",
    "                G.add_node(module_name, type='module')\n",
    "            \n",
    "            # Add edges for dependencies between modules\n",
    "            for module_name, doc in module_docs.items():\n",
    "                # For each function in this module\n",
    "                for func in doc.get(\"functions\", []):\n",
    "                    # For each function call\n",
    "                    for called_func in func.get(\"relationships\", {}).get(\"calls_functions\", []):\n",
    "                        # If the function contains a dot, it might be a cross-module call\n",
    "                        if \".\" in called_func:\n",
    "                            parts = called_func.split(\".\")\n",
    "                            if len(parts) == 2:\n",
    "                                potential_module = parts[0]\n",
    "                                # Check if this is one of our modules\n",
    "                                if potential_module in module_docs:\n",
    "                                    G.add_edge(module_name, potential_module, \n",
    "                                            label=f\"{func['name']} -> {called_func}\")\n",
    "                                    \n",
    "                # Add edges based on imports if we can determine they're our modules\n",
    "                for imp in doc.get(\"imports\", []):\n",
    "                    module = imp.get(\"module\")\n",
    "                    if module in module_docs:\n",
    "                        G.add_edge(module_name, module, label=\"imports\")\n",
    "            \n",
    "            # Check if we have any edges\n",
    "            if not G.edges():\n",
    "                # Add edges based on function similarities\n",
    "                self._add_similarity_edges(G, module_docs)\n",
    "            \n",
    "            # Create the visualization\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            pos = nx.spring_layout(G)\n",
    "            nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "                    font_weight='bold', node_size=2000, arrows=True)\n",
    "            \n",
    "            # Add edge labels\n",
    "            edge_labels = {(u, v): d.get('label', '') for u, v, d in G.edges(data=True)}\n",
    "            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "            \n",
    "            return G\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating dependency graph: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _add_similarity_edges(self, G, module_docs):\n",
    "        \"\"\"Add edges based on function and class name similarities.\"\"\"\n",
    "        # Create a dictionary of all function names to their modules\n",
    "        function_to_module = {}\n",
    "        for module_name, doc in module_docs.items():\n",
    "            for func in doc.get(\"functions\", []):\n",
    "                function_to_module[func[\"name\"]] = module_name\n",
    "        \n",
    "        # Look for similar function names across modules\n",
    "        for module_name, doc in module_docs.items():\n",
    "            for func in doc.get(\"functions\", []):\n",
    "                for called_func in func.get(\"relationships\", {}).get(\"calls_functions\", []):\n",
    "                    # If the function appears in another module\n",
    "                    if called_func in function_to_module and function_to_module[called_func] != module_name:\n",
    "                        target_module = function_to_module[called_func]\n",
    "                        G.add_edge(module_name, target_module, \n",
    "                                label=f\"{func['name']} -> {called_func}\")\n",
    "    \n",
    "    def format_api_docs_for_llm(self, module_docs):\n",
    "        \"\"\"Format API documentation for use in LLM prompt, including relationship information.\"\"\"\n",
    "        formatted_docs = []\n",
    "        \n",
    "        for module_name, doc in module_docs.items():\n",
    "            module_text = [f\"MODULE: {module_name}_code.py\"]\n",
    "            \n",
    "            # Add module docstring\n",
    "            module_text.append(f\"Description: {doc['docstring']}\")\n",
    "            module_text.append(\"\")\n",
    "            \n",
    "            # Add imports\n",
    "            if doc[\"imports\"]:\n",
    "                module_text.append(\"Imports:\")\n",
    "                for imp in doc[\"imports\"]:\n",
    "                    if \"name\" in imp:\n",
    "                        from_txt = f\"from {imp['module']} \" if imp['module'] else \"from \"\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  {from_txt}import {imp['name']}{as_txt}\")\n",
    "                    else:\n",
    "                        as_txt = f\" as {imp['alias']}\" if imp['alias'] else \"\"\n",
    "                        module_text.append(f\"  import {imp['module']}{as_txt}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add global variables\n",
    "            if doc[\"global_vars\"]:\n",
    "                module_text.append(\"Global Variables:\")\n",
    "                for var in doc[\"global_vars\"]:\n",
    "                    module_text.append(f\"  {var['name']} = {var['value']}\")\n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            # Add classes\n",
    "            if doc[\"classes\"]:\n",
    "                module_text.append(\"Classes:\")\n",
    "                for cls in doc[\"classes\"]:\n",
    "                    bases = f\"({', '.join(cls['bases'])})\" if cls['bases'] else \"\"\n",
    "                    module_text.append(f\"  class {cls['name']}{bases}:\")\n",
    "                    module_text.append(f\"    \\\"{cls['docstring']}\\\"\")\n",
    "                    \n",
    "                    # Add class relationships\n",
    "                    if cls.get(\"relationships\"):\n",
    "                        module_text.append(\"    Relationships:\")\n",
    "                        inherits = cls[\"relationships\"].get(\"inherits_from\", [])\n",
    "                        if inherits:\n",
    "                            module_text.append(f\"      Inherits from: {', '.join(inherits)}\")\n",
    "                        \n",
    "                        used_by = cls[\"relationships\"].get(\"used_by_functions\", [])\n",
    "                        if used_by:\n",
    "                            module_text.append(f\"      Used by functions: {', '.join(used_by)}\")\n",
    "                        \n",
    "                        inst_by = cls[\"relationships\"].get(\"instantiated_by\", [])\n",
    "                        if inst_by:\n",
    "                            module_text.append(f\"      Instantiated by: {', '.join(inst_by)}\")\n",
    "                        \n",
    "                        module_text.append(\"\")\n",
    "                    \n",
    "                    if cls[\"methods\"]:\n",
    "                        module_text.append(\"    Methods:\")\n",
    "                        for method in cls[\"methods\"]:\n",
    "                            params = []\n",
    "                            for p in method[\"parameters\"]:\n",
    "                                param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                                params.append(f\"{p['name']}{param_type}\")\n",
    "                            \n",
    "                            returns = f\" -> {method['returns']}\" if method['returns'] else \"\"\n",
    "                            module_text.append(f\"      def {method['name']}({', '.join(params)}){returns}:\")\n",
    "                            module_text.append(f\"        \\\"{method['docstring']}\\\"\")\n",
    "                            \n",
    "                            # Add method relationships\n",
    "                            if method.get(\"relationships\"):\n",
    "                                module_text.append(\"        Relationships:\")\n",
    "                                calls = method[\"relationships\"].get(\"calls_functions\", [])\n",
    "                                if calls:\n",
    "                                    module_text.append(f\"          Calls functions: {', '.join(calls)}\")\n",
    "                                \n",
    "                                instantiates = method[\"relationships\"].get(\"instantiates_classes\", [])\n",
    "                                if instantiates:\n",
    "                                    module_text.append(f\"          Instantiates classes: {', '.join(instantiates)}\")\n",
    "                                \n",
    "                                accesses = method[\"relationships\"].get(\"accesses_attributes\", [])\n",
    "                                if accesses:\n",
    "                                    module_text.append(f\"          Accesses attributes: {', '.join(accesses)}\")\n",
    "                                \n",
    "                                called_by = method[\"relationships\"].get(\"called_by\", [])\n",
    "                                if called_by:\n",
    "                                    module_text.append(f\"          Called by: {', '.join(called_by)}\")\n",
    "                                \n",
    "                                module_text.append(\"\")\n",
    "                            \n",
    "                            # Add parameter descriptions\n",
    "                            if any(p[\"description\"] != \"Parameter description not available.\" for p in method[\"parameters\"]):\n",
    "                                module_text.append(\"        Parameters:\")\n",
    "                                for p in method[\"parameters\"]:\n",
    "                                    if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                        module_text.append(f\"          {p['name']}: {p['description']}\")\n",
    "                            \n",
    "                            module_text.append(\"\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            # Add functions\n",
    "            if doc[\"functions\"]:\n",
    "                module_text.append(\"Functions:\")\n",
    "                for func in doc[\"functions\"]:\n",
    "                    params = []\n",
    "                    for p in func[\"parameters\"]:\n",
    "                        param_type = f\": {p['type']}\" if p['type'] else \"\"\n",
    "                        params.append(f\"{p['name']}{param_type}\")\n",
    "                    \n",
    "                    returns = f\" -> {func['returns']}\" if func['returns'] else \"\"\n",
    "                    module_text.append(f\"  def {func['name']}({', '.join(params)}){returns}:\")\n",
    "                    module_text.append(f\"    \\\"{func['docstring']}\\\"\")\n",
    "                    \n",
    "                    # Add function relationships\n",
    "                    if func.get(\"relationships\"):\n",
    "                        module_text.append(\"    Relationships:\")\n",
    "                        calls = func[\"relationships\"].get(\"calls_functions\", [])\n",
    "                        if calls:\n",
    "                            module_text.append(f\"      Calls functions: {', '.join(calls)}\")\n",
    "                        \n",
    "                        instantiates = func[\"relationships\"].get(\"instantiates_classes\", [])\n",
    "                        if instantiates:\n",
    "                            module_text.append(f\"      Instantiates classes: {', '.join(instantiates)}\")\n",
    "                        \n",
    "                        accesses = func[\"relationships\"].get(\"accesses_attributes\", [])\n",
    "                        if accesses:\n",
    "                            module_text.append(f\"      Accesses attributes: {', '.join(accesses)}\")\n",
    "                        \n",
    "                        called_by = func[\"relationships\"].get(\"called_by\", [])\n",
    "                        if called_by:\n",
    "                            module_text.append(f\"      Called by: {', '.join(called_by)}\")\n",
    "                        \n",
    "                        module_text.append(\"\")\n",
    "                    \n",
    "                    # Add parameter descriptions\n",
    "                    if any(p[\"description\"] != \"Parameter description not available.\" for p in func[\"parameters\"]):\n",
    "                        module_text.append(\"    Parameters:\")\n",
    "                        for p in func[\"parameters\"]:\n",
    "                            if p[\"description\"] != \"Parameter description not available.\":\n",
    "                                module_text.append(f\"      {p['name']}: {p['description']}\")\n",
    "                    \n",
    "                    module_text.append(\"\")\n",
    "            \n",
    "            # Add module-level relationships\n",
    "            if doc.get(\"relationships\"):\n",
    "                module_text.append(\"Module Relationships:\")\n",
    "                \n",
    "                # Dependencies\n",
    "                deps = doc[\"relationships\"].get(\"dependencies\", {})\n",
    "                imports = deps.get(\"imports\", [])\n",
    "                if imports:\n",
    "                    module_text.append(f\"  Imports modules: {', '.join(imports)}\")\n",
    "                \n",
    "                from_imports = deps.get(\"from_imports\", [])\n",
    "                if from_imports:\n",
    "                    module_text.append(f\"  Imports from: {', '.join(from_imports)}\")\n",
    "                \n",
    "                # Entry points\n",
    "                entry_points = doc[\"relationships\"].get(\"entry_points\", [])\n",
    "                if entry_points:\n",
    "                    module_text.append(f\"  Entry points: {', '.join(entry_points)}\")\n",
    "                \n",
    "                module_text.append(\"\")\n",
    "            \n",
    "            formatted_docs.append(\"\\n\".join(module_text))\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs) + \"\\n\"\n",
    "\n",
    "class EnhancedCodeIntegrator:\n",
    "    \"\"\"Class to integrate code modules based on enhanced API documentation with relationships.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, doc_generator):\n",
    "        self.model = model\n",
    "        self.doc_generator = doc_generator\n",
    "    \n",
    "    def create_integration_prompt(self, api_docs):\n",
    "        \"\"\"Create a prompt for the LLM to integrate the code with relationship awareness.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "I have multiple Python modules that need to be integrated into a cohesive solution.\n",
    "Below is the ENHANCED API documentation for each module, which includes detailed relationship information\n",
    "showing which functions call other functions, which classes are instantiated, and other dependencies.\n",
    "Each module is stored in a separate file with the naming pattern of \"US_XXX_code.py\" where XXX is the user story ID.\n",
    "\n",
    "{api_docs}\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Create a single integrated Python file that coordinates functionality from all these modules\n",
    "2. Design the integrated solution to import modules correctly using their filenames (e.g., \"import US_142_code\" NOT \"import US_142\")\n",
    "3. Create proper references to functions and classes from each module with correct module prefixes\n",
    "4. Make sure to import all necessary standard and third-party libraries needed by the solution\n",
    "5. Ensure proper sequencing based on the function call relationships documented above\n",
    "6. Include a main execution block that coordinates the overall flow\n",
    "7. Write clear comments to explain how the integration works, especially noting important function relationships\n",
    "8. Add detailed documentation explaining which functions call which other functions and their dependencies\n",
    "\n",
    "IMPORTANT: Each module should be imported using its full filename (e.g., \"import US_142_code\" not \"import US_142\").\n",
    "When referring to functions, classes, or variables from these modules, use the proper module prefix\n",
    "(e.g., \"US_142_code.process_file()\" not \"US_142.process_file()\").\n",
    "\n",
    "Your integrated solution should include:\n",
    "1. A detailed module docstring explaining the overall architecture and how the modules interact\n",
    "2. Comments for each section explaining which components depend on each other\n",
    "3. A function relationship map in comments to help developers understand the code flow\n",
    "4. A main execution function that coordinates the execution flow based on the identified relationships\n",
    "\n",
    "Format your response as a single Python file with all necessary imports, functions, \n",
    "and a main execution block. Add helpful comments to explain your integration strategy.\n",
    "\n",
    "Return only the final integrated Python code without explanation or other text.\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def validate_integrated_code(self, code, module_names):\n",
    "        \"\"\"Validate that the integrated code properly imports all modules with correct names.\"\"\"\n",
    "        # Check if modules are imported with _code suffix\n",
    "        proper_imports = True\n",
    "        module_import_checks = []\n",
    "        \n",
    "        for module_name in module_names:\n",
    "            module_import_name = f\"{module_name}_code\"\n",
    "            if f\"import {module_name}\" in code and f\"import {module_import_name}\" not in code:\n",
    "                proper_imports = False\n",
    "                module_import_checks.append((module_name, False))\n",
    "            else:\n",
    "                module_import_checks.append((module_name, True))\n",
    "        \n",
    "        # Check for any functions or classes referenced without proper module prefix\n",
    "        improper_references = []\n",
    "        \n",
    "        for module_name in module_names:\n",
    "            # Look for patterns like \"ModuleName.function\" instead of \"ModuleName_code.function\"\n",
    "            pattern = fr\"{module_name}\\.[a-zA-Z0-9_]+\"\n",
    "            matches = re.findall(pattern, code)\n",
    "            if matches:\n",
    "                improper_references.extend(matches)\n",
    "        \n",
    "        return {\n",
    "            \"proper_imports\": proper_imports,\n",
    "            \"module_import_checks\": module_import_checks,\n",
    "            \"improper_references\": improper_references\n",
    "        }\n",
    "    \n",
    "    def fix_integrated_code(self, code, validation_result):\n",
    "        \"\"\"Fix issues with the integrated code based on validation results.\"\"\"\n",
    "        fixed_code = code\n",
    "        \n",
    "        # Fix improper imports\n",
    "        for module_name, is_proper in validation_result[\"module_import_checks\"]:\n",
    "            if not is_proper:\n",
    "                # Replace \"import ModuleName\" with \"import ModuleName_code\"\n",
    "                fixed_code = re.sub(\n",
    "                    fr\"import\\s+{module_name}(?!_code)\",\n",
    "                    f\"import {module_name}_code\",\n",
    "                    fixed_code\n",
    "                )\n",
    "                \n",
    "                # Replace \"from ModuleName import\" with \"from ModuleName_code import\"\n",
    "                fixed_code = re.sub(\n",
    "                    fr\"from\\s+{module_name}(?!_code)\\s+import\",\n",
    "                    f\"from {module_name}_code import\",\n",
    "                    fixed_code\n",
    "                )\n",
    "        \n",
    "        # Fix improper references\n",
    "        for ref in validation_result[\"improper_references\"]:\n",
    "            module_name = ref.split('.')[0]\n",
    "            fixed_code = fixed_code.replace(ref, ref.replace(f\"{module_name}.\", f\"{module_name}_code.\"))\n",
    "        \n",
    "        return fixed_code\n",
    "    \n",
    "    def generate_integrated_code(self, module_docs):\n",
    "        \"\"\"Generate integrated code based on enhanced API documentation with relationships.\"\"\"\n",
    "        # Format API docs for the LLM\n",
    "        api_docs_text = self.doc_generator.format_api_docs_for_llm(module_docs)\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = self.create_integration_prompt(api_docs_text)\n",
    "        \n",
    "        # Send to LLM\n",
    "        system_message = SystemMessage(content=\"\"\"You are a Python expert who specializes in integrating multiple code modules \n",
    "into cohesive solutions. You excel at understanding module dependencies and creating orchestration code.\"\"\")\n",
    "        human_message = HumanMessage(content=prompt)\n",
    "        \n",
    "        logger.info(\"Sending enhanced API documentation to LLM for integration\")\n",
    "        response = self.model.invoke([system_message, human_message])\n",
    "        \n",
    "        # Extract code from response\n",
    "        content = response.content\n",
    "        \n",
    "        # Check if the response is wrapped in code blocks\n",
    "        if \"```python\" in content and \"```\" in content.split(\"```python\", 1)[1]:\n",
    "            # Extract code between the markers\n",
    "            code = content.split(\"```python\", 1)[1].split(\"```\", 1)[0].strip()\n",
    "        elif \"```\" in content and content.count(\"```\") >= 2:\n",
    "            # Extract code between the markers\n",
    "            parts = content.split(\"```\", 2)\n",
    "            code = parts[1]\n",
    "            if code.startswith(\"python\"):\n",
    "                code = code[6:]\n",
    "            code = code.strip()\n",
    "        else:\n",
    "            # If not wrapped in code blocks, return as is\n",
    "            code = content\n",
    "        \n",
    "        # Validate and fix the code\n",
    "        validation_result = self.validate_integrated_code(code, module_docs.keys())\n",
    "        \n",
    "        if not validation_result[\"proper_imports\"] or validation_result[\"improper_references\"]:\n",
    "            logger.info(\"Fixing issues in the integrated code\")\n",
    "            code = self.fix_integrated_code(code, validation_result)\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    def generate_init_file(self, output_dir, module_docs):\n",
    "        \"\"\"Generate an __init__.py file to make importing modules easier.\"\"\"\n",
    "        init_content = ['\"\"\"Package initialization file with module relationships documented.\"\"\"\\n']\n",
    "        \n",
    "        # Add imports for all modules\n",
    "        for module_name in module_docs.keys():\n",
    "            # Import the module\n",
    "            init_content.append(f\"import {module_name}_code\")\n",
    "            \n",
    "            # Create shorter aliases for convenience\n",
    "            init_content.append(f\"{module_name} = {module_name}_code\")\n",
    "        \n",
    "        # Add module relationship documentation\n",
    "        init_content.append(\"\\n# Module relationships:\")\n",
    "        for module_name, doc in module_docs.items():\n",
    "            # Document entry points\n",
    "            entry_points = doc.get(\"relationships\", {}).get(\"entry_points\", [])\n",
    "            if entry_points:\n",
    "                init_content.append(f\"# {module_name}_code entry points: {', '.join(entry_points)}\")\n",
    "            \n",
    "            # Document function calls between modules\n",
    "            calls_found = False\n",
    "            for func in doc.get(\"functions\", []):\n",
    "                for called_func in func.get(\"relationships\", {}).get(\"calls_functions\", []):\n",
    "                    if \".\" in called_func:\n",
    "                        parts = called_func.split(\".\")\n",
    "                        if len(parts) == 2 and parts[0] in module_docs:\n",
    "                            if not calls_found:\n",
    "                                init_content.append(f\"# {module_name}_code function dependencies:\")\n",
    "                                calls_found = True\n",
    "                            init_content.append(f\"#   {func['name']} -> {called_func}\")\n",
    "            \n",
    "            if not calls_found:\n",
    "                init_content.append(f\"# {module_name}_code: No external function calls identified\")\n",
    "        \n",
    "        # Write the file\n",
    "        init_path = os.path.join(output_dir, \"__init__.py\")\n",
    "        with open(init_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(init_content))\n",
    "        \n",
    "        logger.info(f\"Created enhanced __init__.py file at {init_path}\")\n",
    "\n",
    "def save_modules_with_proper_names(output_dir, code_contents):\n",
    "    \"\"\"Save individual modules with proper names based on user story IDs.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for module_name, code in code_contents.items():\n",
    "        file_name = f\"{module_name}_code.py\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved module {module_name} to {file_path}\")\n",
    "\n",
    "def create_relationship_documentation(output_dir, module_docs):\n",
    "    \"\"\"Create a RELATIONSHIPS.md file documenting the relationships between all components.\"\"\"\n",
    "    content = [\n",
    "        \"# Module Relationship Documentation\",\n",
    "        \"\",\n",
    "        \"This document provides detailed information about the relationships between modules, functions, and classes.\",\n",
    "        \"\",\n",
    "        \"## Overview\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    # Create a list of all modules\n",
    "    content.append(\"### Modules\")\n",
    "    for module_name in module_docs.keys():\n",
    "        content.append(f\"- {module_name}_code.py\")\n",
    "    content.append(\"\")\n",
    "    \n",
    "    # Document module-level relationships\n",
    "    content.append(\"## Module Dependencies\")\n",
    "    content.append(\"\")\n",
    "    \n",
    "    for module_name, doc in module_docs.items():\n",
    "        content.append(f\"### {module_name}_code.py\")\n",
    "        content.append(f\"*{doc['docstring']}*\")\n",
    "        content.append(\"\")\n",
    "        \n",
    "        # Dependencies\n",
    "        deps = doc.get(\"relationships\", {}).get(\"dependencies\", {})\n",
    "        imports = deps.get(\"imports\", [])\n",
    "        if imports:\n",
    "            content.append(\"**Imports modules:**\")\n",
    "            for imp in imports:\n",
    "                content.append(f\"- {imp}\")\n",
    "            content.append(\"\")\n",
    "        \n",
    "        # Entry points\n",
    "        entry_points = doc.get(\"relationships\", {}).get(\"entry_points\", [])\n",
    "        if entry_points:\n",
    "            content.append(\"**Entry points:**\")\n",
    "            for ep in entry_points:\n",
    "                content.append(f\"- {ep}\")\n",
    "            content.append(\"\")\n",
    "        \n",
    "        # Functions\n",
    "        if doc.get(\"functions\"):\n",
    "            content.append(\"**Functions:**\")\n",
    "            for func in doc[\"functions\"]:\n",
    "                # Add function with its relationships\n",
    "                # Safely extract the docstring summary\n",
    "                docstring_summary = get_docstring_summary(func.get('docstring'))\n",
    "                content.append(f\"- `{func['name']}`: {docstring_summary}\")\n",
    "                \n",
    "                # Function calls\n",
    "                calls = func.get(\"relationships\", {}).get(\"calls_functions\", [])\n",
    "                if calls:\n",
    "                    content.append(f\"  - Calls: {', '.join([f'`{c}`' for c in calls])}\")\n",
    "                \n",
    "                # Function instantiations\n",
    "                instantiates = func.get(\"relationships\", {}).get(\"instantiates_classes\", [])\n",
    "                if instantiates:\n",
    "                    content.append(f\"  - Instantiates: {', '.join([f'`{c}`' for c in instantiates])}\")\n",
    "                \n",
    "                # Called by\n",
    "                called_by = func.get(\"relationships\", {}).get(\"called_by\", [])\n",
    "                if called_by:\n",
    "                    content.append(f\"  - Called by: {', '.join([f'`{c}`' for c in called_by])}\")\n",
    "            \n",
    "            content.append(\"\")\n",
    "        \n",
    "        # Classes\n",
    "        if doc.get(\"classes\"):\n",
    "            content.append(\"**Classes:**\")\n",
    "            for cls in doc[\"classes\"]:\n",
    "                # Add class with its relationships\n",
    "                docstring_summary = get_docstring_summary(cls.get('docstring'))\n",
    "                content.append(f\"- `{cls['name']}`: {docstring_summary}\")\n",
    "                \n",
    "                # Inheritance\n",
    "                inherits = cls.get(\"relationships\", {}).get(\"inherits_from\", [])\n",
    "                if inherits:\n",
    "                    content.append(f\"  - Inherits from: {', '.join([f'`{c}`' for c in inherits])}\")\n",
    "                \n",
    "                # Used by\n",
    "                used_by = cls.get(\"relationships\", {}).get(\"used_by_functions\", [])\n",
    "                if used_by:\n",
    "                    content.append(f\"  - Used by: {', '.join([f'`{c}`' for c in used_by])}\")\n",
    "                \n",
    "                # Instantiated by\n",
    "                inst_by = cls.get(\"relationships\", {}).get(\"instantiated_by\", [])\n",
    "                if inst_by:\n",
    "                    content.append(f\"  - Instantiated by: {', '.join([f'`{c}`' for c in inst_by])}\")\n",
    "            \n",
    "            content.append(\"\")\n",
    "    \n",
    "    # Create function call graph section\n",
    "    content.append(\"## Function Call Graph\")\n",
    "    content.append(\"\")\n",
    "    content.append(\"This section shows which functions call other functions across all modules.\")\n",
    "    content.append(\"\")\n",
    "    \n",
    "    # Build the function call graph\n",
    "    call_graph = defaultdict(list)\n",
    "    \n",
    "    for module_name, doc in module_docs.items():\n",
    "        for func in doc.get(\"functions\", []):\n",
    "            func_full_name = f\"{module_name}_code.{func['name']}\"\n",
    "            for called_func in func.get(\"relationships\", {}).get(\"calls_functions\", []):\n",
    "                if \".\" in called_func:\n",
    "                    call_graph[func_full_name].append(called_func)\n",
    "                else:\n",
    "                    # It's in the same module\n",
    "                    call_graph[func_full_name].append(f\"{module_name}_code.{called_func}\")\n",
    "    \n",
    "    # Print call graph\n",
    "    for caller, callees in sorted(call_graph.items()):\n",
    "        if callees:\n",
    "            content.append(f\"- `{caller}` calls:\")\n",
    "            for callee in sorted(callees):\n",
    "                content.append(f\"  - `{callee}`\")\n",
    "            content.append(\"\")\n",
    "    \n",
    "    # Write to file\n",
    "    rel_path = os.path.join(output_dir, \"RELATIONSHIPS.md\")\n",
    "    with open(rel_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(content))\n",
    "    \n",
    "    logger.info(f\"Created detailed relationship documentation at {rel_path}\")\n",
    "\n",
    "def create_setup_py(output_dir, module_name=\"integrated_solution\"):\n",
    "    \"\"\"Create a setup.py file to make the package installable.\"\"\"\n",
    "    setup_content = f'''\"\"\"\n",
    "Setup script for {module_name} package.\n",
    "This package combines multiple modules with their relationships preserved.\n",
    "\"\"\"\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"{module_name}\",\n",
    "    version=\"0.1.0\",\n",
    "    packages=find_packages(),\n",
    "    author=\"AI Code Generator\",\n",
    "    author_email=\"ai@example.com\",\n",
    "    description=\"Integrated solution generated from multiple modules with relationship awareness\",\n",
    "    classifiers=[\n",
    "        \"Programming Language :: Python :: 3\",\n",
    "        \"License :: OSI Approved :: MIT License\",\n",
    "        \"Operating System :: OS Independent\",\n",
    "    ],\n",
    "    python_requires=\">=3.6\",\n",
    ")\n",
    "'''\n",
    "    \n",
    "    setup_path = os.path.join(output_dir, \"setup.py\")\n",
    "    with open(setup_path, 'w') as f:\n",
    "        f.write(setup_content)\n",
    "    \n",
    "    logger.info(f\"Created setup.py file at {setup_path}\")\n",
    "\n",
    "def create_readme(output_dir, module_docs):\n",
    "    \"\"\"Create a README.md file with information about the integrated solution.\"\"\"\n",
    "    readme_content = [\n",
    "        \"# Relationship-Enhanced Integrated Solution\",\n",
    "        \"\",\n",
    "        \"This is an automatically generated integrated solution that combines functionality from multiple modules,\",\n",
    "        \"with enhanced documentation of relationships between functions and classes.\",\n",
    "        \"\",\n",
    "        \"## Architecture Overview\",\n",
    "        \"\",\n",
    "        \"The solution consists of the following modules, each with distinct responsibilities:\",\n",
    "        \"\"\n",
    "    ]\n",
    "    \n",
    "    # Add module descriptions\n",
    "    for module_name, doc in module_docs.items():\n",
    "        readme_content.append(f\"### {module_name}_code\")\n",
    "        readme_content.append(f\"{doc['docstring']}\")\n",
    "        \n",
    "        # Add entry points\n",
    "        entry_points = doc.get(\"relationships\", {}).get(\"entry_points\", [])\n",
    "        if entry_points:\n",
    "            readme_content.append(\"\\nEntry Points:\")\n",
    "            for ep in entry_points:\n",
    "                readme_content.append(f\"- `{ep}`\")\n",
    "        \n",
    "        # Add functions with their relationships\n",
    "        if doc[\"functions\"]:\n",
    "            readme_content.append(\"\\nKey Functions:\")\n",
    "            for func in doc[\"functions\"]:\n",
    "                # Only include functions that have relationships or are entry points\n",
    "                has_relationships = (\n",
    "                    func.get(\"relationships\", {}).get(\"calls_functions\") or \n",
    "                    func.get(\"relationships\", {}).get(\"instantiates_classes\") or\n",
    "                    func.get(\"relationships\", {}).get(\"called_by\")\n",
    "                )\n",
    "                \n",
    "                is_entry_point = func[\"name\"] in entry_points\n",
    "                \n",
    "                if has_relationships or is_entry_point:\n",
    "                    # Safely get docstring summary\n",
    "                    docstring_summary = get_docstring_summary(func.get('docstring'))\n",
    "                    readme_content.append(f\"- `{func['name']}`: {docstring_summary}\")\n",
    "                    \n",
    "                    # Add relationship info\n",
    "                    if has_relationships:\n",
    "                        rel = func.get(\"relationships\", {})\n",
    "                        calls = rel.get(\"calls_functions\", [])\n",
    "                        if calls:\n",
    "                            readme_content.append(f\"  - Calls: {', '.join(calls)}\")\n",
    "                        \n",
    "                        called_by = rel.get(\"called_by\", [])\n",
    "                        if called_by:\n",
    "                            readme_content.append(f\"  - Called by: {', '.join(called_by)}\")\n",
    "                        \n",
    "                        instantiates = rel.get(\"instantiates_classes\", [])\n",
    "                        if instantiates:\n",
    "                            readme_content.append(f\"  - Instantiates: {', '.join(instantiates)}\")\n",
    "        \n",
    "        readme_content.append(\"\")\n",
    "    \n",
    "    # Add integration information\n",
    "    readme_content.extend([\n",
    "        \"## Integration Strategy\",\n",
    "        \"\",\n",
    "        \"The integration follows these principles:\",\n",
    "        \"\",\n",
    "        \"1. **Dependency-Based Execution**: Functions are called in an order that respects their dependencies\",\n",
    "        \"2. **Module Isolation**: Each module maintains its own namespace to prevent conflicts\",\n",
    "        \"3. **Coordinated Execution**: The main execution orchestrates the flow across modules\",\n",
    "        \"\",\n",
    "        \"## Documentation\",\n",
    "        \"\",\n",
    "        \"For more detailed information about the relationships between components, see:\",\n",
    "        \"\",\n",
    "        \"- `RELATIONSHIPS.md`: Detailed documentation of all module and function relationships\",\n",
    "        \"- `integrated_solution.py`: The main integration file with relationship comments\",\n",
    "        \"- `__init__.py`: Contains module relationship information\"\n",
    "    ])\n",
    "    \n",
    "    readme_path = os.path.join(output_dir, \"README.md\")\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(readme_content))\n",
    "    \n",
    "    logger.info(f\"Created enhanced README.md file at {readme_path}\")\n",
    "\n",
    "def integrate_code_with_enhanced_relationships(code_generation_folder=None):\n",
    "    \"\"\"\n",
    "    Integrate code with enhanced relationship documentation and analysis\n",
    "    \n",
    "    Args:\n",
    "        code_generation_folder: Path to the folder containing generated code (if None, uses the latest folder)\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the output directory with integrated solution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check OpenAI configuration\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = AzureChatOpenAI(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "            deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "            temperature=0.1  # Low temperature for more deterministic output\n",
    "        )\n",
    "        \n",
    "        # Find the code generation folder (latest or specified)\n",
    "        if code_generation_folder is None:\n",
    "            code_generation_folder = find_latest_code_generation_folder()\n",
    "        \n",
    "        logger.info(f\"Using code generation folder: {code_generation_folder}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_dir = os.path.join(os.path.dirname(code_generation_folder), f\"integrated_solution_{timestamp}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all code files\n",
    "        code_files = find_code_files(code_generation_folder)\n",
    "        logger.info(f\"Found {len(code_files)} code files to integrate\")\n",
    "        \n",
    "        if not code_files:\n",
    "            logger.error(\"No code files found to integrate\")\n",
    "            return None\n",
    "        \n",
    "        # Read all code files\n",
    "        code_contents = read_code_files(code_files)\n",
    "        \n",
    "        # Save modules with proper names\n",
    "        save_modules_with_proper_names(output_dir, code_contents)\n",
    "        \n",
    "        # Generate enhanced API documentation with relationships\n",
    "        doc_generator = EnhancedAPIDocGenerator(model)\n",
    "        module_docs = doc_generator.generate_all_module_docs(code_contents)\n",
    "        \n",
    "        # Save enhanced API documentation\n",
    "        api_docs_path = os.path.join(output_dir, \"enhanced_api_documentation.json\")\n",
    "        with open(api_docs_path, 'w') as f:\n",
    "            json.dump(module_docs, f, indent=2)\n",
    "        logger.info(f\"Saved enhanced API documentation to {api_docs_path}\")\n",
    "        \n",
    "        # Create detailed relationship documentation\n",
    "        create_relationship_documentation(output_dir, module_docs)\n",
    "        \n",
    "        # Try to generate dependency graph\n",
    "        try:\n",
    "            # Try to import required libraries\n",
    "            import matplotlib.pyplot as plt\n",
    "            import networkx as nx\n",
    "            \n",
    "            # Try to generate the graph\n",
    "            dependency_graph = doc_generator.try_generate_dependency_graph(module_docs)\n",
    "            if dependency_graph:\n",
    "                graph_path = os.path.join(output_dir, \"module_dependencies.png\")\n",
    "                plt.savefig(graph_path)\n",
    "                logger.info(f\"Saved dependency graph visualization to {graph_path}\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"matplotlib or networkx not available, skipping graph generation\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not generate dependency graph: {e}\")\n",
    "        \n",
    "        # Generate integrated code\n",
    "        integrator = EnhancedCodeIntegrator(model, doc_generator)\n",
    "        integrated_code = integrator.generate_integrated_code(module_docs)\n",
    "        \n",
    "        # Add header\n",
    "        header = f'''\"\"\"\n",
    "Relationship-Enhanced Integrated Solution\n",
    "This file was automatically generated by the Combined Code Generation and Integration System.\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "This code serves as an integration layer that coordinates all the individual modules.\n",
    "Each module's code is stored in separate files named by their user story IDs with \"_code.py\" suffix.\n",
    "The integration is based on detailed analysis of function and class relationships between modules.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "        integrated_code = header + integrated_code\n",
    "        \n",
    "        # Save integrated code\n",
    "        integrated_code_path = os.path.join(output_dir, \"integrated_solution.py\")\n",
    "        with open(integrated_code_path, 'w') as f:\n",
    "            f.write(integrated_code)\n",
    "        \n",
    "        # Create enhanced __init__.py file\n",
    "        integrator.generate_init_file(output_dir, module_docs)\n",
    "        \n",
    "        # Create enhanced README.md\n",
    "        create_readme(output_dir, module_docs)\n",
    "        \n",
    "        # Create setup.py\n",
    "        create_setup_py(output_dir)\n",
    "        \n",
    "        logger.info(f\"Successfully created relationship-enhanced integrated solution: {integrated_code_path}\")\n",
    "        print(f\"Relationship-enhanced integrated solution created at: {output_dir}\")\n",
    "        \n",
    "        return output_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in code integration: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "#############################################################\n",
    "# Combined Workflow\n",
    "#############################################################\n",
    "\n",
    "def combined_code_generation_and_integration(excel_file_path=\"tech.xlsx\"):\n",
    "    \"\"\"\n",
    "    Combined workflow function that runs both code generation and integration\n",
    "    \n",
    "    Args:\n",
    "        excel_file_path: Path to Excel file with technical specifications\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, str]: Paths to the code generation and integrated solution directories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Configure OpenAI\n",
    "        check_openai_config()\n",
    "        \n",
    "        # Step 2: Generate code from technical specifications\n",
    "        print(\"Starting code generation from technical specifications...\")\n",
    "        code_generation_dir = process_tech_specs(excel_file_path)\n",
    "        \n",
    "        if not code_generation_dir:\n",
    "            logger.error(\"Code generation failed or no tech specs found\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"Completed code generation. Directory: {code_generation_dir}\")\n",
    "        \n",
    "        # Step 3: Integrate the generated code with relationship enhancement\n",
    "        print(\"Starting code integration with relationship analysis...\")\n",
    "        integrated_solution_dir = integrate_code_with_enhanced_relationships(code_generation_dir)\n",
    "        \n",
    "        if not integrated_solution_dir:\n",
    "            logger.error(\"Code integration failed\")\n",
    "            return code_generation_dir, None\n",
    "        \n",
    "        print(f\"Completed code integration. Directory: {integrated_solution_dir}\")\n",
    "        \n",
    "        # Return both directory paths\n",
    "        return code_generation_dir, integrated_solution_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in combined workflow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the combined workflow\n",
    "    code_gen_dir, integrated_dir = combined_code_generation_and_integration(\"tech.xlsx\")\n",
    "    \n",
    "    if code_gen_dir and integrated_dir:\n",
    "        print(\"\\nWorkflow completed successfully!\")\n",
    "        print(f\"Generated code: {code_gen_dir}\")\n",
    "        print(f\"Integrated solution: {integrated_dir}\")\n",
    "    else:\n",
    "        print(\"\\nWorkflow completed with errors. Check logs for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
