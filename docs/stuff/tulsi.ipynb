{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRAG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentLoader\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRAG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunker_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkerFactory\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mRAG\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedings_geneartor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EnhancedEmbeddingGenerator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from app.RAG.document_loader import DocumentLoader\n",
    "from app.RAG.chunker_factory import ChunkerFactory\n",
    "from app.RAG.embedings_geneartor import EnhancedEmbeddingGenerator\n",
    "from app.RAG.vector_store import VectorStoreFactory\n",
    "from app.RAG.vector_search import SearchFactory\n",
    "from app.RAG.rag_types import StandardRAG, GraphRAG, AdaptiveRAG, RaptorRAG, CorrectiveRAG, IterativeRAG\n",
    "from app.RAG.prompt_optimizer import PromptOptimizer\n",
    "from app.RAG.reranking_model import RerankingModel\n",
    "from app.RAG.fact_checker import FactChecker\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, rag_type: str, llm_model: str, embedding_model: str, \n",
    "                 chunking_option: str, vector_db: str, search_option: str, database_name: str = None, \n",
    "                 database_id: str = None, load_only: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize RAG pipeline with provided configurations\n",
    "        \n",
    "        Args:\n",
    "            rag_type (str): Type of RAG implementation to use\n",
    "            llm_model (str): Language model to use\n",
    "            embedding_model (str): Embedding model to use\n",
    "            chunking_option (str): Method for chunking documents\n",
    "            vector_db (str): Vector database to use\n",
    "            search_option (str): Search algorithm to use\n",
    "            database_name (str): Name of the database\n",
    "            database_id (str): ID of the database\n",
    "            load_only (bool): If True, only load existing vector store without creating new components\n",
    "        \"\"\"\n",
    "        self.cleanup_required = False\n",
    "        self.rag_type = rag_type\n",
    "        self.llm_model = llm_model\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunking_option = chunking_option\n",
    "        self.vector_db = vector_db\n",
    "        self.search_option = search_option\n",
    "        self.database_name = database_name\n",
    "        self.database_id = database_id\n",
    "        \n",
    "        try:\n",
    "            collection_name = None\n",
    "            if database_name:\n",
    "                collection_name = f\"rag-{database_name.lower()}-{database_id or 'temp'}\"\n",
    "            # When load_only is True, we'll still initialize all required components\n",
    "            # but we'll load the vector store instead of creating a new one\n",
    "            if load_only:\n",
    "                print(f\"???? Initializing RAG Pipeline in load-only mode\")\n",
    "                print(f\"???? Database: {database_name}\")\n",
    "                print(f\"???? Vector DB: {vector_db}\")\n",
    "                \n",
    "                # Load the vector store first\n",
    "                print(f\"???? Loading existing Vector Store: {vector_db}...\")\n",
    "                if self.load_vector_store():\n",
    "                    print(\"? Vector Store loaded successfully\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Failed to load vector store for {database_name}\")\n",
    "                \n",
    "                # Initialize language model for queries\n",
    "                print(f\"???? Initializing Language Model: {llm_model}...\")\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(llm_model)\n",
    "                self.generator = pipeline(\n",
    "                    \"text2text-generation\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    max_length=512\n",
    "                )\n",
    "                print(\"? Language Model initialized successfully\")\n",
    "                \n",
    "                # Initialize other required components\n",
    "                print(f\"???? Initializing Search Algorithm: {search_option}...\")\n",
    "                self.search_algorithm = SearchFactory.create_search_algorithm(search_option)\n",
    "                print(\"? Search Algorithm initialized successfully\")\n",
    "                \n",
    "                print(f\"???? Initializing Embedding Generator with model: {embedding_model}...\")\n",
    "                self.embedding_generator = EnhancedEmbeddingGenerator(\n",
    "                    model_name=embedding_model,\n",
    "                    batch_size=32\n",
    "                )\n",
    "                print(\"? Embedding Generator initialized successfully\")\n",
    "                \n",
    "                # Initialize minimal components needed for RAG implementations\n",
    "                print(\"???? Initializing Document Loader...\")\n",
    "                self.document_loader = DocumentLoader()\n",
    "                print(\"? Document Loader initialized successfully\")\n",
    "\n",
    "                print(f\"???? Initializing Chunker with option: {chunking_option}...\")\n",
    "                max_tokens = 100\n",
    "                token_overlap = 10\n",
    "                self.chunker = ChunkerFactory.create_chunker(chunking_option, max_tokens, token_overlap)\n",
    "                print(\"? Chunker initialized successfully\")\n",
    "                \n",
    "                # Initialize fact checker for certain RAG types\n",
    "                if rag_type in [\"corrective\"]:\n",
    "                    self.fact_checker = FactChecker()\n",
    "                \n",
    "                # Initialize RAG implementation\n",
    "                self.initialize_rag_implementation()\n",
    "                \n",
    "                print(\"???? RAG Pipeline loaded in query-only mode!\")\n",
    "                return\n",
    "            \n",
    "            # Normal initialization for creating new vector stores\n",
    "            collection_name = None\n",
    "            if database_name:\n",
    "                collection_name = f\"rag-{database_name.lower()}-{database_id or 'temp'}\"\n",
    "            print(\"collection_name\", collection_name)\n",
    "            print(\"database_name\", database_name)\n",
    "            print(f\"???? Initializing RAG Pipeline\")\n",
    "            print(f\"  RAG Type: {rag_type}\")\n",
    "            print(f\"  LLM Model: {llm_model}\")\n",
    "            print(f\"  Embedding Model: {embedding_model}\")\n",
    "            print(f\"  Chunking Option: {chunking_option}\")\n",
    "            print(f\"  Vector DB: {vector_db}\")\n",
    "            print(f\"  Search Option: {search_option}\")\n",
    "\n",
    "            # Handle iterative and corrective RAG types\n",
    "            if rag_type in [\"iterative\", \"corrective\"]:\n",
    "                rag_type = \"graph\"\n",
    "                print(f\"???? Note: {rag_type} RAG will use Graph RAG implementation.\")\n",
    "            \n",
    "            # Set fixed confidence threshold for adaptive RAG\n",
    "            confidence_threshold = 0.7 if rag_type == \"adaptive\" else None\n",
    "\n",
    "            # Set fixed RAPTOR parameters\n",
    "            raptor_params = {}\n",
    "            if rag_type == \"raptor\":\n",
    "                raptor_params = {\n",
    "                    'token_weight_threshold': 0.5,\n",
    "                    'max_prompt_attempts': 3\n",
    "                }\n",
    "\n",
    "            # Set fixed reranking model for RAPTOR\n",
    "            reranking_model_name = None\n",
    "            if rag_type == \"raptor\":\n",
    "                reranking_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "            # Handle GTE model case\n",
    "            if embedding_model == \"Alibaba-NLP/gte-large-en-v1.5\":\n",
    "                embedding_model = \"intfloat/e5-large-v2\"\n",
    "\n",
    "            # Set fixed values for tokens and overlap\n",
    "            max_tokens = 100\n",
    "            token_overlap = 10\n",
    "\n",
    "            # Initialize components\n",
    "            print(\"???? Initializing Document Loader...\")\n",
    "            self.document_loader = DocumentLoader()\n",
    "            print(\"? Document Loader initialized successfully\")\n",
    "\n",
    "            print(f\"???? Initializing Chunker with option: {chunking_option}...\")\n",
    "            self.chunker = ChunkerFactory.create_chunker(chunking_option, max_tokens, token_overlap)\n",
    "            print(\"? Chunker initialized successfully\")\n",
    "\n",
    "            print(f\"???? Initializing Embedding Generator with model: {embedding_model}...\")\n",
    "            self.embedding_generator = EnhancedEmbeddingGenerator(\n",
    "                model_name=embedding_model,\n",
    "                batch_size=32\n",
    "            )\n",
    "            print(\"? Embedding Generator initialized successfully\")\n",
    "\n",
    "            print(f\"???? Initializing Vector Store: {vector_db}...\")\n",
    "            # self.vector_store = VectorStoreFactory.create_store(\n",
    "            #     store_type=vector_db,\n",
    "            #     config={\n",
    "            #         \"embedding_model\": embedding_model,\n",
    "            #         \"collection_name\": collection_name,\n",
    "            #         \"database_name\": database_name\n",
    "            #     }\n",
    "            # )\n",
    "\n",
    "            self.vector_store = VectorStoreFactory.create_store(\n",
    "                store_type=vector_db,\n",
    "                embedding_model=embedding_model,\n",
    "                collection_name=collection_name,\n",
    "                database_name = database_name\n",
    "                \n",
    "            )\n",
    "            print(\"? Vector Store initialized successfully\")\n",
    "\n",
    "            print(f\"???? Initializing Search Algorithm: {search_option}...\")\n",
    "            self.search_algorithm = SearchFactory.create_search_algorithm(search_option)\n",
    "            print(\"? Search Algorithm initialized successfully\")\n",
    "            \n",
    "            # Initialize language model\n",
    "            print(f\"???? Initializing Language Model: {llm_model}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(llm_model)\n",
    "            self.generator = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_length=512\n",
    "            )\n",
    "            print(\"? Language Model initialized successfully\")\n",
    "\n",
    "            self.fact_checker = FactChecker()\n",
    "            \n",
    "            # Initialize RAG implementation\n",
    "            self.initialize_rag_implementation()\n",
    "            print(\"???? RAG Pipeline initialized completely!\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"? Error during initialization: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "    \n",
    "    def initialize_rag_implementation(self):\n",
    "        \"\"\"Initialize the appropriate RAG implementation based on rag_type\"\"\"\n",
    "        print(f\"???? Initializing RAG Implementation: {self.rag_type}...\")\n",
    "        \n",
    "        base_params = {\n",
    "            'document_loader': self.document_loader,\n",
    "            'chunker': self.chunker,\n",
    "            'embedding_generator': self.embedding_generator,\n",
    "            'vector_store': self.vector_store,\n",
    "            'search_algorithm': self.search_algorithm,\n",
    "            'generator': self.generator\n",
    "        }\n",
    "        \n",
    "        if self.rag_type == \"standard\":\n",
    "            self.rag_implementation = StandardRAG(**base_params)\n",
    "        elif self.rag_type == \"graph\":\n",
    "            self.rag_implementation = GraphRAG(**base_params)\n",
    "        elif self.rag_type == \"corrective\":\n",
    "            if not hasattr(self, 'fact_checker'):\n",
    "                self.fact_checker = FactChecker()\n",
    "            base_params['fact_checker'] = self.fact_checker\n",
    "            self.rag_implementation = CorrectiveRAG(**base_params)\n",
    "        elif self.rag_type == \"iterative\":\n",
    "            self.rag_implementation = IterativeRAG(**base_params)\n",
    "        elif self.rag_type == \"adaptive\":\n",
    "            self.rag_implementation = AdaptiveRAG(\n",
    "                confidence_threshold=0.7,\n",
    "                **base_params\n",
    "            )\n",
    "        elif self.rag_type == \"raptor\":\n",
    "            reranking_model = RerankingModel(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "            prompt_optimizer = PromptOptimizer(\n",
    "                embedding_generator=self.embedding_generator,\n",
    "                tokenizer=self.tokenizer\n",
    "            )\n",
    "            \n",
    "            raptor_params = {\n",
    "                'token_weight_threshold': 0.5,\n",
    "                'max_prompt_attempts': 3,\n",
    "                'reranking_model': reranking_model,\n",
    "                'prompt_optimizer': prompt_optimizer\n",
    "            }\n",
    "            \n",
    "            self.rag_implementation = RaptorRAG(**raptor_params, **base_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported RAG type: {self.rag_type}\")\n",
    "        \n",
    "        print(\"? RAG Implementation initialized successfully\")\n",
    "\n",
    "    def process_documents(self, zip_path: str):\n",
    "        \"\"\"\n",
    "        Process documents through the selected RAG implementation\n",
    "        \n",
    "        Args:\n",
    "            zip_path (str): Path to the ZIP file containing documents\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of processed documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"???? Starting document processing for: {zip_path}\")\n",
    "            documents = self.rag_implementation.process_documents(zip_path)\n",
    "            \n",
    "            if documents:\n",
    "                print(f\"? Successfully processed {len(documents)} documents\")\n",
    "                print(f\"????? Documents loaded into vector store\")\n",
    "                self.cleanup_required = True\n",
    "            else:\n",
    "                print(\"?? No documents were processed\")\n",
    "            \n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            print(f\"? Error processing documents: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            self.cleanup()\n",
    "            return []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def generate_response(self, query: str, max_length: int = 512) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using the selected RAG implementation\n",
    "        \n",
    "        Args:\n",
    "            query (str): User query\n",
    "            max_length (int): Maximum length of generated response\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"? Generating response for query: {query[:50]}...\")\n",
    "            \n",
    "            # For loaded RAG databases, we may need to retrieve context directly\n",
    "            if hasattr(self, 'vector_store') and not hasattr(self.rag_implementation, 'generate_response'):\n",
    "                print(\"???? Using direct context retrieval for loaded RAG database\")\n",
    "                \n",
    "                # Retrieve relevant context\n",
    "                context_docs = self.rag_implementation.retrieve_context(query, max_docs=3)\n",
    "                \n",
    "                if context_docs:\n",
    "                    context_text = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "                    print(f\"? Retrieved {len(context_docs)} context documents\")\n",
    "                    \n",
    "                    # Construct prompt with context\n",
    "                    prompt = f\"\"\"Context information:\n",
    "    {context_text}\n",
    "\n",
    "    Given the context information, please respond to the following:\n",
    "    {query}\"\"\"\n",
    "                    \n",
    "                    # Generate response with context\n",
    "                    response = self.generator(prompt, max_length=max_length)[0]['generated_text']\n",
    "                else:\n",
    "                    print(\"?? No relevant context found, generating response without RAG\")\n",
    "                    response = self.generator(query, max_length=max_length)[0]['generated_text']\n",
    "                    \n",
    "                return response\n",
    "                \n",
    "            # Use the RAG implementation's generate_response if available\n",
    "            elif hasattr(self.rag_implementation, 'generate_response'):\n",
    "                response = self.rag_implementation.generate_response(query, max_length)\n",
    "                \n",
    "                if response:\n",
    "                    print(\"? Response generated successfully\")\n",
    "                else:\n",
    "                    print(\"?? No response generated\")\n",
    "                \n",
    "                return response\n",
    "            else:\n",
    "                # Fallback to direct generation without RAG\n",
    "                print(\"?? No RAG implementation found, using direct generation\")\n",
    "                response = self.generator(query, max_length=max_length)[0]['generated_text']\n",
    "                return response\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"? Error generating response: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        if self.cleanup_required:\n",
    "            try:\n",
    "                print(\"???? Starting cleanup process...\")\n",
    "                if hasattr(self, 'document_loader') and self.document_loader:\n",
    "                    self.document_loader.cleanup()\n",
    "                print(\"? Cleanup completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"? Error during cleanup: {str(e)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Destructor to ensure cleanup\"\"\"\n",
    "        self.cleanup()\n",
    "\n",
    "\n",
    "\n",
    "    def retrieve_context(self, query: str, max_docs: int = 3):\n",
    "        \"\"\"\n",
    "        Retrieve relevant context for a given query using the appropriate RAG implementation\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user query\n",
    "            max_docs (int): Maximum number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: List of relevant documents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a Document object with the query as page_content\n",
    "            from langchain_core.documents import Document\n",
    "            query_doc = Document(page_content=query)\n",
    "            \n",
    "            # Generate embeddings for the query using the embedding_generator\n",
    "            query_embedding = self.embedding_generator.generate_embeddings([query_doc])[0]\n",
    "            \n",
    "            if self.vector_db == \"chromadb\":\n",
    "                import chromadb\n",
    "                \n",
    "                # Create persistent client\n",
    "                client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "                \n",
    "                # Generate collection name\n",
    "                collection_names = [\n",
    "                    f\"rag_{self.database_name.lower()}_Temp\",\n",
    "                    f\"rag-{self.database_name.lower()}-{self.database_id or 'temp'}\",\n",
    "                    f\"rag_{self.database_name.lower()}_{self.database_id}\"\n",
    "                ]\n",
    "                \n",
    "                for collection_name in collection_names:\n",
    "                    try:\n",
    "                        # Get the collection\n",
    "                        collection = client.get_collection(name=collection_name)\n",
    "                        \n",
    "                        # Query the collection\n",
    "                        results = collection.query(\n",
    "                            query_embeddings=[query_embedding],\n",
    "                            n_results=max_docs\n",
    "                        )\n",
    "                        \n",
    "                        # Convert results to document format\n",
    "                        documents = []\n",
    "                        for i, (doc_content, metadata) in enumerate(zip(\n",
    "                            results.get('documents', [[]])[0],\n",
    "                            results.get('metadatas', [[]])[0] \n",
    "                        )):\n",
    "                            documents.append(Document(\n",
    "                                page_content=doc_content,\n",
    "                                metadata=metadata or {}\n",
    "                            ))\n",
    "                        \n",
    "                        return documents\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with ChromaDB collection {collection_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(\"? No suitable ChromaDB collection found\")\n",
    "                return []\n",
    "                        \n",
    "            elif self.vector_db == \"pinecone\":\n",
    "                from pinecone import Pinecone\n",
    "                \n",
    "                # Initialize Pinecone client\n",
    "                pc = Pinecone(api_key=\"your_pinecone_api_key\")  # Replace with actual API key\n",
    "                \n",
    "                # Try different naming patterns\n",
    "                index_names = [\n",
    "                    f\"rag-{self.database_name.lower()}-temp\",\n",
    "                    f\"rag_{self.database_name.lower()}_{self.database_id}\",\n",
    "                    f\"rag-{self.database_name.lower()}-{self.database_id}\"\n",
    "                ]\n",
    "                \n",
    "                for index_name in index_names:\n",
    "                    try:\n",
    "                        # Get the index\n",
    "                        index = pc.Index(index_name)\n",
    "                        \n",
    "                        # Query Pinecone index\n",
    "                        results = index.query(\n",
    "                            vector=query_embedding,\n",
    "                            top_k=max_docs,\n",
    "                            include_metadata=True\n",
    "                        )\n",
    "                        \n",
    "                        # Convert results to document format\n",
    "                        documents = []\n",
    "                        for match in results.get('matches', []):\n",
    "                            content = match.get('metadata', {}).get('content', '')\n",
    "                            metadata = {k: v for k, v in match.get('metadata', {}).items() if k != 'content'}\n",
    "                            documents.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata=metadata\n",
    "                            ))\n",
    "                            \n",
    "                        return documents\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with Pinecone index {index_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(\"? No suitable Pinecone index found\")\n",
    "                return []\n",
    "                        \n",
    "            elif self.vector_db == \"weaviate\":\n",
    "                import weaviate\n",
    "                \n",
    "                # Initialize Weaviate client\n",
    "                client = weaviate.Client(url=\"http://localhost:8080\")\n",
    "                \n",
    "                # Try different class naming patterns\n",
    "                class_names = [\n",
    "                    f\"RAG_{self.database_name.replace(' ', '_').lower()}_Temp\",\n",
    "                    f\"RAG_{self.database_name.replace(' ', '_').lower()}_{self.database_id}\",\n",
    "                    f\"Rag{self.database_name.replace(' ', '')}\"\n",
    "                ]\n",
    "                \n",
    "                for class_name in class_names:\n",
    "                    try:\n",
    "                        # Query Weaviate\n",
    "                        results = (\n",
    "                            client.query.get(class_name, [\"content\", \"file_name\", \"chunk_id\"])\n",
    "                            .with_near_vector({\n",
    "                                \"vector\": query_embedding\n",
    "                            })\n",
    "                            .with_limit(max_docs)\n",
    "                            .do()\n",
    "                        )\n",
    "                        \n",
    "                        # Convert results to document format\n",
    "                        documents = []\n",
    "                        for item in results.get('data', {}).get('Get', {}).get(class_name, []):\n",
    "                            content = item.get('content', '')\n",
    "                            metadata = {\n",
    "                                'file_name': item.get('file_name', ''),\n",
    "                                'chunk_id': item.get('chunk_id', '')\n",
    "                            }\n",
    "                            documents.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata=metadata\n",
    "                            ))\n",
    "                            \n",
    "                        return documents\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with Weaviate class {class_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(\"? No suitable Weaviate class found\")\n",
    "                return []\n",
    "                        \n",
    "            elif self.vector_db == \"faiss\":\n",
    "                import numpy as np\n",
    "                import faiss\n",
    "                \n",
    "                try:\n",
    "                    # Convert query embedding to the right format\n",
    "                    query_vector = np.array([query_embedding]).astype('float32')\n",
    "                    \n",
    "                    # Search in the index\n",
    "                    D, I = self.vector_store.search(query_vector, max_docs)\n",
    "                    \n",
    "                    # Retrieve documents based on indices\n",
    "                    documents = []\n",
    "                    for i in I[0]:\n",
    "                        if i < len(self.stored_documents):\n",
    "                            documents.append(self.stored_documents[i])\n",
    "                    \n",
    "                    return documents\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"FAISS error: {str(e)}\")\n",
    "                    return []\n",
    "                    \n",
    "            else:\n",
    "                print(f\"Unsupported vector store type: {self.vector_db}\")\n",
    "                return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving context: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "    def load_vector_store(self):\n",
    "        \"\"\"Load an existing vector store based on the database configuration\"\"\"\n",
    "        try:\n",
    "            if self.vector_db == \"chromadb\":\n",
    "                import chromadb\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                \n",
    "                # Create persistent client\n",
    "                client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "                # existing_collections = client.list_collections()\n",
    "                # print(\"Existing ChromaDB collections:\")\n",
    "                # for collection in existing_collections:\n",
    "                #     print(f\"- {collection.name}\")\n",
    "            \n",
    "                \n",
    "                # Try different naming patterns\n",
    "                collection_names = [\n",
    "                    f\"rag_{self.database_name.lower()}_Temp\",\n",
    "                    f\"rag-{self.database_name.lower()}-{self.database_id or 'temp'}\",\n",
    "                    f\"rag_{self.database_name.lower()}_{self.database_id}\"\n",
    "                ]\n",
    "                \n",
    "                for collection_name in collection_names:\n",
    "                    try:\n",
    "                        print(f\"Attempting to load ChromaDB collection: {collection_name}\")\n",
    "                        self.vector_store = client.get_collection(name=collection_name)\n",
    "                        print(f\"? Successfully loaded ChromaDB collection: {collection_name}\")\n",
    "                        return True\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Collection {collection_name} not found: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # If we get here, we couldn't find any matching collection\n",
    "                print(\"? Failed to find any matching ChromaDB collection\")\n",
    "                return False\n",
    "                        \n",
    "            elif self.vector_db == \"pinecone\":\n",
    "                from pinecone import Pinecone\n",
    "                \n",
    "                # Initialize Pinecone client\n",
    "                pc = Pinecone(api_key=\"pcsk_2sqpR7_FY6XeaGrqY1NikHysefnoj37anCK9fWMZ5rrxPzW3HU5xWUPVgJZSep9sYpdsCw\")\n",
    "                \n",
    "                # Try different naming patterns\n",
    "                index_names = [\n",
    "                    # f\"rag-{self.database_name.lower()}-Temp\",\n",
    "                    # f\"rag_{self.database_name.lower()}_{self.database_id}\",\n",
    "                    # f\"rag-{self.database_name.lower()}-{self.database_id}\",\n",
    "                    \"pineenv\"\n",
    "                ]\n",
    "                \n",
    "                # List available indexes for debugging\n",
    "                available_indexes = pc.list_indexes().names()\n",
    "                print(f\"Available Pinecone indexes: {available_indexes}\")\n",
    "                \n",
    "                for index_name in index_names:\n",
    "                    try:\n",
    "                        print(f\"Attempting to load Pinecone index: {index_name}\")\n",
    "                        # Check if index exists in available indexes\n",
    "                        matching_indexes = [idx for idx in available_indexes if idx.startswith(index_name)]\n",
    "                        \n",
    "                        if matching_indexes:\n",
    "                            # Get the most recently created index\n",
    "                            actual_index_name = sorted(matching_indexes)[-1]\n",
    "                            self.vector_store = pc.Index(actual_index_name)\n",
    "                            print(f\"? Successfully loaded Pinecone index: {actual_index_name}\")\n",
    "                            return True\n",
    "                        else:\n",
    "                            print(f\"No Pinecone index found matching pattern: {index_name}\")\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error connecting to Pinecone index {index_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # If we get here, we couldn't find any matching index\n",
    "                print(\"? Failed to find any matching Pinecone index\")\n",
    "                return False\n",
    "                    \n",
    "            elif self.vector_db == \"weaviate\":\n",
    "                import weaviate\n",
    "                \n",
    "                # Initialize Weaviate client\n",
    "                client = weaviate.Client(url=\"http://localhost:8080\")\n",
    "                \n",
    "                # Try different naming patterns\n",
    "                class_names = [\n",
    "                    f\"RAG_{self.database_name.replace(' ', '_').lower()}_Temp\",\n",
    "                    f\"RAG_{self.database_name.replace(' ', '_').lower()}_{self.database_id}\",\n",
    "                    f\"Rag{self.database_name.replace(' ', '')}\"  # Another possible pattern\n",
    "                ]\n",
    "                \n",
    "                # List available classes for debugging\n",
    "                schema = client.schema.get()\n",
    "                available_classes = [cls['class'] for cls in schema['classes']] if 'classes' in schema else []\n",
    "                print(f\"Available Weaviate classes: {available_classes}\")\n",
    "                \n",
    "                for class_name in class_names:\n",
    "                    try:\n",
    "                        print(f\"Checking for Weaviate class: {class_name}\")\n",
    "                        if client.schema.exists(class_name):\n",
    "                            self.vector_store = client\n",
    "                            self.class_name = class_name\n",
    "                            print(f\"? Successfully loaded Weaviate class: {class_name}\")\n",
    "                            return True\n",
    "                        else:\n",
    "                            print(f\"Weaviate class {class_name} not found\")\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error checking Weaviate class {class_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # If we get here, we couldn't find any matching class\n",
    "                print(\"? Failed to find any matching Weaviate class\")\n",
    "                return False\n",
    "                    \n",
    "            elif self.vector_db == \"faiss\":\n",
    "                import faiss\n",
    "                import pickle\n",
    "                import os\n",
    "                \n",
    "                # Try different file naming patterns\n",
    "                index_paths = [\n",
    "                    f\"./faiss_indexes/rag-{self.database_name.lower()}-Temp.index\",\n",
    "                    f\"./faiss_indexes/rag_{self.database_name.lower()}_Temp.index\",\n",
    "                    f\"./faiss_indexes/rag_{self.database_name.lower()}_{self.database_id}.index\",\n",
    "                    f\"./faiss_indexes/rag-{self.database_name.lower()}-{self.database_id or 'temp'}.index\"\n",
    "                ]\n",
    "                \n",
    "                doc_paths = [\n",
    "                     f\"./faiss_indexes/rag-{self.database_name.lower()}-Temp_docs.pkl\",\n",
    "                    f\"./faiss_indexes/rag_{self.database_name.lower()}_Temp_docs.pkl\",\n",
    "                    f\"./faiss_indexes/rag_{self.database_name.lower()}_{self.database_id}_docs.pkl\",\n",
    "                    f\"./faiss_indexes/rag-{self.database_name.lower()}-{self.database_id or 'temp'}_docs.pkl\"\n",
    "                ]\n",
    "                \n",
    "                # List available files for debugging\n",
    "                if os.path.exists(\"./faiss_indexes\"):\n",
    "                    available_files = os.listdir(\"./faiss_indexes\")\n",
    "                    print(f\"Available FAISS files: {available_files}\")\n",
    "                \n",
    "                for i, index_path in enumerate(index_paths):\n",
    "                    doc_path = doc_paths[i]\n",
    "                    try:\n",
    "                        print(f\"Attempting to load FAISS index: {index_path}\")\n",
    "                        if os.path.exists(index_path) and os.path.exists(doc_path):\n",
    "                            # Load FAISS index and stored documents\n",
    "                            self.vector_store = faiss.read_index(index_path)\n",
    "                            with open(doc_path, 'rb') as f:\n",
    "                                self.stored_documents = pickle.load(f)\n",
    "                            print(f\"? Successfully loaded FAISS index: {index_path}\")\n",
    "                            return True\n",
    "                        else:\n",
    "                            print(f\"FAISS files not found: {index_path} or {doc_path}\")\n",
    "                            continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading FAISS index {index_path}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # If we get here, we couldn't find any matching files\n",
    "                print(\"? Failed to find any matching FAISS index files\")\n",
    "                return False\n",
    "                \n",
    "            # Unsupported vector database type    \n",
    "            print(f\"? Unsupported vector database type: {self.vector_db}\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"? Error loading vector store: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# routes/rag.py\n",
    "from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Form, Body\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from ..database.connection import get_db\n",
    "from ..models.rag import RAGDatabase, RAGFile\n",
    "from ..schemas.rag import RAGDatabase as RAGDatabaseSchema\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from sqlalchemy.orm import joinedload\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from app.RAG.rag_pipeline import RAGPipeline\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from app.RAG.document_loader import DocumentLoader\n",
    "from app.RAG.chunker_factory import ChunkerFactory\n",
    "from app.RAG.embedings_geneartor import EnhancedEmbeddingGenerator\n",
    "from app.RAG.vector_store import VectorStoreFactory\n",
    "from app.RAG.vector_search import SearchFactory\n",
    "from app.RAG.rag_types import StandardRAG, GraphRAG, AdaptiveRAG, RaptorRAG\n",
    "from app.RAG.prompt_optimizer import PromptOptimizer\n",
    "from app.RAG.reranking_model import RerankingModel\n",
    "import chromadb\n",
    "import weaviate\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.documents import Document\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "\n",
    "router = APIRouter(prefix=\"/rag\", tags=[\"rag\"])\n",
    "os.makedirs(\"./chroma_db\", exist_ok=True)\n",
    "\n",
    "def process_zip_file(zip_path: str, extract_path: str):\n",
    "    \"\"\"Extract and process zip file, returning file information\"\"\"\n",
    "    file_info = []\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "            for file_name in zip_ref.namelist():\n",
    "                if not file_name.endswith('/'):  # Skip directories\n",
    "                    file_path = os.path.join(extract_path, file_name)\n",
    "                    try:\n",
    "                        content = \"\"\n",
    "                        is_text = False\n",
    "                        \n",
    "                        text_extensions = {\n",
    "                            '.txt', '.md', '.csv', '.json', '.xml', \n",
    "                            '.yaml', '.yml', '.html', '.htm', '.css', \n",
    "                            '.js', '.py', '.java', '.c', '.cpp', '.h', \n",
    "                            '.hpp', '.sh', '.bat', '.ps1', '.log',\n",
    "                            '.ini', '.conf', '.cfg'\n",
    "                        }\n",
    "                        \n",
    "                        file_extension = os.path.splitext(file_name)[1].lower()\n",
    "                        \n",
    "                        if file_extension in text_extensions:\n",
    "                            try:\n",
    "                                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                                    content = f.read()\n",
    "                                is_text = True\n",
    "                            except UnicodeDecodeError:\n",
    "                                encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n",
    "                                for encoding in encodings:\n",
    "                                    try:\n",
    "                                        with open(file_path, 'r', encoding=encoding) as f:\n",
    "                                            content = f.read()\n",
    "                                        is_text = True\n",
    "                                        break\n",
    "                                    except UnicodeDecodeError:\n",
    "                                        continue\n",
    "                        \n",
    "                        file_info.append({\n",
    "                            \"file_name\": os.path.basename(file_name),\n",
    "                            \"file_extension\": file_extension,\n",
    "                            \"file_path\": file_path,\n",
    "                            \"file_content\": content if is_text else \"\",\n",
    "                            \"file_size\": os.path.getsize(file_path) / 1024  # Convert to KB\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_name}: {str(e)}\")\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=f\"Error processing zip file: {str(e)}\"\n",
    "        )\n",
    "    return file_info\n",
    "\n",
    "\n",
    "@router.post(\"/create\", response_model=RAGDatabaseSchema)\n",
    "async def create_rag_database(\n",
    "    dataset: UploadFile = File(...),\n",
    "    name: str = Form(...),\n",
    "    rag_type: str = Form(...),\n",
    "    llm_model: str = Form(...),\n",
    "    embedding_model: str = Form(...),\n",
    "    chunking_option: str = Form(...),\n",
    "    vector_db: str = Form(...),\n",
    "    search_option: str = Form(...),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    # Validate file is a zip\n",
    "    if not dataset.filename.endswith('.zip'):\n",
    "        raise HTTPException(\n",
    "            status_code=400,\n",
    "            detail=\"File must be a ZIP archive\"\n",
    "        )\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\"rag_zip_uploads\", exist_ok=True)\n",
    "    os.makedirs(\"rag_extracted_uploads\", exist_ok=True)\n",
    "    \n",
    "    # Create unique directories for this upload\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_dir = f\"rag_zip_uploads/{name}_{timestamp}\"\n",
    "    extract_dir = f\"rag_extracted_uploads/{name}_{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # Create directories for this specific upload\n",
    "        os.makedirs(zip_dir, exist_ok=True)\n",
    "        os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "        # Save zip file\n",
    "        zip_path = os.path.join(zip_dir, dataset.filename)\n",
    "        try:\n",
    "            with open(zip_path, \"wb\") as buffer:\n",
    "                shutil.copyfileobj(dataset.file, buffer)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=f\"Failed to save uploaded file: {str(e)}\"\n",
    "            )\n",
    "\n",
    "        # Validate zip file\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                if zip_ref.testzip() is not None:\n",
    "                    raise HTTPException(\n",
    "                        status_code=400,\n",
    "                        detail=\"Corrupted ZIP file\"\n",
    "                    )\n",
    "        except zipfile.BadZipFile:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Invalid ZIP file\"\n",
    "            )\n",
    "\n",
    "        # Initialize RAG Pipeline BEFORE database creation\n",
    "        pipeline = RAGPipeline(\n",
    "            rag_type=rag_type,\n",
    "            llm_model=llm_model,\n",
    "            embedding_model=embedding_model,\n",
    "            chunking_option=chunking_option,\n",
    "            vector_db=vector_db,\n",
    "            search_option=search_option,\n",
    "            database_name=name,\n",
    "            database_id='Temp'\n",
    "        )\n",
    "        \n",
    "        # Process documents first\n",
    "        processed_documents = pipeline.process_documents(zip_path)\n",
    "        \n",
    "        # Check if documents were processed\n",
    "        if not processed_documents:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"No documents were successfully processed\"\n",
    "            )\n",
    "\n",
    "        # Create RAG database record AFTER processing documents\n",
    "        try:\n",
    "            db_rag = RAGDatabase(\n",
    "                name=name,\n",
    "                dataset_path=extract_dir,\n",
    "                zip_file_path=zip_path,\n",
    "                rag_type=rag_type,\n",
    "                llm_model=llm_model,\n",
    "                embedding_model=embedding_model,\n",
    "                chunking_option=chunking_option,\n",
    "                vector_db=vector_db,\n",
    "                search_option=search_option,\n",
    "                total_files=len(processed_documents),\n",
    "                status=\"Processed\"\n",
    "            )\n",
    "            db.add(db_rag)\n",
    "            db.flush()  # This assigns an ID to db_rag\n",
    "\n",
    "            # Create file records\n",
    "            for doc in processed_documents:\n",
    "                db_file = RAGFile(\n",
    "                    rag_database_id=db_rag.id,\n",
    "                    file_name=doc.metadata.get(\"file_name\", \"\"),\n",
    "                    file_extension=doc.metadata.get(\"file_type\", \"\"),\n",
    "                    file_path=doc.metadata.get(\"source\", \"\"),\n",
    "                    file_size=doc.metadata.get(\"file_size\", 0),\n",
    "                    file_content=doc.page_content\n",
    "                )\n",
    "                db.add(db_file)\n",
    "\n",
    "            # Commit all changes\n",
    "            db.commit()\n",
    "            db.refresh(db_rag)\n",
    "            \n",
    "            return db_rag\n",
    "\n",
    "        except SQLAlchemyError as e:\n",
    "            db.rollback()\n",
    "            raise HTTPException(\n",
    "                status_code=500,\n",
    "                detail=f\"Database error: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    except HTTPException:\n",
    "        # Re-raise HTTP exceptions\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        # Catch and handle any other unexpected errors\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to process RAG database: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.get(\"/list\")\n",
    "async def get_rag_databases(db: Session = Depends(get_db)):\n",
    "    \"\"\"Get all RAG databases\"\"\"\n",
    "    try:\n",
    "        from ..models.rag import RAGDatabase as RAGDatabaseModel\n",
    "        return db.query(RAGDatabaseModel).all()\n",
    "    except SQLAlchemyError as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Database error: {str(e)}\"\n",
    "        )\n",
    "        \n",
    "\n",
    "@router.get(\"/{rag_id}\")\n",
    "async def get_rag_database(rag_id: int, db: Session = Depends(get_db)):\n",
    "    \"\"\"Get a specific RAG database by ID\"\"\"\n",
    "    try:\n",
    "        db_rag = db.query(RAGDatabase).filter(RAGDatabase.id == rag_id).first()\n",
    "        if db_rag is None:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=\"RAG database not found\"\n",
    "            )\n",
    "        return db_rag\n",
    "    except SQLAlchemyError as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Database error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@router.delete(\"/{rag_id}\")\n",
    "async def delete_rag_database(rag_id: int, db: Session = Depends(get_db)):\n",
    "    \"\"\"Delete a RAG database by ID\"\"\"\n",
    "    try:\n",
    "        db_rag = db.query(RAGDatabase).filter(RAGDatabase.id == rag_id).first()\n",
    "        if db_rag is None:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=\"RAG database not found\"\n",
    "            )\n",
    "\n",
    "        # Delete associated files and directories\n",
    "        if os.path.exists(db_rag.zip_file_path):\n",
    "            os.remove(db_rag.zip_file_path)\n",
    "        if os.path.exists(db_rag.dataset_path):\n",
    "            shutil.rmtree(db_rag.dataset_path)\n",
    "\n",
    "        # Delete database record\n",
    "        db.delete(db_rag)\n",
    "        db.commit()\n",
    "        return {\"message\": \"RAG database deleted successfully\"}\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        db.rollback()\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Database error: {str(e)}\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to delete RAG database: {str(e)}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@router.get(\"/test/{rag_id}/status\")\n",
    "async def test_vector_store(rag_id: int, db: Session = Depends(get_db)):\n",
    "    \"\"\"Test any vector store database and return its status\"\"\"\n",
    "    try:\n",
    "        # Get RAG database info\n",
    "        db_rag = db.query(RAGDatabase).filter(RAGDatabase.id == rag_id).first()\n",
    "        if not db_rag:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=\"RAG database not found\"\n",
    "            )\n",
    "\n",
    "        status_info = {}\n",
    "        print(\"Name of db\", db_rag.name)\n",
    "        \n",
    "        if db_rag.vector_db == \"chromadb\":\n",
    "            try:\n",
    "                print(\"1\")\n",
    "                # Use PersistentClient with specific path\n",
    "                client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "                print(\"2\")\n",
    "                # Generate consistent collection name\n",
    "                # collection_name = f\"rag_{db_rag.name.lower()}_{db_rag.id}\"\n",
    "                collection_name = f\"rag_{db_rag.name.lower()}_Temp\"\n",
    "                print(\"3\")\n",
    "                try:\n",
    "                    # Try to get the collection\n",
    "                    collection = client.get_collection(name=collection_name)\n",
    "                    print(\"4\")\n",
    "                    status_info = {\n",
    "                        \"collection_name\": collection.name,\n",
    "                        \"total_documents\": collection.count(),\n",
    "                        \"status\": \"active\"\n",
    "                    }\n",
    "                \n",
    "                except ValueError:\n",
    "                    # Collection not found\n",
    "                    collections = client.list_collections()\n",
    "                    status_info = {\n",
    "                        \"status\": \"not_found\",\n",
    "                        \"available_collections\": [c.name for c in collections],\n",
    "                        \"error\": f\"No collection found matching {collection_name}\"\n",
    "                    }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    # Fallback error handling\n",
    "                    status_info = {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e),\n",
    "                        \"available_collections\": [c.name for c in client.list_collections()]\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                status_info = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "        elif db_rag.vector_db == \"pinecone\":\n",
    "            try:\n",
    "                # Initialize Pinecone client\n",
    "                pc = Pinecone(api_key=\"pcsk_7BJ2Bj_DRSQZvLAjq9CdAtcNsXNTB38PsCYbbSC38REHTANgfY2PznaEAY48ReEQBWozni\")\n",
    "                \n",
    "                # List all available indexes\n",
    "                # available_indexes = pc.list_indexes().names()\n",
    "                available_indexes = \"pineenv\"\n",
    "                \n",
    "                # Look for an index that matches our naming pattern\n",
    "                base_name = f\"rag-{db_rag.name.lower()}-temp\"\n",
    "                # matching_indexes = [idx for idx in available_indexes if idx.startswith(base_name)]\n",
    "                matching_indexes = [idx for idx in available_indexes if idx.startswith(available_indexes)]\n",
    "                if matching_indexes:\n",
    "                    # Get the most recently created index (highest number)\n",
    "                    index_name = sorted(matching_indexes)[-1]\n",
    "                    index = pc.Index(index_name)\n",
    "                    index_stats = index.describe_index_stats()\n",
    "                    \n",
    "                    status_info = {\n",
    "                        \"collection_name\": index_name,\n",
    "                        \"total_vectors\": index_stats.get('total_vector_count', 0),\n",
    "                        \"dimension\": index_stats.get('dimension', 0),\n",
    "                        \"status\": \"active\",\n",
    "                        \"all_matching_indexes\": matching_indexes  # Added for debugging\n",
    "                    }\n",
    "                else:\n",
    "                    status_info = {\n",
    "                        \"status\": \"not_found\",\n",
    "                        \"available_indexes\": available_indexes,\n",
    "                        \"error\": f\"No index found matching pattern {base_name}\"\n",
    "                    }\n",
    "                \n",
    "            except Exception as e:\n",
    "                status_info = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "        elif db_rag.vector_db == \"weaviate\":\n",
    "            try:\n",
    "                # Initialize Weaviate client\n",
    "                client = weaviate.Client(url=\"http://localhost:8080\")\n",
    "                \n",
    "                # Generate class name\n",
    "                # class_name = f\"RAG_{db_rag.name.replace(' ', '_').lower()}_{db_rag.id}\"\n",
    "                class_name = f\"RAG_{db_rag.name.replace(' ', '_').lower()}_Temp\"\n",
    "                \n",
    "                try:\n",
    "                    # Check if class exists\n",
    "                    if client.schema.exists(class_name):\n",
    "                        # Count objects in the class\n",
    "                        count = client.query.aggregate(class_name).with_meta_count().do()\n",
    "                        \n",
    "                        status_info = {\n",
    "                            \"collection_name\": class_name,\n",
    "                            \"total_objects\": count['data']['Aggregate'][class_name][0]['meta']['count'],\n",
    "                            \"status\": \"active\"\n",
    "                        }\n",
    "                    else:\n",
    "                        status_info = {\n",
    "                            \"status\": \"not_found\",\n",
    "                            \"error\": f\"No class found matching {class_name}\"\n",
    "                        }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    status_info = {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e)\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                status_info = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "        elif db_rag.vector_db == \"faiss\":\n",
    "            try:\n",
    "                # For FAISS, we'll need to load the index file\n",
    "                import faiss\n",
    "                \n",
    "                # Assuming indexes are saved with a specific naming convention\n",
    "                # index_path = f\"./faiss_indexes/rag_{db_rag.name.lower()}_{db_rag.id}.index\"\n",
    "                index_path = f\"./faiss_indexes/rag_{db_rag.name.lower()}_Temp.index\"\n",
    "                \n",
    "                try:\n",
    "                    # Load the index\n",
    "                    index = faiss.read_index(index_path)\n",
    "                    \n",
    "                    status_info = {\n",
    "                        \"collection_name\": f\"rag_{db_rag.name.lower()}_{db_rag.id}\",\n",
    "                        \"total_vectors\": index.ntotal,\n",
    "                        \"dimension\": index.d,\n",
    "                        \"status\": \"active\"\n",
    "                    }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    status_info = {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": str(e)\n",
    "                    }\n",
    "            \n",
    "            except Exception as e:\n",
    "                status_info = {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"db_info\": {\n",
    "                \"id\": db_rag.id,\n",
    "                \"name\": db_rag.name,\n",
    "                \"vector_db\": db_rag.vector_db,\n",
    "                \"created_at\": db_rag.created_at,\n",
    "                \"status\": db_rag.status\n",
    "            },\n",
    "            \"store_info\": status_info\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500, \n",
    "            detail=f\"Error testing vector store: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "@router.post(\"/test/{rag_id}/query\")\n",
    "async def test_rag_query(\n",
    "    rag_id: int,\n",
    "    query: str = Body(...),\n",
    "    top_k: int = Body(5),\n",
    "    db: Session = Depends(get_db)\n",
    "):\n",
    "    \"\"\"Test RAG by running a query against the vector store\"\"\"\n",
    "    try:\n",
    "        # Get RAG database info\n",
    "        db_rag = db.query(RAGDatabase).filter(RAGDatabase.id == rag_id).first()\n",
    "        if not db_rag:\n",
    "            raise HTTPException(\n",
    "                status_code=404,\n",
    "                detail=\"RAG database not found\"\n",
    "            )\n",
    "\n",
    "        # Embedding model for query conversion\n",
    "        embedding_model = SentenceTransformer(db_rag.embedding_model)\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "\n",
    "        # Documents to store results\n",
    "        documents = []\n",
    "\n",
    "        if db_rag.vector_db == \"chromadb\":\n",
    "            try:\n",
    "                # Initialize ChromaDB client\n",
    "                client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "                \n",
    "                # Generate collection name\n",
    "                # collection_name = f\"rag_{db_rag.name.lower()}_{db_rag.id}\"\n",
    "                collection_name = f\"rag_{db_rag.name.lower()}_Temp\"\n",
    "                \n",
    "                # Get the collection\n",
    "                collection = client.get_collection(name=collection_name)\n",
    "                \n",
    "                # Query the collection\n",
    "                results = collection.query(\n",
    "                    query_embeddings=[query_embedding],\n",
    "                    n_results=top_k\n",
    "                )\n",
    "                \n",
    "                # Extract documents\n",
    "                documents = results.get('documents', [[]])[0]\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(\n",
    "                    status_code=500,\n",
    "                    detail=f\"ChromaDB query error: {str(e)}\"\n",
    "                )\n",
    "\n",
    "        elif db_rag.vector_db == \"pinecone\":\n",
    "            try:\n",
    "                # Initialize Pinecone client\n",
    "                pc = Pinecone(api_key=\"pcsk_2sqpR7_FY6XeaGrqY1NikHysefnoj37anCK9fWMZ5rrxPzW3HU5xWUPVgJZSep9sYpdsCw\")\n",
    "                # index_name = f\"rag-{db_rag.name.lower()}-{db_rag.id}\"\n",
    "                index_name = f\"rag-{db_rag.name.lower()}-temp\"\n",
    "                \n",
    "                # Get the index\n",
    "                index = pc.Index(index_name)\n",
    "                \n",
    "                # Query Pinecone index\n",
    "                results = index.query(\n",
    "                    vector=query_embedding,\n",
    "                    top_k=top_k,\n",
    "                    include_metadata=True\n",
    "                )\n",
    "                \n",
    "                # Extract documents from metadata\n",
    "                documents = [\n",
    "                    match.get('metadata', {}).get('content', '') \n",
    "                    for match in results.get('matches', [])\n",
    "                ]\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(\n",
    "                    status_code=500,\n",
    "                    detail=f\"Pinecone query error: {str(e)}\"\n",
    "                )\n",
    "\n",
    "        elif db_rag.vector_db == \"weaviate\":\n",
    "            try:\n",
    "                # Initialize Weaviate client\n",
    "                client = weaviate.Client(url=\"http://localhost:8080\")\n",
    "                \n",
    "                # Generate class name\n",
    "                # class_name = f\"RAG_{db_rag.name.replace(' ', '_').lower()}_{db_rag.id}\"\n",
    "                class_name = f\"RAG_{db_rag.name.replace(' ', '_').lower()}_Temp\"\n",
    "                \n",
    "                # Perform vector search\n",
    "                results = (\n",
    "                    client.query.get(class_name, [\"content\"])\n",
    "                    .with_near_vector({\n",
    "                        \"vector\": query_embedding\n",
    "                    })\n",
    "                    .with_limit(top_k)\n",
    "                    .do()\n",
    "                )\n",
    "                \n",
    "                # Extract documents\n",
    "                documents = [\n",
    "                    item.get('content', '') \n",
    "                    for item in results.get('data', {}).get('Get', {}).get(class_name, [])\n",
    "                ]\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(\n",
    "                    status_code=500,\n",
    "                    detail=f\"Weaviate query error: {str(e)}\"\n",
    "                )\n",
    "\n",
    "        elif db_rag.vector_db == \"faiss\":\n",
    "            try:\n",
    "                # Import necessary libraries\n",
    "                import faiss\n",
    "                import pickle\n",
    "                \n",
    "                # Paths for FAISS index and documents\n",
    "               \n",
    "               \n",
    "                index_path = f\"./faiss_indexes/rag_{db_rag.name.lower()}_Temp.index\"\n",
    "                documents_path = f\"./faiss_indexes/rag_{db_rag.name.lower()}_Temp_docs.pkl\"\n",
    "                \n",
    "                # Load FAISS index and stored documents\n",
    "                index = faiss.read_index(index_path)\n",
    "                with open(documents_path, 'rb') as f:\n",
    "                    stored_documents = pickle.load(f)\n",
    "                \n",
    "                # Search in FAISS index\n",
    "                D, I = index.search(\n",
    "                    np.array(query_embedding).reshape(1, -1).astype('float32'), \n",
    "                    top_k\n",
    "                )\n",
    "                \n",
    "                # Extract documents\n",
    "                documents = [stored_documents[i] for i in I[0]]\n",
    "            \n",
    "            except Exception as e:\n",
    "                raise HTTPException(\n",
    "                    status_code=500,\n",
    "                    detail=f\"FAISS query error: {str(e)}\"\n",
    "                )\n",
    "\n",
    "        # If no documents found\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"query\": query,\n",
    "                \"response\": \"No relevant documents found\",\n",
    "                \"similar_documents\": [],\n",
    "                \"metadata\": {\n",
    "                    \"vector_db\": db_rag.vector_db,\n",
    "                    \"model\": db_rag.llm_model,\n",
    "                    \"embedding_model\": db_rag.embedding_model\n",
    "                }\n",
    "            }\n",
    "\n",
    "        # Generate response using LLM\n",
    "        context = \"\\n\".join(documents)\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(db_rag.llm_model)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(db_rag.llm_model)\n",
    "        generator = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        response = generator(prompt)[0][\"generated_text\"]\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"similar_documents\": documents,\n",
    "            \"metadata\": {\n",
    "                \"vector_db\": db_rag.vector_db,\n",
    "                \"model\": db_rag.llm_model,\n",
    "                \"embedding_model\": db_rag.embedding_model,\n",
    "                \"total_chunks_found\": len(documents)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error testing query: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RAGLoadRequest(BaseModel):\n",
    "    rag_name: str\n",
    "\n",
    "@router.post(\"/test/load\")\n",
    "async def load_rag_database(request: RAGLoadRequest, db: Session = Depends(get_db)):\n",
    "    \"\"\"Load a RAG database for use with the chatbot\"\"\"\n",
    "    try:\n",
    "        # Find the RAG database by name\n",
    "        db_rag = db.query(RAGDatabase).filter(RAGDatabase.name == request.rag_name).first()\n",
    "        if not db_rag:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"RAG database '{request.rag_name}' not found\"\n",
    "            }\n",
    "            \n",
    "        # Initialize RAG Pipeline\n",
    "        pipeline = RAGPipeline(\n",
    "            rag_type=db_rag.rag_type,\n",
    "            llm_model=db_rag.llm_model,\n",
    "            embedding_model=db_rag.embedding_model,\n",
    "            chunking_option=db_rag.chunking_option,\n",
    "            vector_db=db_rag.vector_db,\n",
    "            search_option=db_rag.search_option,\n",
    "            database_name=db_rag.name,\n",
    "            database_id=str(db_rag.id),\n",
    "            load_only=True\n",
    "        )\n",
    "        \n",
    "        # Load the vector store\n",
    "        success = pipeline.load_vector_store()\n",
    "        \n",
    "       \n",
    "        # Update database status\n",
    "        db_rag.status = \"Loaded\"\n",
    "        db.commit()\n",
    "            \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": f\"RAG database '{request.rag_name}' loaded successfully\",\n",
    "            \"database_info\": {\n",
    "                \"id\": db_rag.id,\n",
    "                \"name\": db_rag.name,\n",
    "                \"vector_db\": db_rag.vector_db,\n",
    "                \"status\": db_rag.status\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Error loading RAG database: {str(e)}\"\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
