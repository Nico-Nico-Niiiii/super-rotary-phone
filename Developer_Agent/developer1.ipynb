{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 13:44:23,113 - INFO - Using GPU: NVIDIA GeForce RTX 4090 with 23.61GB VRAM\n",
      "2025-02-22 13:44:23,113 - INFO - Initializing Llama pipeline with 8-bit quantization...\n",
      "2025-02-22 13:44:23,113 - INFO - Loading model meta-llama/Llama-3.1-8B with 8-bit quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "2025-02-22 13:44:23,734 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:34<00:00, 38.67s/it]\n",
      "Device set to use cuda:0\n",
      "2025-02-22 13:46:59,650 - INFO - GPU Memory Allocated: 8665.77MB\n",
      "2025-02-22 13:46:59,651 - INFO - GPU Memory Reserved: 8718.00MB\n",
      "/tmp/ipykernel_146171/1302597178.py:58: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(\n",
      "2025-02-22 13:46:59,651 - INFO - Model loaded successfully with 8-bit quantization\n",
      "2025-02-22 13:46:59,651 - INFO - Initializing code generator...\n",
      "2025-02-22 13:46:59,652 - INFO - Starting code generation process...\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2025-02-22 13:47:00,540 - ERROR - Error in model generation: 1 validation error for ChatGenerationChunk\n",
      "message\n",
      "  Input should be a valid dictionary or instance of BaseMessageChunk [type=model_type, input_value=AIMessage(content='System...}, response_metadata={}), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_146171/1302597178.py\", line 105, in _generate\n",
      "    generation = ChatGenerationChunk(\n",
      "        message=AIMessage(content=response_text),\n",
      "        generation_info={\"finish_reason\": \"stop\"}\n",
      "    )\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/pydantic/main.py\", line 214, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatGenerationChunk\n",
      "message\n",
      "  Input should be a valid dictionary or instance of BaseMessageChunk [type=model_type, input_value=AIMessage(content='System...}, response_metadata={}), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n",
      "2025-02-22 13:47:00,542 - ERROR - Error generating code: 1 validation error for ChatGenerationChunk\n",
      "message\n",
      "  Input should be a valid dictionary or instance of BaseMessageChunk [type=model_type, input_value=AIMessage(content='Error ...}, response_metadata={}), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_146171/1302597178.py\", line 262, in developer\n",
      "    response = self.model._generate(messages)\n",
      "  File \"/tmp/ipykernel_146171/1302597178.py\", line 117, in _generate\n",
      "    ChatGenerationChunk(\n",
      "    ~~~~~~~~~~~~~~~~~~~^\n",
      "        message=AIMessage(content=\"Error generating response\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        generation_info={\"finish_reason\": \"error\"}\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/pydantic/main.py\", line 214, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for ChatGenerationChunk\n",
      "message\n",
      "  Input should be a valid dictionary or instance of BaseMessageChunk [type=model_type, input_value=AIMessage(content='Error ...}, response_metadata={}), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/model_type\n",
      "2025-02-22 13:47:00,544 - ERROR - Error in code generation: 'attempt_number'\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_146171/1302597178.py\", line 494, in generate_code\n",
      "    f.write(f\"\\nAttempt {event['attempt_number']}:\\n\")\n",
      "                         ~~~~~^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'attempt_number'\n",
      "2025-02-22 13:47:00,544 - ERROR - Failed to generate valid code\n",
      "2025-02-22 13:47:00,545 - ERROR - Error: 'attempt_number'\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, List, Dict, Any\n",
    "import operator\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from datetime import datetime\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import transformers\n",
    "import torch\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.outputs import ChatGenerationChunk, ChatResult\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_llama_pipeline(model_id: str = \"meta-llama/Llama-3.1-8B\"):\n",
    "    \"\"\"Create a HuggingFace pipeline for Llama 3.1 with 8-bit quantization\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading model {model_id} with 8-bit quantization\")\n",
    "        \n",
    "        # Load model with 8-bit quantization\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            load_in_8bit=True,  # Enable 8-bit quantization\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Create the pipeline\n",
    "        pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Log memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "            logger.info(f\"GPU Memory Allocated: {memory_allocated:.2f}MB\")\n",
    "            logger.info(f\"GPU Memory Reserved: {memory_reserved:.2f}MB\")\n",
    "        \n",
    "        # Wrap pipeline in LangChain's HuggingFacePipeline\n",
    "        llm = HuggingFacePipeline(\n",
    "            pipeline=pipeline,\n",
    "            model_kwargs={\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_length\": 4096,\n",
    "                \"top_p\": 0.95,\n",
    "                \"repetition_penalty\": 1.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Model loaded successfully with 8-bit quantization\")\n",
    "        return llm\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "class TransformersChatModel(BaseChatModel):\n",
    "    \"\"\"Wrapper class to make HuggingFacePipeline work with chat interface\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_model):\n",
    "        \"\"\"Initialize with a HuggingFacePipeline.\"\"\"\n",
    "        super().__init__()\n",
    "        self._llm_model = llm_model\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"transformers_chat_model\"\n",
    "\n",
    "    def _generate(self, messages, stop=None, run_manager=None, **kwargs):\n",
    "        \"\"\"Generate response from the model\"\"\"\n",
    "        try:\n",
    "            # Convert chat messages to prompt string\n",
    "            prompt = \"\"\n",
    "            for message in messages:\n",
    "                if isinstance(message, SystemMessage):\n",
    "                    prompt += f\"System: {message.content}\\n\\n\"\n",
    "                elif isinstance(message, HumanMessage):\n",
    "                    prompt += f\"Human: {message.content}\\n\\n\"\n",
    "                else:\n",
    "                    prompt += f\"{message.content}\\n\\n\"\n",
    "            \n",
    "            # Get response from model\n",
    "            response_text = self._llm_model.invoke(prompt, stop=stop)\n",
    "            \n",
    "            # Create generation chunks\n",
    "            generation = ChatGenerationChunk(\n",
    "                message=AIMessage(content=response_text),\n",
    "                generation_info={\"finish_reason\": \"stop\"}\n",
    "            )\n",
    "            \n",
    "            # Return a ChatResult\n",
    "            return ChatResult(generations=[generation])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model generation: {str(e)}\", exc_info=True)\n",
    "            # Return empty response in case of error\n",
    "            return ChatResult(generations=[\n",
    "                ChatGenerationChunk(\n",
    "                    message=AIMessage(content=\"Error generating response\"),\n",
    "                    generation_info={\"finish_reason\": \"error\"}\n",
    "                )\n",
    "            ])\n",
    "\n",
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: Optional[str]\n",
    "    validation_status: Optional[bool]\n",
    "    error_messages: Optional[list[str]]\n",
    "    attempt_number: int\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model: TransformersChatModel, checkpointer, base_output_dir: str):\n",
    "        self.model = model\n",
    "        self.base_output_dir = base_output_dir\n",
    "        self.current_attempt = 0\n",
    "        self.max_attempts = 15\n",
    "        self.max_tokens = 4000\n",
    "        \n",
    "        # Create timestamped output directory\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.output_dir = os.path.join(base_output_dir, f\"generation_{timestamp}\")\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Set up logging for this generation\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Initialize graph\n",
    "        self.init_graph(checkpointer)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Set up logging for this generation instance\"\"\"\n",
    "        self.log_file = os.path.join(self.output_dir, \"generation.log\")\n",
    "        file_handler = logging.FileHandler(self.log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s'\n",
    "        ))\n",
    "        logger.addHandler(file_handler)\n",
    "        \n",
    "    def init_graph(self, checkpointer):\n",
    "        \"\"\"Initialize the state graph\"\"\"\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\",\n",
    "            lambda state: self.validator(state)[\"validation_status\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge(\"correction\", \"validator\")\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def save_code_attempt(self, code: str, attempt_number: int, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt and return directory path\"\"\"\n",
    "        attempt_dir = os.path.join(self.output_dir, f\"attempt_{attempt_number}_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        # Save code\n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_file = os.path.join(attempt_dir, \"summary.txt\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Attempt: {attempt_number}\\n\")\n",
    "            f.write(f\"Status: {status}\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        \n",
    "        logger.info(f\"Saved code attempt {attempt_number} to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def save_validation_results(self, validation_result: Dict[str, Any], attempt_number: int) -> None:\n",
    "        \"\"\"Save validation results\"\"\"\n",
    "        attempt_dir = os.path.join(self.output_dir, f\"attempt_{attempt_number}_validation\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        # Save JSON results\n",
    "        validation_file = os.path.join(attempt_dir, \"validation.json\")\n",
    "        with open(validation_file, 'w') as f:\n",
    "            json.dump(validation_result, f, indent=2)\n",
    "        \n",
    "        # Save human-readable summary\n",
    "        summary_file = os.path.join(attempt_dir, \"validation_summary.txt\")\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"Validation Results for Attempt {attempt_number}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Valid: {validation_result['valid']}\\n\\n\")\n",
    "            if validation_result.get('errors'):\n",
    "                f.write(\"Errors Found:\\n\")\n",
    "                for error in validation_result['errors']:\n",
    "                    f.write(f\"- [{error['severity']}] {error['description']}\\n\")\n",
    "        \n",
    "        logger.info(f\"Saved validation results to {validation_file}\")\n",
    "\n",
    "    def developer(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Generate initial code or process corrections\"\"\"\n",
    "        try:\n",
    "            messages = state['messages']\n",
    "            requirements = json.loads(messages[0].content)\n",
    "            \n",
    "            developer_prompt = f\"\"\"\n",
    "Role: Python Developer for Image Processing System\n",
    "Task: Generate complete Python code for:\n",
    "\n",
    "Title: {requirements.get('Title')}\n",
    "Description: {requirements.get('Description')}\n",
    "\n",
    "Requirements:\n",
    "{chr(10).join(f'- {criterion}' for criterion in requirements.get('AcceptanceCriteria', []))}\n",
    "\n",
    "Include:\n",
    "1. All necessary imports (numpy, cv2, etc.)\n",
    "2. Complete class implementation with:\n",
    "   - Configuration management (dataclass)\n",
    "   - Image processing methods\n",
    "   - Error handling\n",
    "   - Type hints\n",
    "   - Logging\n",
    "3. Example usage\n",
    "\n",
    "Return ONLY the complete Python code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [SystemMessage(content=developer_prompt)]\n",
    "            \n",
    "            # Generate code\n",
    "            try:\n",
    "                response = self.model._generate(messages)\n",
    "                if response and response.generations:\n",
    "                    generated_code = response.generations[0].message.content\n",
    "                else:\n",
    "                    raise ValueError(\"No response generated\")\n",
    "                \n",
    "                # Save code\n",
    "                self.current_attempt += 1\n",
    "                self.save_code_attempt(generated_code, self.current_attempt)\n",
    "                \n",
    "                return {\n",
    "                    'messages': messages,\n",
    "                    'current_code': generated_code,\n",
    "                    'validation_status': None,\n",
    "                    'error_messages': [],\n",
    "                    'attempt_number': self.current_attempt\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating code: {str(e)}\", exc_info=True)\n",
    "                return {\n",
    "                    'messages': messages,\n",
    "                    'error_messages': [str(e)],\n",
    "                    'attempt_number': self.current_attempt\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in developer node: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                'messages': messages if 'messages' in locals() else [],\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        try:\n",
    "            current_code = state.get(\"current_code\", \"\")\n",
    "            \n",
    "            validator_prompt = \"\"\"\n",
    "Role: Code Reviewer\n",
    "Task: Validate the provided Python code.\n",
    "\n",
    "Check:\n",
    "1. All imports and classes present\n",
    "2. Proper error handling\n",
    "3. Type hints and docstrings\n",
    "4. Functionality implementation\n",
    "5. Code quality\n",
    "\n",
    "Return validation result as JSON:\n",
    "{\n",
    "    \"valid\": boolean,\n",
    "    \"errors\": [\n",
    "        {\n",
    "            \"description\": \"Issue description\",\n",
    "            \"severity\": \"high|medium|low\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=validator_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                # Get validation feedback\n",
    "                response = self.model._generate(messages)\n",
    "                if response and response.generations:\n",
    "                    validation_text = response.generations[0].message.content\n",
    "                    validation_result = json.loads(validation_text)\n",
    "                else:\n",
    "                    raise ValueError(\"No validation response generated\")\n",
    "                \n",
    "                # Save results\n",
    "                self.save_validation_results(validation_result, state['attempt_number'])\n",
    "                self.save_code_attempt(\n",
    "                    current_code, \n",
    "                    state['attempt_number'],\n",
    "                    f\"validated_{'pass' if validation_result['valid'] else 'fail'}\"\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    'messages': messages,\n",
    "                    'current_code': current_code,\n",
    "                    'validation_status': validation_result['valid'],\n",
    "                    'error_messages': [e['description'] for e in validation_result.get('errors', [])],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in validation: {str(e)}\", exc_info=True)\n",
    "                return {\n",
    "                    'messages': messages if 'messages' in locals() else [],\n",
    "                    'validation_status': False,\n",
    "                    'error_messages': [str(e)],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in validator node: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'validation_status': False,\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        try:\n",
    "            if self.current_attempt >= self.max_attempts:\n",
    "                logger.error(\"Maximum correction attempts reached\")\n",
    "                return {\n",
    "                    'messages': [],\n",
    "                    'validation_status': False,\n",
    "                    'error_messages': [\"Maximum correction attempts reached\"],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "            \n",
    "            error_messages = state.get('error_messages', [])\n",
    "            current_code = state.get('current_code', '')\n",
    "            \n",
    "            correction_prompt = f\"\"\"\n",
    "Role: Python Developer\n",
    "Task: Fix the code. Issues found:\n",
    "\n",
    "{chr(10).join(f'- {error}' for error in error_messages)}\n",
    "\n",
    "Important:\n",
    "1. Keep existing structure\n",
    "2. Fix all issues\n",
    "3. Ensure code is complete\n",
    "4. Add proper documentation\n",
    "\n",
    "Return the complete corrected code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=correction_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                # Generate corrected code\n",
    "                response = self.model._generate(messages)\n",
    "                if response and response.generations:\n",
    "                    corrected_code = response.generations[0].message.content\n",
    "                else:\n",
    "                    raise ValueError(\"No correction response generated\")\n",
    "                \n",
    "                # Save correction attempt\n",
    "                self.current_attempt += 1\n",
    "                self.save_code_attempt(corrected_code, self.current_attempt, \"correction\")\n",
    "                \n",
    "                return {\n",
    "                    'messages': messages,\n",
    "                    'current_code': corrected_code,\n",
    "                    'validation_status': None,\n",
    "                    'error_messages': [],\n",
    "                    'attempt_number': self.current_attempt\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in correction: {str(e)}\", exc_info=True)\n",
    "                return {\n",
    "                    'messages': messages if 'messages' in locals() else [],\n",
    "                    'error_messages': [str(e)],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in correction node: {str(e)}\", exc_info=True)\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "def generate_code(requirements: Dict[str, Any], output_dir: str = \"code_generation\") -> Dict[str, Any]:\n",
    "    \"\"\"Main function to generate code based on requirements\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Check CUDA availability and print GPU info\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            logger.info(f\"Using GPU: {gpu_name} with {gpu_memory:.2f}GB VRAM\")\n",
    "        else:\n",
    "            logger.warning(\"No GPU detected - using CPU only\")\n",
    "        \n",
    "        # Initialize the Llama pipeline with 8-bit quantization\n",
    "        logger.info(\"Initializing Llama pipeline with 8-bit quantization...\")\n",
    "        llm = create_llama_pipeline()\n",
    "        model = TransformersChatModel(llm_model=llm)\n",
    "        \n",
    "        # Create a results summary file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_dir = os.path.join(output_dir, f\"run_{timestamp}\")\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        summary_file = os.path.join(results_dir, \"generation_summary.txt\")\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"Code Generation Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(\"Requirements:\\n\")\n",
    "            f.write(json.dumps(requirements, indent=2) + \"\\n\\n\")\n",
    "        \n",
    "        # Initialize code generator\n",
    "        with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "            logger.info(\"Initializing code generator...\")\n",
    "            generator = CodeGenerator(model, checkpointer, results_dir)\n",
    "            \n",
    "            # Create initial message with requirements\n",
    "            user_message = HumanMessage(content=json.dumps(requirements))\n",
    "            \n",
    "            # Generate and validate code\n",
    "            thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "            final_result = None\n",
    "            \n",
    "            logger.info(\"Starting code generation process...\")\n",
    "            for event in generator.graph.stream({\"messages\": [user_message]}, thread):\n",
    "                final_result = event\n",
    "                \n",
    "                # Update summary file\n",
    "                with open(summary_file, 'a') as f:\n",
    "                    f.write(f\"\\nAttempt {event['attempt_number']}:\\n\")\n",
    "                    if event.get('error_messages'):\n",
    "                        f.write(\"Issues found:\\n\")\n",
    "                        for error in event['error_messages']:\n",
    "                            f.write(f\"- {error}\\n\")\n",
    "                    \n",
    "                    if event.get('validation_status') == True:\n",
    "                        f.write(\"\\nSuccessfully generated valid code!\\n\")\n",
    "                \n",
    "                if event.get('validation_status') == True:\n",
    "                    logger.info(f\"Successfully generated valid code on attempt {event['attempt_number']}\")\n",
    "                    \n",
    "                    # Save final code to root of results directory\n",
    "                    final_code_path = os.path.join(results_dir, \"final_code.py\")\n",
    "                    with open(final_code_path, 'w') as f:\n",
    "                        f.write(event['current_code'])\n",
    "                    \n",
    "                    logger.info(f\"Final code saved to: {final_code_path}\")\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                'success': final_result.get('validation_status', False),\n",
    "                'attempts': final_result.get('attempt_number', 0),\n",
    "                'output_directory': results_dir,\n",
    "                'final_code_path': final_code_path if final_result.get('validation_status') else None\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in code generation: {str(e)}\", exc_info=True)\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'output_directory': results_dir if 'results_dir' in locals() else None\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example requirements\n",
    "    requirements = {\n",
    "        \"Title\": \"Image Preprocessing as per Model\",\n",
    "        \"Description\": \"As a user, I want the system to perform image preprocessing (standardizing spatial and bit resolution through resizing and normalization) so that input images are ready for AI model training or inferencing (AI prediction).\",\n",
    "        \"AcceptanceCriteria\": [\n",
    "            \"Pre-processing should successfully be implemented on images with the following bit representations: 1-bit, 8-bit,16-bit, 24-bit, 32-bit.\",\n",
    "            \"The pre-processing system shall be able to resize the input images into the resolution required by the AI model.\",\n",
    "            \"The pre-processing system shall be able to normalize the input image data into appropriate pixel datatype required by the AI model.\",\n",
    "            \"When pre-processing is successful for the training dataset, the data shall be sent further for batch creation.\",\n",
    "            \"When pre-processing is successful for inferencing (prediction) dataset, the data shall be sent for model inferencing.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate code\n",
    "    result = generate_code(requirements)\n",
    "    \n",
    "    # Print final results\n",
    "    if result['success']:\n",
    "        logger.info(\"Code generation successful!\")\n",
    "        logger.info(f\"Output directory: {result['output_directory']}\")\n",
    "        logger.info(f\"Final code: {result['final_code_path']}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to generate valid code\")\n",
    "        if 'error' in result:\n",
    "            logger.error(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, List, Dict, Any\n",
    "import operator\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from datetime import datetime\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: Optional[str]\n",
    "    validation_status: Optional[bool]\n",
    "    error_messages: Optional[list[str]]\n",
    "    attempt_number: int\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model, checkpointer, base_output_dir: str):\n",
    "        self.model = model\n",
    "        self.base_output_dir = base_output_dir\n",
    "        self.current_attempt = 0\n",
    "        self.max_attempts = 15\n",
    "        self.max_tokens = 4000  # Set max tokens for safety\n",
    "        \n",
    "        # Create base output directory with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.base_output_dir = os.path.join(base_output_dir, f\"generation_{timestamp}\")\n",
    "        os.makedirs(self.base_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\",\n",
    "            lambda state: self.validator(state)[\"validation_status\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge(\"correction\", \"validator\")\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def save_code_attempt(self, code: str, attempt_number: int, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt to file and return the directory path\"\"\"\n",
    "        attempt_dir = os.path.join(self.base_output_dir, f\"attempt_{attempt_number}_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved code attempt {attempt_number} to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def save_validation_results(self, validation_result: Dict[str, Any], attempt_number: int) -> None:\n",
    "        \"\"\"Save validation results to file\"\"\"\n",
    "        validation_file = os.path.join(\n",
    "            self.base_output_dir, \n",
    "            f\"attempt_{attempt_number}_validation.json\"\n",
    "        )\n",
    "        with open(validation_file, 'w') as f:\n",
    "            json.dump(validation_result, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved validation results to {validation_file}\")\n",
    "\n",
    "    def developer(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Generate initial code or process corrections\"\"\"\n",
    "        try:\n",
    "            messages = state['messages']\n",
    "            requirements = json.loads(messages[0].content)\n",
    "            \n",
    "            developer_prompt = f\"\"\"\n",
    "Role: Python Developer for Image Processing System\n",
    "Task: Generate complete Python code for:\n",
    "\n",
    "Title: {requirements.get('Title')}\n",
    "Description: {requirements.get('Description')}\n",
    "\n",
    "Requirements:\n",
    "{chr(10).join(f'- {criterion}' for criterion in requirements.get('AcceptanceCriteria', []))}\n",
    "\n",
    "Include:\n",
    "1. All necessary imports (numpy, cv2, etc.)\n",
    "2. Complete class implementation with:\n",
    "   - Configuration management (dataclass)\n",
    "   - Image processing methods\n",
    "   - Error handling\n",
    "   - Type hints\n",
    "   - Logging\n",
    "3. Example usage\n",
    "\n",
    "Return ONLY the complete Python code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [SystemMessage(content=developer_prompt)]\n",
    "            \n",
    "            # Generate code\n",
    "            response = self.model.invoke(messages)\n",
    "            generated_code = response.content\n",
    "            \n",
    "            # Save initial code\n",
    "            self.current_attempt += 1\n",
    "            self.save_code_attempt(generated_code, self.current_attempt)\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': generated_code,\n",
    "                'validation_status': None,\n",
    "                'error_messages': [],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in developer node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        try:\n",
    "            current_code = state.get(\"current_code\", \"\")\n",
    "            \n",
    "            validator_prompt = \"\"\"\n",
    "Role: Code Reviewer\n",
    "Task: Validate the provided Python code.\n",
    "\n",
    "Check:\n",
    "1. All imports and classes present\n",
    "2. Proper error handling\n",
    "3. Type hints and docstrings\n",
    "4. Functionality implementation\n",
    "5. Code quality\n",
    "\n",
    "Return validation result as JSON:\n",
    "{\n",
    "    \"valid\": boolean,\n",
    "    \"errors\": [\n",
    "        {\n",
    "            \"description\": \"Issue description\",\n",
    "            \"severity\": \"high|medium|low\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=validator_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            # Get validation feedback\n",
    "            response = self.model.invoke(messages)\n",
    "            validation_result = json.loads(response.content)\n",
    "            \n",
    "            # Save validation results\n",
    "            self.save_validation_results(validation_result, state['attempt_number'])\n",
    "            self.save_code_attempt(\n",
    "                current_code, \n",
    "                state['attempt_number'],\n",
    "                f\"validated_{'pass' if validation_result['valid'] else 'fail'}\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': current_code,\n",
    "                'validation_status': validation_result['valid'],\n",
    "                'error_messages': [e['description'] for e in validation_result.get('errors', [])],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in validator node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'validation_status': False,\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        try:\n",
    "            if self.current_attempt >= self.max_attempts:\n",
    "                logger.error(\"Maximum correction attempts reached\")\n",
    "                return {\n",
    "                    'messages': [],\n",
    "                    'validation_status': False,\n",
    "                    'error_messages': [\"Maximum correction attempts reached\"],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "            \n",
    "            error_messages = state.get('error_messages', [])\n",
    "            current_code = state.get('current_code', '')\n",
    "            \n",
    "            correction_prompt = f\"\"\"\n",
    "Role: Python Developer\n",
    "Task: Fix the code. Issues found:\n",
    "\n",
    "{chr(10).join(f'- {error}' for error in error_messages)}\n",
    "\n",
    "Important:\n",
    "1. Keep existing structure\n",
    "2. Fix all issues\n",
    "3. Ensure code is complete\n",
    "4. Add proper documentation\n",
    "\n",
    "Return the complete corrected code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=correction_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            # Generate corrected code\n",
    "            response = self.model.invoke(messages)\n",
    "            corrected_code = response.content\n",
    "            \n",
    "            # Save correction attempt\n",
    "            self.current_attempt += 1\n",
    "            self.save_code_attempt(corrected_code, self.current_attempt, \"correction\")\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': corrected_code,\n",
    "                'validation_status': None,\n",
    "                'error_messages': [],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in correction node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "def generate_code(requirements: Dict[str, Any], output_dir: str = \"code_generation\"):\n",
    "    \"\"\"Main function to generate code based on requirements\"\"\"\n",
    "    \n",
    "    # Initialize the model\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=4000,\n",
    "        do_sample=False,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    model = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize code generator\n",
    "    with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "        generator = CodeGenerator(model, checkpointer, output_dir)\n",
    "        \n",
    "        # Create initial message with requirements\n",
    "        user_message = HumanMessage(content=json.dumps(requirements))\n",
    "        \n",
    "        # Generate and validate code\n",
    "        thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        final_result = None\n",
    "        \n",
    "        for event in generator.graph.stream({\"messages\": [user_message]}, thread):\n",
    "            final_result = event\n",
    "            \n",
    "            # Log progress\n",
    "            if event.get('error_messages'):\n",
    "                logger.info(f\"Attempt {event['attempt_number']} - Issues found:\")\n",
    "                for error in event['error_messages']:\n",
    "                    logger.info(f\"- {error}\")\n",
    "            \n",
    "            if event.get('validation_status') == True:\n",
    "                logger.info(f\"Successfully generated valid code on attempt {event['attempt_number']}\")\n",
    "                break\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example requirements\n",
    "    requirements = {\n",
    "        \"Title\": \"Image Preprocessing as per Model\",\n",
    "        \"Description\": \"As a user, I want the system to perform image preprocessing (standardizing spatial and bit resolution through resizing and normalization) so that input images are ready for AI model training or inferencing (AI prediction).\",\n",
    "        \"AcceptanceCriteria\": [\n",
    "            \"Pre-processing should successfully be implemented on images with the following bit representations: 1-bit, 8-bit,16-bit, 24-bit, 32-bit.\",\n",
    "            \"The pre-processing system shall be able to resize the input images into the resolution required by the AI model.\",\n",
    "            \"The pre-processing system shall be able to normalize the input image data into appropriate pixel datatype required by the AI model.\",\n",
    "            \"When pre-processing is successful for the training dataset, the data shall be sent further for batch creation.\",\n",
    "            \"When pre-processing is successful for inferencing (prediction) dataset, the data shall be sent for model inferencing.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate code\n",
    "    result = generate_code(requirements)\n",
    "    \n",
    "    if result and result.get('validation_status'):\n",
    "        logger.info(\"Code generation successful!\")\n",
    "        logger.info(f\"Check the generated code in the output directory\")\n",
    "    else:\n",
    "        logger.error(\"Failed to generate valid code after maximum attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airangers/anaconda3/envs/e/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-02-22 13:40:59,313 - INFO - Saved code attempt 1 to code_generation/generation_20250222_134057/attempt_1_initial/code.py\n",
      "2025-02-22 13:41:03,730 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:03,965 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:06,687 - INFO - Saved code attempt 2 to code_generation/generation_20250222_134057/attempt_2_correction/code.py\n",
      "2025-02-22 13:41:07,040 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:07,274 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:07,580 - INFO - Saved code attempt 3 to code_generation/generation_20250222_134057/attempt_3_correction/code.py\n",
      "2025-02-22 13:41:07,959 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:08,194 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:08,514 - INFO - Saved code attempt 4 to code_generation/generation_20250222_134057/attempt_4_correction/code.py\n",
      "2025-02-22 13:41:08,840 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:09,077 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:09,401 - INFO - Saved code attempt 5 to code_generation/generation_20250222_134057/attempt_5_correction/code.py\n",
      "2025-02-22 13:41:09,733 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:09,968 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:10,290 - INFO - Saved code attempt 6 to code_generation/generation_20250222_134057/attempt_6_correction/code.py\n",
      "2025-02-22 13:41:10,668 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:10,904 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:11,224 - INFO - Saved code attempt 7 to code_generation/generation_20250222_134057/attempt_7_correction/code.py\n",
      "2025-02-22 13:41:11,675 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:11,913 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:12,258 - INFO - Saved code attempt 8 to code_generation/generation_20250222_134057/attempt_8_correction/code.py\n",
      "2025-02-22 13:41:12,585 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:12,822 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:13,117 - INFO - Saved code attempt 9 to code_generation/generation_20250222_134057/attempt_9_correction/code.py\n",
      "2025-02-22 13:41:13,353 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:13,588 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:13,827 - INFO - Saved code attempt 10 to code_generation/generation_20250222_134057/attempt_10_correction/code.py\n",
      "2025-02-22 13:41:14,063 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:14,297 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:14,537 - INFO - Saved code attempt 11 to code_generation/generation_20250222_134057/attempt_11_correction/code.py\n",
      "2025-02-22 13:41:14,774 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:15,011 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:15,250 - INFO - Saved code attempt 12 to code_generation/generation_20250222_134057/attempt_12_correction/code.py\n",
      "2025-02-22 13:41:15,487 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:15,722 - ERROR - Error in validator node: Expecting value: line 1 column 1 (char 0)\n",
      "2025-02-22 13:41:15,960 - INFO - Saved code attempt 13 to code_generation/generation_20250222_134057/attempt_13_correction/code.py\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphRecursionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 324\u001b[0m\n\u001b[1;32m    311\u001b[0m requirements \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage Preprocessing as per Model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs a user, I want the system to perform image preprocessing (standardizing spatial and bit resolution through resizing and normalization) so that input images are ready for AI model training or inferencing (AI prediction).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     ]\n\u001b[1;32m    321\u001b[0m }\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Generate code\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_status\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    327\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode generation successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 294\u001b[0m, in \u001b[0;36mgenerate_code\u001b[0;34m(requirements, output_dir)\u001b[0m\n\u001b[1;32m    291\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[1;32m    292\u001b[0m final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Log progress\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/e/lib/python3.13/site-packages/langgraph/pregel/__init__.py:1680\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_of_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1672\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_error_message(\n\u001b[1;32m   1673\u001b[0m         message\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1674\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mGRAPH_RECURSION_LIMIT,\n\u001b[1;32m   1679\u001b[0m     )\n\u001b[0;32m-> 1680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(loop\u001b[38;5;241m.\u001b[39moutput)\n",
      "\u001b[0;31mGraphRecursionError\u001b[0m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, List, Dict, Any\n",
    "import operator\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from datetime import datetime\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CodeGenerationState(TypedDict):\n",
    "    \"\"\"State management for code generation process\"\"\"\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    current_code: Optional[str]\n",
    "    validation_status: Optional[bool]\n",
    "    error_messages: Optional[list[str]]\n",
    "    attempt_number: int\n",
    "\n",
    "class CodeGenerator:\n",
    "    \"\"\"Main class for generating, validating, and correcting code\"\"\"\n",
    "    \n",
    "    def __init__(self, model, checkpointer, base_output_dir: str):\n",
    "        self.model = model\n",
    "        self.base_output_dir = base_output_dir\n",
    "        self.current_attempt = 0\n",
    "        self.max_attempts = 15\n",
    "        self.max_tokens = 4000  # Set max tokens for safety\n",
    "        \n",
    "        # Create base output directory with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.base_output_dir = os.path.join(base_output_dir, f\"generation_{timestamp}\")\n",
    "        os.makedirs(self.base_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize graph\n",
    "        graph = StateGraph(CodeGenerationState)\n",
    "        \n",
    "        # Add nodes\n",
    "        graph.add_node(\"developer\", self.developer)\n",
    "        graph.add_node(\"validator\", self.validator)\n",
    "        graph.add_node(\"correction\", self.correction)\n",
    "        \n",
    "        # Add edges\n",
    "        graph.add_edge(\"developer\", \"validator\")\n",
    "        graph.add_conditional_edges(\n",
    "            \"validator\",\n",
    "            lambda state: self.validator(state)[\"validation_status\"],\n",
    "            {\n",
    "                True: END,\n",
    "                False: \"correction\"\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge(\"correction\", \"validator\")\n",
    "        \n",
    "        # Set entry point\n",
    "        graph.set_entry_point(\"developer\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def save_code_attempt(self, code: str, attempt_number: int, status: str = \"initial\") -> str:\n",
    "        \"\"\"Save code attempt to file and return the directory path\"\"\"\n",
    "        attempt_dir = os.path.join(self.base_output_dir, f\"attempt_{attempt_number}_{status}\")\n",
    "        os.makedirs(attempt_dir, exist_ok=True)\n",
    "        \n",
    "        code_file = os.path.join(attempt_dir, \"code.py\")\n",
    "        with open(code_file, 'w') as f:\n",
    "            f.write(code)\n",
    "        \n",
    "        logger.info(f\"Saved code attempt {attempt_number} to {code_file}\")\n",
    "        return attempt_dir\n",
    "\n",
    "    def save_validation_results(self, validation_result: Dict[str, Any], attempt_number: int) -> None:\n",
    "        \"\"\"Save validation results to file\"\"\"\n",
    "        validation_file = os.path.join(\n",
    "            self.base_output_dir, \n",
    "            f\"attempt_{attempt_number}_validation.json\"\n",
    "        )\n",
    "        with open(validation_file, 'w') as f:\n",
    "            json.dump(validation_result, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved validation results to {validation_file}\")\n",
    "\n",
    "    def developer(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Generate initial code or process corrections\"\"\"\n",
    "        try:\n",
    "            messages = state['messages']\n",
    "            requirements = json.loads(messages[0].content)\n",
    "            \n",
    "            developer_prompt = f\"\"\"\n",
    "Role: Python Developer for Image Processing System\n",
    "Task: Generate complete Python code for:\n",
    "\n",
    "Title: {requirements.get('Title')}\n",
    "Description: {requirements.get('Description')}\n",
    "\n",
    "Requirements:\n",
    "{chr(10).join(f'- {criterion}' for criterion in requirements.get('AcceptanceCriteria', []))}\n",
    "\n",
    "Include:\n",
    "1. All necessary imports (numpy, cv2, etc.)\n",
    "2. Complete class implementation with:\n",
    "   - Configuration management (dataclass)\n",
    "   - Image processing methods\n",
    "   - Error handling\n",
    "   - Type hints\n",
    "   - Logging\n",
    "3. Example usage\n",
    "\n",
    "Return ONLY the complete Python code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [SystemMessage(content=developer_prompt)]\n",
    "            \n",
    "            # Generate code\n",
    "            response = self.model.invoke(messages)\n",
    "            generated_code = response.content\n",
    "            \n",
    "            # Save initial code\n",
    "            self.current_attempt += 1\n",
    "            self.save_code_attempt(generated_code, self.current_attempt)\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': generated_code,\n",
    "                'validation_status': None,\n",
    "                'error_messages': [],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in developer node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "\n",
    "    def validator(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Validate generated code\"\"\"\n",
    "        try:\n",
    "            current_code = state.get(\"current_code\", \"\")\n",
    "            \n",
    "            validator_prompt = \"\"\"\n",
    "Role: Code Reviewer\n",
    "Task: Validate the provided Python code.\n",
    "\n",
    "Check:\n",
    "1. All imports and classes present\n",
    "2. Proper error handling\n",
    "3. Type hints and docstrings\n",
    "4. Functionality implementation\n",
    "5. Code quality\n",
    "\n",
    "Return validation result as JSON:\n",
    "{\n",
    "    \"valid\": boolean,\n",
    "    \"errors\": [\n",
    "        {\n",
    "            \"description\": \"Issue description\",\n",
    "            \"severity\": \"high|medium|low\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=validator_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            # Get validation feedback\n",
    "            response = self.model.invoke(messages)\n",
    "            validation_result = json.loads(response.content)\n",
    "            \n",
    "            # Save validation results\n",
    "            self.save_validation_results(validation_result, state['attempt_number'])\n",
    "            self.save_code_attempt(\n",
    "                current_code, \n",
    "                state['attempt_number'],\n",
    "                f\"validated_{'pass' if validation_result['valid'] else 'fail'}\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': current_code,\n",
    "                'validation_status': validation_result['valid'],\n",
    "                'error_messages': [e['description'] for e in validation_result.get('errors', [])],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in validator node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'validation_status': False,\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "    def correction(self, state: CodeGenerationState) -> Dict[str, Any]:\n",
    "        \"\"\"Correct code based on validation feedback\"\"\"\n",
    "        try:\n",
    "            if self.current_attempt >= self.max_attempts:\n",
    "                logger.error(\"Maximum correction attempts reached\")\n",
    "                return {\n",
    "                    'messages': [],\n",
    "                    'validation_status': False,\n",
    "                    'error_messages': [\"Maximum correction attempts reached\"],\n",
    "                    'attempt_number': state['attempt_number']\n",
    "                }\n",
    "            \n",
    "            error_messages = state.get('error_messages', [])\n",
    "            current_code = state.get('current_code', '')\n",
    "            \n",
    "            correction_prompt = f\"\"\"\n",
    "Role: Python Developer\n",
    "Task: Fix the code. Issues found:\n",
    "\n",
    "{chr(10).join(f'- {error}' for error in error_messages)}\n",
    "\n",
    "Important:\n",
    "1. Keep existing structure\n",
    "2. Fix all issues\n",
    "3. Ensure code is complete\n",
    "4. Add proper documentation\n",
    "\n",
    "Return the complete corrected code.\n",
    "\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                SystemMessage(content=correction_prompt),\n",
    "                HumanMessage(content=current_code)\n",
    "            ]\n",
    "            \n",
    "            # Generate corrected code\n",
    "            response = self.model.invoke(messages)\n",
    "            corrected_code = response.content\n",
    "            \n",
    "            # Save correction attempt\n",
    "            self.current_attempt += 1\n",
    "            self.save_code_attempt(corrected_code, self.current_attempt, \"correction\")\n",
    "            \n",
    "            return {\n",
    "                'messages': messages,\n",
    "                'current_code': corrected_code,\n",
    "                'validation_status': None,\n",
    "                'error_messages': [],\n",
    "                'attempt_number': self.current_attempt\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in correction node: {str(e)}\")\n",
    "            return {\n",
    "                'messages': [],\n",
    "                'error_messages': [str(e)],\n",
    "                'attempt_number': state['attempt_number']\n",
    "            }\n",
    "\n",
    "def generate_code(requirements: Dict[str, Any], output_dir: str = \"code_generation\"):\n",
    "    \"\"\"Main function to generate code based on requirements\"\"\"\n",
    "    \n",
    "    # Initialize the model\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=4000,\n",
    "        do_sample=False,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    model = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize code generator\n",
    "    with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "        generator = CodeGenerator(model, checkpointer, output_dir)\n",
    "        \n",
    "        # Create initial message with requirements\n",
    "        user_message = HumanMessage(content=json.dumps(requirements))\n",
    "        \n",
    "        # Generate and validate code\n",
    "        thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        final_result = None\n",
    "        \n",
    "        for event in generator.graph.stream({\"messages\": [user_message]}, thread):\n",
    "            final_result = event\n",
    "            \n",
    "            # Log progress\n",
    "            if event.get('error_messages'):\n",
    "                logger.info(f\"Attempt {event['attempt_number']} - Issues found:\")\n",
    "                for error in event['error_messages']:\n",
    "                    logger.info(f\"- {error}\")\n",
    "            \n",
    "            if event.get('validation_status') == True:\n",
    "                logger.info(f\"Successfully generated valid code on attempt {event['attempt_number']}\")\n",
    "                break\n",
    "        \n",
    "        return final_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example requirements\n",
    "    requirements = {\n",
    "        \"Title\": \"Image Preprocessing as per Model\",\n",
    "        \"Description\": \"As a user, I want the system to perform image preprocessing (standardizing spatial and bit resolution through resizing and normalization) so that input images are ready for AI model training or inferencing (AI prediction).\",\n",
    "        \"AcceptanceCriteria\": [\n",
    "            \"Pre-processing should successfully be implemented on images with the following bit representations: 1-bit, 8-bit,16-bit, 24-bit, 32-bit.\",\n",
    "            \"The pre-processing system shall be able to resize the input images into the resolution required by the AI model.\",\n",
    "            \"The pre-processing system shall be able to normalize the input image data into appropriate pixel datatype required by the AI model.\",\n",
    "            \"When pre-processing is successful for the training dataset, the data shall be sent further for batch creation.\",\n",
    "            \"When pre-processing is successful for inferencing (prediction) dataset, the data shall be sent for model inferencing.\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate code\n",
    "    result = generate_code(requirements)\n",
    "    \n",
    "    if result and result.get('validation_status'):\n",
    "        logger.info(\"Code generation successful!\")\n",
    "        logger.info(f\"Check the generated code in the output directory\")\n",
    "    else:\n",
    "        logger.error(\"Failed to generate valid code after maximum attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
