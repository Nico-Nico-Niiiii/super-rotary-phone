{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c22d54bd-de0c-4606-af63-d88e6c0b9312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, LongType, TimestampType, IntegerType, StructField\n",
    "import datetime\n",
    "import ast\n",
    "from pyspark.sql.functions import udf, to_timestamp, expr, lit, col, date_format, lower, to_date, current_timestamp, md5, concat_ws\n",
    "# param_json = dbutils.widgets.get(\"param_key\")\n",
    "# params = json.loads(param_json)\n",
    "\n",
    "#TODO:\n",
    "params = {\"param\": {\"campaign_name\": \"TestConfig\", \"no_of_execution\": \"1\"}, \"execution_id\": \"8b146246-db6d-482d-9689-aaff100d54db\"}\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Advertisement Campaign\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7e351353-6225-4f97-b5db-118ba398df54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AdvertisementCampaign:\n",
    "  def __init__(self, params):\n",
    "    '''\n",
    "    Description : Initializes the class by fetching configuration settings, network events, and PRIZM segmentation data from the Unity Catalog, used by the instances of the class for further operations.\n",
    "    Parameters : \n",
    "    - params(dictionary containing configuration values from Advertisement Campaign parent notebook).\n",
    "    Return value : None\n",
    "    '''\n",
    "    parameters = params['param']\n",
    "    self.camp_name = parameters['campaign_name']\n",
    "    self.iteration = int(parameters['no_of_execution'])\n",
    "    self.execution_ids = params['execution_id']\n",
    "    self._catalog = \"edl_dev\"\n",
    "    self._drvd_schema = \"drvd__app_rsmgeo5g\"\n",
    "    self._rawstd_schema = \"rawstd__rsmgeo5g\"\n",
    "    self._environics_schema = \"rawstd__environics\"\n",
    "\n",
    "    self.ntb_start_time = datetime.datetime.now()\n",
    "\n",
    "    self.config = self.get_data_from_db(self._catalog, self._rawstd_schema, \n",
    "                                       \"ad_campaigns_config\", \n",
    "                                       f\"where campaign_name = '{self.camp_name}' AND status = 'N'\")\n",
    "    self.safegraph = self.get_data_from_db(self._catalog, self._drvd_schema, \n",
    "                                             \"safegraph_poi_h3\")\n",
    "    self.advertiser = self.get_data_from_db(self._catalog, self._drvd_schema, \n",
    "                                            \"advertiser_poi_h3\") \n",
    "    self.custom_geofence = self.get_data_from_db(self._catalog, self._drvd_schema,\n",
    "                                                 \"custom_geofence_h3\")\n",
    "    self.network_df = self.get_data_from_db(self._catalog, self._drvd_schema,\n",
    "                                            \"events_agg_bkp1\") \n",
    "    self.prizm_df = self.get_data_from_db(self._catalog, self._environics_schema,\n",
    "                                          \"prizm5_uniquelicenses_2023\")\n",
    "    self.ad_camp_id = self.get_data_from_db(self._catalog, self._rawstd_schema, \n",
    "                                            \"ad_campaigns_config\",\n",
    "                                            f\"WHERE campaign_name = '{self.camp_name}' AND status = 'N'\",\n",
    "                                            \"DISTINCT config_id\")\n",
    "    self.distinct_config_ids = [row['config_id'] for row in self.ad_camp_id.collect()]\n",
    "    self.campaign_id = self.distinct_config_ids[0]\n",
    "    # ignore columns while generating checksum column\n",
    "    self.ignore_column_list_md5=['_checksum', '_az_insert_ts', '_az_update_ts', \"exec_run_id\"]\n",
    "\n",
    "  def get_data_from_db(self, catalog, schema, table, where_clause=\"\", selection=\"*\"):\n",
    "    \"\"\"\n",
    "    Description: Retrieves data from a specified database table.\n",
    "    Parameters:\n",
    "    - catalog : The name of the catalog from which to retrieve the data.\n",
    "    - schema : The schema within the catalog where the table resides.\n",
    "    - table : The name of the table from which to retrieve data.\n",
    "    - where_clause : A condition to filter the data. Defaults to an empty string, which means no filter is applied.\n",
    "    - selection : The columns to be retrieved, either as a list of column names or \n",
    "      '*' for all columns. Defaults to '*'.\n",
    "    Return value: DataFrame.\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"SELECT {selection} FROM {catalog}.{schema}.{table} {where_clause}\")\n",
    "    \n",
    "  def insert_campaign_status(self, camp_name, campaign_id, execution_ids, iteration, duration,ntb_start_time, ntb_end_time,  exec_status, func_name, message, extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col):\n",
    "    \"\"\"\n",
    "    Description: Inserts campaign computation status into the `adv_campaign_computation_window_status` table.\n",
    "    Parameters:\n",
    "    - camp_name : The name of the campaign.\n",
    "    - campaign_id : The unique ID of the campaign.\n",
    "    - execution_ids : The execution identifier associated with the current execution.\n",
    "    - iteration : The current iteration number for the campaign execution.\n",
    "    - duration : The duration of the notebook execution.\n",
    "    - ntb_start_time: The start time of the notebook execution.\n",
    "    - ntb_end_time : The end time of the notebook execution.\n",
    "    - exec_status : The status of the execution.\n",
    "    - func_name : The name of the function that failed, if any.\n",
    "    - message : An error message\n",
    "    - extraction : Indicates whether data was extracted.\n",
    "    - _az_insert_ts : Timestamp of when the record was inserted.\n",
    "    - _az_update_ts : Timestamp of the last update to the record.\n",
    "    - checksum_col : The checksum value for the campaign data.\n",
    "    - exec_run_id_col : The execution run identifier.\n",
    "    Return value:  None\n",
    "    \"\"\"\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {self._catalog}.{self._drvd_schema}.adv_campaign_computation_window_status\n",
    "        (Campaign_Name, Campaign_ID, Execution_ID, Iteration, Duration, Start_Time, \n",
    "            End_Time, Execution_Status, Failed_Function, Error_Message, Extraction_To_FS,\n",
    "            _az_insert_ts, _az_update_ts, _checksum, _exec_run_id)\n",
    "        VALUES\n",
    "        ('{self.camp_name}', '{self.campaign_id}', '{self.execution_ids}', '{self.iteration}', '{duration}',\n",
    "         '{self.ntb_start_time}', '{ntb_end_time}', '{self.exec_status}', '{func_name}', \n",
    "         '{message}', '{extraction}', '{_az_insert_ts}', '{_az_update_ts}', '{checksum_col}',\n",
    "         '{exec_run_id_col}')\n",
    "    \"\"\")\n",
    "\n",
    "  def update_campaign_status(self, campaign_id, duration, ntb_end_time, exec_status, func_name, message, extraction, execution_ids, _az_update_ts, checksum_col):\n",
    "    '''\n",
    "    Description: Updates an existing campaign computation status record in the `adv_campaign_computation_window_status` table.\n",
    "    Parameters:\n",
    "    - campaign_id : Unique identifier for the campaign.\n",
    "    - duration : Updated duration of the notebook run.\n",
    "    - ntb_end_time : Updated end time of the campaign.\n",
    "    - exec_status : Updated execution status .\n",
    "    - func_name : Name of the function that failed, if applicable.\n",
    "    - message : Error message.\n",
    "    - extraction : Updated status of extraction.\n",
    "    - execution_ids : ID related to the execution of the campaign.\n",
    "    - _az_update_ts : Timestamp for when the record is updated.\n",
    "    - checksum_col : Updated MD5 checksum value.\n",
    "    Return value: None\n",
    "    '''\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {self._catalog}.{self._drvd_schema}.adv_campaign_computation_window_status\n",
    "        SET Duration = '{duration}',\n",
    "            End_Time = '{ntb_end_time}',\n",
    "            Execution_Status = '{self.exec_status}',\n",
    "            Failed_Function = '{func_name}',\n",
    "            Error_Message = '{message}',\n",
    "            Extraction_To_FS = '{extraction}',\n",
    "            _az_update_ts = '{_az_update_ts}',\n",
    "            _checksum = '{checksum_col}'\n",
    "        WHERE Campaign_ID = '{self.campaign_id}' AND Execution_ID = '{self.execution_ids}' AND Execution_Status = 'Processing'\n",
    "    \"\"\")    \n",
    "      \n",
    "  def adv_campaign_computation_window_status(self, campaign_id, camp_name, execution_ids, iteration, duration, exec_status, ntb_start_time, ntb_end_time, func_name, message, extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col):\n",
    "    \"\"\"\n",
    "    Description: Updates or inserts the status of an advertising campaign based on its current execution status.\n",
    "    - If the campaign's execution status is \"Processing\", the function calls `insert_campaign_status` otherwise `update_campaign_status`.\n",
    "    Parameters:\n",
    "    - campaign_id : Unique identifier for the campaign.\n",
    "    - camp_name : Name of the campaign.\n",
    "    - execution_ids : Unique identifiers for the execution.\n",
    "    - iteration : The current iteration of the execution.\n",
    "    - duration : The duration of the execution in seconds.\n",
    "    - exec_status : The current execution status of the campaign.\n",
    "    - ntb_start_time : Start time of the notebook.\n",
    "    - ntb_end_time : End time of the notebook.\n",
    "    - func_name : Name of the function that is failed if any.\n",
    "    - message : error message.\n",
    "    - extraction : Status about the data extraction process.\n",
    "    - _az_insert_ts : Timestamp when the record is inserted.\n",
    "    - _az_update_ts : Timestamp when the record is last updated.\n",
    "    - checksum_col : Updated MD5 checksum value.\n",
    "    - exec_run_id_col : Execution run ID column. \n",
    "    Return value: None   \n",
    "    \"\"\"\n",
    "    if self.exec_status == \"Processing\":\n",
    "        self.insert_campaign_status(camp_name, campaign_id, execution_ids, iteration, duration,ntb_start_time, ntb_end_time,  exec_status, func_name, message, extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col)\n",
    "    else:\n",
    "        self.update_campaign_status(campaign_id, duration, ntb_end_time, exec_status, func_name, message, extraction, execution_ids, _az_update_ts, checksum_col)\n",
    "\n",
    "  def sent_responce_to_parent(self, func_name, message):\n",
    "    '''\n",
    "    Description : Records the status of a campaign computation by inserting or updating a record in a Spark SQL table, including campaign ID, name, iteration, duration, start and end time of notebook, execution status, function name, and error message.\n",
    "    Parameters : \n",
    "    - func_name: Name of the function\n",
    "    - message: Error occurred if function failed or status message\n",
    "    Return value : None\n",
    "    '''\n",
    "    ntb_end_time = datetime.datetime.now()\n",
    "    duration = ntb_end_time - self.ntb_start_time\n",
    "    \n",
    "    if message == \"Processing\":\n",
    "        message = \"\" \n",
    "        self.exec_status = \"Processing\"\n",
    "        extraction = \"Processing is ongoing.\"\n",
    "    elif message:\n",
    "        self.exec_status = \"Failed\"\n",
    "        extraction = \"Can not be extracted due to failure.\"\n",
    "    else:\n",
    "        self.exec_status = \"Success\"\n",
    "        extraction = \"yet to start.\"\n",
    "\n",
    "    _az_insert_ts = datetime.datetime.now()\n",
    "    _az_update_ts = datetime.datetime.now()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"campaign_id\", StringType(), False),\n",
    "        StructField(\"camp_name\", StringType(), False),\n",
    "        StructField(\"execution_ids\", StringType(), False),\n",
    "        StructField(\"iteration\", IntegerType(), False),\n",
    "        StructField(\"duration\", StringType(), False),\n",
    "        StructField(\"exec_status\", StringType(), False),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "        StructField(\"extraction\", StringType(), True),\n",
    "        StructField(\"func_name\", StringType(), False),\n",
    "        StructField(\"_az_insert_ts\", TimestampType(), False),\n",
    "        StructField(\"_az_update_ts\", TimestampType(), False),\n",
    "        StructField(\"ntb_start_time\", TimestampType(), False),\n",
    "        StructField(\"ntb_end_time\", TimestampType(), False)\n",
    "    ])\n",
    "\n",
    "    Adv_window_status_df = spark.createDataFrame([(self.campaign_id, self.camp_name, self.execution_ids, \n",
    "                                self.iteration, str(duration), self.exec_status, message, extraction,\n",
    "                                func_name, _az_insert_ts, _az_update_ts, self.ntb_start_time,\n",
    "                                ntb_end_time)], schema)\n",
    "\n",
    "\n",
    "    Adv_window_checksum_df = Adv_window_status_df.withColumn(\"checksum\", md5(concat_ws(\"||\", \n",
    "            lit(self.campaign_id), lit(self.camp_name), lit(self.execution_ids), lit(self.iteration),\n",
    "            lit(duration), lit(self.exec_status), lit(message), lit(extraction),\n",
    "            lit(func_name), lit(_az_insert_ts), lit(_az_update_ts),\n",
    "            lit(self.ntb_start_time), lit(ntb_end_time)\n",
    "        )))\n",
    "    # Extract the computed checksum value from the DataFrame\n",
    "    checksum_col = Adv_window_checksum_df.select(\"checksum\").collect()[0][0]\n",
    "    exec_run_id_col = self.execution_ids\n",
    "\n",
    "    self.adv_campaign_computation_window_status(self.campaign_id, self.camp_name, self.execution_ids, self.iteration, duration, self.exec_status, self.ntb_start_time, ntb_end_time, func_name, message, extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col)\n",
    "\n",
    "  def concat_checksum_cols(self,df):\n",
    "    '''\n",
    "    Description : This filters columns in a DataFrame by excluding those listed in ignore_column_list_md5\n",
    "    Parameters : dataframe\n",
    "    Return value : columns list(that are not in the ignore_column_list_md5)\n",
    "    '''\n",
    "    bizColList= [col for col in df.columns if (col not in self.ignore_column_list_md5)]\n",
    "    columnList = []\n",
    "\n",
    "    for column in bizColList:\n",
    "        if column is None:\n",
    "            columnList.append(':')\n",
    "        else:\n",
    "            columnList.append(column)\n",
    "\n",
    "    return columnList\n",
    "\n",
    "  def process_safegraph(self, safegraph_df, config_df):\n",
    "    '''\n",
    "    Description : This function joins DataFrames (safegraph_df and config_df) on matching location and address fields, filters for non-null POI location names, and selects relevant columns.\n",
    "    Parameters : \n",
    "    - safegraph_df: safegraph_poi_h3 dataframe\n",
    "    - config_df: advertisement_campaign config dataframe\n",
    "    Return value : dataframe\n",
    "    \n",
    "    '''\n",
    "    filtered_safegraph = (\n",
    "        safegraph_df\n",
    "        .join(config_df, [\n",
    "            safegraph_df.location_name == config_df.poi_loc_name,\n",
    "            safegraph_df.street_address == config_df.street_addr,\n",
    "            safegraph_df.city == config_df.city,\n",
    "            safegraph_df.province == config_df.province,\n",
    "            safegraph_df.postal_code == config_df.postal_code,\n",
    "            safegraph_df.top_category == config_df.top_category,\n",
    "            safegraph_df.sub_category == config_df.sub_category\n",
    "        ], \"left\")\n",
    "        .filter(config_df.poi_loc_name.isNotNull())\n",
    "        .select(\n",
    "            \"location_name\", \"location_id\", \"brands\", \"latitude\", \"longitude\", \"location_perimeter\",\n",
    "            lit(None).alias(\"location_radius\"), \"street_address\", safegraph_df[\"city\"], safegraph_df[\"province\"], safegraph_df[\"postal_code\"],\n",
    "            safegraph_df[\"top_category\"], safegraph_df[\"sub_category\"], safegraph_df[\"category_tags\"], \"opened_on\", \"closed_on\",\n",
    "            \"iso_country_code\", \"naics_code\", \"census_code\",\n",
    "            \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\",\n",
    "            \"eff_to\", \"eff_from\", \"hexagon_wkt\"\n",
    "        )\n",
    "    )\n",
    "    return filtered_safegraph\n",
    "\n",
    "  def process_custom_geofence(self, custom_geofence_df, config_df):\n",
    "    '''\n",
    "    Description : This function filters a DataFrame of custom geofence data by joining it with a configuration DataFrame.\n",
    "    Parameters : \n",
    "    - custom_geofence_df: custom_geofence_poi_h3 dataframe\n",
    "    - config_df: advertisement_campaign config dataframe\n",
    "    Return value : dataframe\n",
    "    '''\n",
    "    filtered_custom_geofence = (\n",
    "        custom_geofence_df\n",
    "        .join(config_df, [\n",
    "            custom_geofence_df.location_name == config_df.poi_loc_name,\n",
    "            custom_geofence_df.street_address == config_df.street_addr,\n",
    "            custom_geofence_df.city == config_df.city,\n",
    "            custom_geofence_df.province == config_df.province,\n",
    "            custom_geofence_df.postal_code == config_df.postal_code,\n",
    "            custom_geofence_df.top_category == config_df.top_category,\n",
    "            custom_geofence_df.sub_category == config_df.sub_category\n",
    "        ] + [\n",
    "            custom_geofence_df.category_tags == config_df.category_tags\n",
    "        ], \"left\")\n",
    "        .filter(config_df.poi_loc_name.isNotNull())\n",
    "        .select(\n",
    "            \"location_name\", \"location_id\", \"brands\", \"latitude\", \"longitude\", \"location_perimeter\",\n",
    "            lit(None).alias(\"location_radius\"), \"street_address\", custom_geofence_df[\"city\"], custom_geofence_df[\"province\"], custom_geofence_df[\"postal_code\"],\n",
    "            custom_geofence_df[\"top_category\"], custom_geofence_df[\"sub_category\"], custom_geofence_df[\"category_tags\"], \"opened_on\", lit(None).alias(\"closed_on\"), \"iso_country_code\",\n",
    "            lit(None).alias(\"naics_code\"), lit(None).alias(\"census_code\"), \"hexagon_id\",\n",
    "            \"cellid\", \"site_name\", \"sitecode\", lit(None).alias(\"opened_no_later_than\"),\n",
    "            lit(None).alias(\"tracking_closed_since\"), lit(None).alias(\"eff_to\"), lit(None).alias(\"eff_from\"),\n",
    "            \"hexagon_wkt\"\n",
    "        )\n",
    "    )\n",
    "    return filtered_custom_geofence\n",
    "\n",
    "  def process_advertiser(self, advertiser_df, config_df):\n",
    "    '''\n",
    "    Description : This function selects and filters relevant columns of advertiser data and config_df.\n",
    "    Parameters : \n",
    "    - advertiser_df: advertiser_poi_h3 dataframe\n",
    "    - config_df: advertisement_campaign config dataframe\n",
    "    Return value : dataframe\n",
    "    '''\n",
    "    filtered_advertiser = (\n",
    "        advertiser_df\n",
    "        .join(config_df, [\n",
    "            advertiser_df.location_name == config_df.poi_loc_name,\n",
    "            advertiser_df.street_address == config_df.street_addr,\n",
    "            advertiser_df.city == config_df.city,\n",
    "            advertiser_df.province == config_df.province,\n",
    "            advertiser_df.postal_code == config_df.postal_code,\n",
    "            advertiser_df.top_category == config_df.top_category,\n",
    "            advertiser_df.sub_category == config_df.sub_category\n",
    "        ] + [\n",
    "            advertiser_df.category_tags == config_df.category_tags\n",
    "        ], \"left\")\n",
    "        .filter(config_df.poi_loc_name.isNotNull())\n",
    "        .select(\n",
    "            \"location_name\", \"location_id\", \"brands\", \"latitude\", \"longitude\", \"location_perimeter\",\n",
    "            \"location_radius\", \"street_address\", advertiser_df[\"city\"], advertiser_df[\"province\"], advertiser_df[\"postal_code\"], advertiser_df[\"top_category\"],\n",
    "            advertiser_df[\"sub_category\"], advertiser_df[\"category_tags\"], \"opened_on\", lit(None).alias(\"closed_on\"), \"iso_country_code\", lit(None).alias(\"naics_code\"),\n",
    "            lit(None).alias(\"census_code\"), \"hexagon_id\", \"cellid\", \"site_name\",\n",
    "            \"sitecode\", lit(None).alias(\"opened_no_later_than\"), lit(None).alias(\"tracking_closed_since\"),\n",
    "            lit(None).alias(\"eff_to\"), lit(None).alias(\"eff_from\"), \"hexagon_wkt\"\n",
    "        )\n",
    "    )\n",
    "    return filtered_advertiser\n",
    "  \n",
    "  def combine_results(self, filtered_safegraph, filtered_custom_geofence, filtered_advertiser):\n",
    "    '''\n",
    "    Description : This function merges three DataFrames by performing a union operation.\n",
    "    Parameters : \n",
    "    - filtered_safegraph: safegraph_poi_h3 dataframe\n",
    "    - filtered_advertiser: advertiser_poi_h3 dataframe\n",
    "    - filtered_custom_geofence: custom_geofence_poi_h3 dataframe \n",
    "    Return value : dataframe \n",
    "    '''\n",
    "    combined_df = (\n",
    "        filtered_safegraph\n",
    "        .union(filtered_custom_geofence)\n",
    "        .union(filtered_advertiser)\n",
    "    )\n",
    "    return combined_df\n",
    "\n",
    "  def adv_poi_geofence_union(self, result_df):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `adv_poi_geofence_union`.\n",
    "    Parameters:\n",
    "    - result_df: DataFrame with data to be inserted. \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "      result_df.createOrReplaceTempView(\"result_df\")\n",
    "      spark.sql(f\"\"\"\n",
    "          INSERT INTO {self._catalog}.{self._drvd_schema}.adv_poi_geofence_union\n",
    "                (location_name, location_id, brands, latitude, longitude, location_radius, location_perimeter, street_address, city, province, postal_code, top_category, sub_category, category_tags, opened_on, opened_no_later_than, tracking_closed_since, closed_on, iso_country_code, naics_code, census_code, hexagon_id, hexagon_wkt, cellid, site_name, sitecode, eff_from, eff_to, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id)\n",
    "          SELECT \n",
    "                location_name, location_id, brands, latitude, longitude, location_radius, location_perimeter, street_address, city, province, postal_code, top_category, sub_category, category_tags, opened_on, opened_no_later_than, tracking_closed_since, closed_on, iso_country_code, naics_code, census_code, hexagon_id, hexagon_wkt, cellid, site_name, sitecode, eff_from, eff_to, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id \n",
    "          FROM result_df\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def safegraph_advertiser_custom_union(self):  \n",
    "    '''\n",
    "    Description : Union of all POI (safegraph, custom_geofence, and advertiser).\n",
    "    Parameters : None\n",
    "    Return value : DataFrame\n",
    "    '''  \n",
    "    try:\n",
    "        # initially the exec_status = 'Processing' for monitoring in adv_campaign_window_status table \n",
    "        self.sent_responce_to_parent('', 'Processing')\n",
    "        self.config.createOrReplaceTempView('config')\n",
    "        self.safegraph.createOrReplaceTempView('safegraph')\n",
    "        self.advertiser.createOrReplaceTempView(\"advertiser\")\n",
    "        self.custom_geofence.createOrReplaceTempView(\"custom_geofence\") \n",
    "\n",
    "        filtered_safegraph = self.process_safegraph(self.safegraph, self.config)\n",
    "        filtered_custom_geofence = self.process_custom_geofence(self.custom_geofence, self.config)\n",
    "        filtered_advertiser = self.process_advertiser(self.advertiser, self.config)\n",
    "        \n",
    "        result_df = self.combine_results(filtered_safegraph, filtered_custom_geofence, filtered_advertiser)\n",
    "        \n",
    "        exec_run_id_col = self.execution_ids\n",
    "        result_df = result_df.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(result_df)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(exec_run_id_col))\n",
    "        # calling adv_poi_geofence_union function to insert data into table\n",
    "        self.adv_poi_geofence_union(result_df)\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('safegraph_advertiser_custom_union', e.getErrorClass())\n",
    "\n",
    "  def join_union_network_data(self, network_df, poi_union_df):\n",
    "    '''\n",
    "    Description :  Joins result of safegraph_advertiser_custom_union with network data.\n",
    "    Parameters : \n",
    "    - network_df: network DataFrame\n",
    "    - poi_union_df: safegraph_advertiser_custom_union DataFrame \n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        self.network_df.createOrReplaceTempView(\"network_data_df\")\n",
    "        poi_union_df.createOrReplaceTempView(\"poi_union_df\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT\n",
    "                network_data.*,\n",
    "                p.location_name, p.location_id,\n",
    "                p.brands, p.latitude, p.longitude,\n",
    "                p.location_perimeter, p.location_radius, p.street_address, p.city,\n",
    "                p.province, p.postal_code, p.top_category, p.sub_category, p.category_tags,\n",
    "                p.opened_on, p.closed_on, p.iso_country_code, p.naics_code, p.census_code,\n",
    "                p.hexagon_id, p.cellid,\n",
    "                p.opened_no_later_than, p.tracking_closed_since, p.eff_to, p.eff_from, p.hexagon_wkt\n",
    "            FROM network_data_df AS network_data\n",
    "            INNER JOIN poi_union_df AS p\n",
    "            ON ((network_data.eci = p.cellid OR network_data.nci = p.cellid))\n",
    "        \"\"\"\n",
    "        \n",
    "        joined_df = spark.sql(query).distinct()\n",
    "        return joined_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('join_union_network_data',e.getErrorClass())\n",
    "  \n",
    "  def filter_network_data_with_campaign_and_billing(self, network_data_df):\n",
    "    '''\n",
    "    Description : Filters network data based on both campaign and billing configurations. \n",
    "    Parameters : \n",
    "    - network_data_df: network DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "\n",
    "    final_df_list = []\n",
    "\n",
    "    network_data_df = network_data_df.withColumn('event_date', to_date(col('event_timestamp'), 'yyyy-MM-dd').cast(StringType()))\n",
    "    network_data_df = network_data_df.withColumn('event_day', lower(date_format(col('event_timestamp'), 'EEEE')).cast(StringType()))\n",
    "    network_data_df = network_data_df.withColumn('event_time_24h', date_format(col('event_timestamp'), 'HH:mm:ss'))\n",
    "    \n",
    "    network_data_df.createOrReplaceTempView(\"network_df\")\n",
    "    self.config.createOrReplaceTempView(\"campaign_billing_config\")\n",
    "\n",
    "    try:\n",
    "        for row in self.config.collect():\n",
    "            # Convert 'start_date' and 'end_date' from string to date format\n",
    "            start_date = datetime.datetime.strptime(row['start_date'], '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "            end_date = datetime.datetime.strptime(row['end_date'], '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "\n",
    "            # Parse 'days_list' from string to a list of lowercase day names\n",
    "            days_tuple = ast.literal_eval(row['days_list'])\n",
    "            days_list = [day.strip().lower() for day in days_tuple]\n",
    "            days_list_str = ', '.join(f\"'{day}'\" for day in days_list)\n",
    "\n",
    "            if row['province_ctn'] is None or row['province_ctn'] == \"\":\n",
    "               # If 'province_ctn' is None or empty, use 'LIKE \"%_%\"' in the SQL query to match all province codes.\n",
    "               province_ctn_clause = ' LIKE \"%_%\" '\n",
    "            else:\n",
    "               # If 'province_ctn' has specific values, use an SQL 'IN' clause to filter records by those values.\n",
    "               province_ctn_clause = \" IN \" + row['province_ctn']\n",
    "\n",
    "            if row['city_ctn'] is None or row['city_ctn'] == \"\":\n",
    "               city_ctn_clause = ' LIKE \"%_%\" '\n",
    "            else:\n",
    "               city_ctn_clause = \" IN \" + row['city_ctn']\n",
    "            \n",
    "            if row['postal_code_ctn'] is None or row['postal_code_ctn'] == \"\":\n",
    "               postal_ctn_clause = ' LIKE \"%_%\" '\n",
    "            else:\n",
    "               postal_ctn_clause = \" IN \" + row['postal_code_ctn']\n",
    "\n",
    "            # Parse and format time window start and end times\n",
    "            time_window_selection = row['time_window_selection']\n",
    "            time_window_start_str, time_window_end_str = time_window_selection.split('-')\n",
    "            time_window_start_dt = datetime.datetime.strptime(time_window_start_str.strip(), '%I:%M %p')\n",
    "            time_window_end_dt = datetime.datetime.strptime(time_window_end_str.strip(), '%I:%M %p')\n",
    "            time_window_start = time_window_start_dt.strftime('%I:%M:%S %p')\n",
    "            time_window_end = time_window_end_dt.strftime('%I:%M:%S %p')\n",
    "            time_window_start_24hr = datetime.datetime.strptime(time_window_start, '%I:%M:%S %p').strftime('%H:%M:%S')\n",
    "            time_window_end_24hr = datetime.datetime.strptime(time_window_end, '%I:%M:%S %p').strftime('%H:%M:%S')\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT nw.*,\n",
    "                    cc.campaign_name, cc.start_date, cc.end_date, cc.week, cc.month, cc.zone_name, cc.custom_geofence_name,\n",
    "                    cc.poi_loc_name, cc.city AS c_city, cc.street_addr, cc.province AS c_province, cc.postal_code AS c_postal_code,\n",
    "                    cc.top_category AS c_top_category, cc.sub_category AS c_sub_category, cc.category_tags AS c_category_tags,\n",
    "                    nw.ci_province_code AS province_ctn, nw.ci_city_name AS city_ctn, nw.ci_postal_code AS postal_code_ctn,\n",
    "                    cc.config_id as campaign_id, cc.time_window_selection, cc.days_list\n",
    "                FROM network_df nw INNER JOIN campaign_billing_config cc\n",
    "                ON\n",
    "                (nw.location_name = cc.poi_loc_name OR nw.location_name = cc.custom_geofence_name)\n",
    "                AND nw.street_address = cc.street_addr \n",
    "                AND nw.city = cc.city\n",
    "                AND nw.province = cc.province\n",
    "                AND nw.postal_code = cc.postal_code\n",
    "                WHERE\n",
    "                nw.event_date BETWEEN '{start_date}' AND '{end_date}' \n",
    "                AND nw.event_time_24h BETWEEN '{time_window_start_24hr}' AND '{time_window_end_24hr}'\n",
    "                AND nw.event_day IN ({days_list_str})\n",
    "                AND nw.ci_province_code{province_ctn_clause}\n",
    "                AND nw.ci_city_name{city_ctn_clause}\n",
    "                AND nw.ci_postal_code{postal_ctn_clause}       \n",
    "            \"\"\"\n",
    "\n",
    "            final_df = spark.sql(query).distinct()\n",
    "            final_df_list.append(final_df)\n",
    "        \n",
    "        if final_df_list:\n",
    "            combined_df = final_df_list[0]\n",
    "            for df in final_df_list[1:]:\n",
    "                combined_df = combined_df.union(df)\n",
    "            \n",
    "            return combined_df.distinct()\n",
    "        else:\n",
    "           return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('filter_network_data_with_campaign_and_billing', e.getErrorClass())\n",
    "\n",
    "# TO DO (Prizm data is not available)\n",
    "  def filter_prizm_data(self):\n",
    "    '''\n",
    "    Description :  Filters PRIZM data based on postal codes from the configuration.\n",
    "    Parameters : None\n",
    "    Return value : DataFrame\n",
    "    '''     \n",
    "    try: \n",
    "        if row['postal_code_ctn'] is None or row['postal_code_ctn'] == \"\":\n",
    "               postal_code_ctn_clause = ' LIKE \"%_%\" '\n",
    "        else:\n",
    "               postal_code_ctn_clause = \" IN \" + row['postal_code_ctn']\n",
    "\n",
    "        if row['prizm_segment'] is None or row['prizm_segment'] == \"\":\n",
    "               prizm_segment_clause = ' LIKE \"%_%\" '\n",
    "        else:\n",
    "               prizm_segment_clause = \" IN \" + row['prizm_segment']\n",
    "\n",
    "        self.prizm_df = self.prizm_df.withColumn(\"clean_FSALDU\", replace(col(\"FSALDU\"), \" \", \"\"))\n",
    "        self.config.createOrReplaceTempView(\"config\")\n",
    "        self.prizm_df.createOrReplaceTempView(\"prizm\")\n",
    "        query = \"\"\"\n",
    "        SELECT p.*\n",
    "        FROM prizm p\n",
    "        INNER JOIN (\n",
    "            SELECT DISTINCT postal_code_ctn{postal_code_ctn_clause} AS postal_code \n",
    "            AND prizm_segment{prizm_segment_clause} as prizm_segment\n",
    "            FROM config\n",
    "        ) c\n",
    "        ON p.clean_FSALDU = c.postal_code\n",
    "        AND p.NAME = c.prizm_segment\n",
    "        \"\"\"\n",
    "        filtered_prizm_df = spark.sql(query).distinct()\n",
    "        return filtered_prizm_df\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('filter_prizm_data', e.getErrorClass())\n",
    "\n",
    "# TO DO (Prizm data is not available)\n",
    "  def join_network_prizm_data(self, network_df, prizm_df):\n",
    "    '''\n",
    "    Description :  Joins network data with PRIZM data.\n",
    "    Parameters : \n",
    "    - network_df: Network DataFrame\n",
    "    - filter_prizm: Prizm DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        network_df.createOrReplaceTempView(\"network_df\")\n",
    "        prizm_df.createOrReplaceTempView(\"prizm_df\")\n",
    "\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "                network_df_n.*,\n",
    "                p.LSNAME,\n",
    "                p.PRIZM,\n",
    "                p.FSALDU,\n",
    "                p.NAME as prizm_segment\n",
    "            FROM network_df AS network_df_n\n",
    "            INNER JOIN prizm_df AS p\n",
    "            ON network_df_n.ci_postal_code = p.FSALDU\n",
    "        \"\"\"\n",
    "        joined_df = spark.sql(query).distinct()\n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('join_network_prizm_data', e.getErrorClass())\n",
    "\n",
    "  def adv_campaign_computation_window(self, computation_granularity_data):\n",
    "    \"\"\"\n",
    "    Description: Inserts computation data into the `adv_campaign_computation_window` table.\n",
    "    Parameters:\n",
    "    - computation_granularity_data: DataFrame.\n",
    "    Return value: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        computation_granularity_data.createOrReplaceTempView(\"computation_granularity_data\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {self._catalog}.{self._drvd_schema}.adv_campaign_computation_window\n",
    "              (campaign_id, campaign_name, start_date, end_date, time_window_selection, days_list, \n",
    "              location_name, location_id, location_perimeter, longitude, latitude, street_address, city, postal_code, province, msisdn, ctn, ws_subscriber_no, event_date, event_time_24h, event_day, event_timestamp, province_ctn, city_ctn, postal_code_ctn, _az_insert_ts, _az_update_ts,  _checksum, _exec_run_id)\n",
    "            SELECT \n",
    "                campaign_id, campaign_name, start_date, end_date, time_window_selection, days_list,location_name, location_id, location_perimeter, longitude, latitude,street_address, city, postal_code, province, msisdn, ctn, ws_subscriber_no, event_date, event_time_24h, event_day, event_timestamp, province_ctn, city_ctn, postal_code_ctn, _az_insert_ts, _az_update_ts,  _checksum, _exec_run_id\n",
    "            FROM computation_granularity_data\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def computation_granularity(self, computation_granularity_data):\n",
    "    '''\n",
    "    Description : Transforms computation granularity data for analysis.\n",
    "    Parameters : \n",
    "    - computation_granularity_data: DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        columns = computation_granularity_data.columns\n",
    "        unique_columns = []\n",
    "        [unique_columns.append(item) for item in columns if item not in unique_columns]\n",
    "\n",
    "        computation_granularity_data = computation_granularity_data.select(unique_columns)\n",
    "        \n",
    "        computation_granularity_data.createOrReplaceTempView(\"computation_granularity_data\")\n",
    "\n",
    "        query_ = \"\"\"\n",
    "            SELECT \n",
    "                campaign_id, campaign_name, start_date, end_date, time_window_selection, days_list, location_name,\n",
    "                location_perimeter, street_address, city, postal_code, province, msisdn, ctn, ws_subscriber_no, event_date,\n",
    "                event_time_24h, event_day, event_timestamp, NULL AS prizm_segment, province_ctn, city_ctn, postal_code_ctn, longitude, latitude, location_id\n",
    "            from computation_granularity_data\n",
    "            \"\"\"\n",
    "\n",
    "        computation_granularity_data = spark.sql(query_).distinct()\n",
    "\n",
    "        #TODO: \n",
    "        # static_prizm_segment_value = 'DefaultSegmentValue'\n",
    "        # computation_granularity_data = computation_granularity_data.withColumn(\n",
    "        #                                     \"prizm_segment\",lit(static_prizm_segment_value))\n",
    "        \n",
    "        #TODO: \n",
    "        _exec_run_id = self.execution_ids\n",
    "        computation_granularity_data = computation_granularity_data.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(computation_granularity_data)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "        # calling adv_campaign_computation_window function to insert data into table\n",
    "        self.adv_campaign_computation_window(computation_granularity_data)\n",
    "        return computation_granularity_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('computation_granularity', e.getErrorClass())\n",
    "  \n",
    "  def get_ctn_prev_visit_time(self, computation_granularity):\n",
    "    '''\n",
    "    Description : Calculates the previous visit time for each visitor (ctn) separately within each day partitions by both location and date-related columns. \n",
    "    Parameters : \n",
    "    - computation_granularity_data: DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    computation_granularity.createOrReplaceTempView(\"df_filtered\")\n",
    "    df = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_date, event_time_24h, event_timestamp,\n",
    "                        LAG(event_timestamp) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_date, event_day ORDER BY event_timestamp) AS prev_visit_time\n",
    "                    FROM df_filtered\n",
    "                    \"\"\")\n",
    "    return df\n",
    "\n",
    "  def get_ctn_total_prev_visit_time(self, computation_granularity):\n",
    "    '''\n",
    "    Description : Calculates the previous visit time across different days for each visitor partitions by only location and visitor.\n",
    "    Parameters : \n",
    "    - computation_granularity_data: DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    computation_granularity.createOrReplaceTempView(\"df_filtered\")\n",
    "    df = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_date, event_time_24h, event_timestamp,\n",
    "                        LAG(event_timestamp) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn ORDER BY event_timestamp) AS prev_visit_time\n",
    "                    FROM df_filtered\n",
    "                    \"\"\")\n",
    "    return df\n",
    "\n",
    "  def adv_campaign_window_result(self, campaign_window_data):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `adv_campaign_window_result`.\n",
    "    Parameters:\n",
    "    - campaign_window_data: DataFrame with data to be inserted. \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        campaign_window_data.createOrReplaceTempView(\"campaign_window_data\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {self._catalog}.{self._drvd_schema}.adv_campaign_window_result\n",
    "              (campaign_id, campaign_name, event_day, event_date, location_name, location_id, location_perimeter, street_address, longitude, latitude, city, province, postal_code, \n",
    "              no_of_visits, unique_visitors, time_spent, _az_insert_ts, _az_update_ts,  _checksum, _exec_run_id)\n",
    "            SELECT \n",
    "                campaign_id, campaign_name, event_day, event_date, location_name, location_id, location_perimeter, street_address, longitude, latitude, city, province, postal_code,\n",
    "                no_of_visits, unique_visitors, time_spent, _az_insert_ts, _az_update_ts,  _checksum, _exec_run_id\n",
    "            FROM campaign_window_data\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def daily_computation_metrics(self, computation_granularity):\n",
    "      '''\n",
    "      Description : The function processes daily metrics by calculating previous visit times, aggregating time_spent, no_of_visits, and unique visitors.\n",
    "      Parameters : \n",
    "      - computation_granularity: DataFrame\n",
    "      Return value : DataFrame\n",
    "      '''\n",
    "      try:\n",
    "          \n",
    "          days_to_process = [row.event_day for row in computation_granularity.select(\"event_day\").distinct().collect()]\n",
    "          final_result = None\n",
    "          computation_granularity.createOrReplaceTempView(\"df_filtered\")\n",
    "          self.network_df.createOrReplaceTempView(\"network_event\")\n",
    "          df = self.get_ctn_prev_visit_time(computation_granularity)\n",
    "          df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "          # Select previous visit times and compute minutes difference, considering various conditions\n",
    "          df = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_time_24h, event_date, event_timestamp, prev_visit_time,\n",
    "                              CASE\n",
    "                                  WHEN prev_visit_time IS NOT NULL AND (UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60 < 15 THEN \n",
    "                                      GREATEST((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60, 0)\n",
    "                                  ELSE\n",
    "                                    CASE\n",
    "                                        WHEN ((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time)) / 60 > 15\n",
    "                                            AND (SELECT COUNT(*) FROM network_event AS ne WHERE ne.event_timestamp > prev_visit_time AND ne.event_timestamp < event_timestamp AND ne.ctn = ctn) > 0)\n",
    "                                        THEN 0.0\n",
    "                                        ELSE GREATEST((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60, 0)\n",
    "                                    END\n",
    "                              END AS minutes_diff    \n",
    "                          FROM temp_table\n",
    "                         \"\"\")\n",
    "          df.createOrReplaceTempView(\"temp_table1\")\n",
    "\n",
    "          # Aggregate total minutes spent with sum of minutes_diff for location\n",
    "          df = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_date,\n",
    "                              SUM(minutes_diff) AS total_minutes_spent\n",
    "                          FROM temp_table1\n",
    "                          GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_date\n",
    "                         \"\"\")\n",
    "          df.createOrReplaceTempView(\"time1\")\n",
    "\n",
    "          # Aggregate total minutes spent with sum of total_minutes_spent for location\n",
    "          df = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code, event_date, event_day,\n",
    "                              SUM(total_minutes_spent) AS total_minutes_spent\n",
    "                          FROM time1\n",
    "                          GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, event_date, event_day\n",
    "                         \"\"\")\n",
    "          df.createOrReplaceTempView(\"total_time_spent\")\n",
    "\n",
    "          # Calculate visit counts with a flag for new visits\n",
    "          df_count = spark.sql(\"\"\"\n",
    "                               WITH CTE AS (\n",
    "                                        SELECT\n",
    "                                            *,\n",
    "                                            LAG(ctn) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_date, event_day ORDER BY event_timestamp) AS prev_ctn,\n",
    "                                            LAG(event_timestamp) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_date, event_day ORDER BY event_timestamp) AS prev_visit_time\n",
    "                                        FROM df_filtered\n",
    "                                    )\n",
    "                                    SELECT\n",
    "                                        *,\n",
    "                                        CASE\n",
    "                                            WHEN ctn != prev_ctn\n",
    "                                                OR prev_ctn IS NULL\n",
    "                                            THEN 1\n",
    "                                            ELSE \n",
    "                                                CASE\n",
    "                                                    WHEN ((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time)) / 60 > 5\n",
    "                                                        AND (SELECT COUNT(*) FROM network_event AS ne \n",
    "                                                        WHERE ne.event_timestamp > prev_visit_time \n",
    "                                                        AND ne.event_timestamp < event_timestamp \n",
    "                                                        AND ne.ctn = ctn) > 0)\n",
    "                                                    THEN 1\n",
    "                                                    ELSE 0\n",
    "                                                END\n",
    "                                        END AS new_visit_flag\n",
    "                                    FROM CTE;\n",
    "                               \"\"\")\n",
    "          df_count.createOrReplaceTempView(\"visit_count\")\n",
    "\n",
    "          # Compute the number of visits per location\n",
    "          df_visit = spark.sql(\"\"\"\n",
    "                               SELECT\n",
    "                                    *,\n",
    "                                    SUM(new_visit_flag) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, campaign_id, campaign_name \n",
    "                                    ORDER BY event_time_24h ROWS UNBOUNDED PRECEDING) AS no_of_visits\n",
    "                                FROM visit_count\n",
    "                               \"\"\")\n",
    "          df_visit.createOrReplaceTempView(\"visit_number\")\n",
    "          \n",
    "          # Count unique visitors per location\n",
    "          df_unique_visits = spark.sql(\"\"\"\n",
    "                                       SELECT\n",
    "                                            location_name, location_id, street_address, latitude, \n",
    "                                            longitude, city, province, postal_code,\n",
    "                                            campaign_id, campaign_name, event_date, event_day,\n",
    "                                            COUNT(DISTINCT ctn) AS unique_visitor\n",
    "                                        FROM df_filtered\n",
    "                                        GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, campaign_id, campaign_name, event_date, event_day\n",
    "                                        \n",
    "                                       \"\"\")\n",
    "          df_unique_visits.createOrReplaceTempView(\"unique_visits\")\n",
    "\n",
    "          # Combine metrics for final output which includes no_of_visits, unique_visitors, time_spent\n",
    "          final_record = spark.sql(\"\"\"\n",
    "                                   SELECT\n",
    "                                        vn.location_name, vn.location_id, vn.street_address, vn.latitude,\n",
    "                                        vn.longitude, vn.city, vn.province, vn.postal_code,\n",
    "                                        vn.location_perimeter, vn.event_timestamp, vn.campaign_id, \n",
    "                                        vn.campaign_name,vn.event_day, vn.event_date, vn.no_of_visits, \n",
    "                                        uv.unique_visitor,\n",
    "                                        tms.total_minutes_spent AS time_spent\n",
    "                                    FROM visit_number vn\n",
    "                                    JOIN unique_visits uv ON vn.location_name = uv.location_name \n",
    "                                    AND vn.location_id = uv.location_id\n",
    "                                    AND vn.street_address = uv.street_address\n",
    "                                    AND vn.latitude = uv.latitude\n",
    "                                    AND vn.longitude = uv.longitude\n",
    "                                    AND vn.city = uv.city AND vn.province=uv.province\n",
    "                                    AND vn.postal_code=uv.postal_code AND vn.campaign_id = uv.campaign_id\n",
    "                                    AND vn.campaign_name = uv.campaign_name\n",
    "                                    JOIN total_time_spent tms ON vn.location_name = tms.location_name\n",
    "                                    AND vn.street_address = tms.street_address AND vn.city = tms.city\n",
    "                                    AND vn.province=tms.province AND vn.postal_code=tms.postal_code\n",
    "                                    ORDER BY vn.location_name, vn.location_id, vn.street_address, vn.latitude,\n",
    "                                    vn.longitude, vn.city,\n",
    "                                    vn.province, vn.postal_code, vn.campaign_id, vn.campaign_name, vn.no_of_visits, vn.ctn\n",
    "                                   \"\"\")\n",
    "          final_record.createOrReplaceTempView(\"daily_computation\")\n",
    "\n",
    "          # Aggregate final metrics to remove duplicates and summarize data\n",
    "          final_record = spark.sql(\"\"\"\n",
    "                                   SELECT\n",
    "                                        campaign_id, campaign_name, event_day, event_date, \n",
    "                                        location_name, location_id, first(location_perimeter) AS location_perimeter, \n",
    "                                        street_address, latitude, longitude, city, province, postal_code,\n",
    "                                        MAX(no_of_visits) AS no_of_visits, \n",
    "                                        MAX(unique_visitor) AS unique_visitors,\n",
    "                                        MAX(time_spent) AS time_spent\n",
    "                                    FROM daily_computation\n",
    "                                    GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, \n",
    "                                    campaign_id, campaign_name, event_day, event_date\n",
    "                                   \"\"\")\n",
    "          final_record.createOrReplaceTempView(\"final_record\")\n",
    "\n",
    "          # Get the earliest event timestamp for each group\n",
    "          first_record = spark.sql(\"\"\"\n",
    "                                          SELECT\n",
    "                                                campaign_id, campaign_name, event_day, event_date,\n",
    "                                                location_name, location_id, street_address, latitude, longitude, city, province, postal_code,\n",
    "                                                MIN(event_timestamp) AS event_timestamp\n",
    "                                            FROM daily_computation\n",
    "                                            GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code,\n",
    "                                            campaign_id, campaign_name, event_day, event_date\n",
    "                                          \"\"\")\n",
    "          first_record.createOrReplaceTempView(\"first_record\")\n",
    "          \n",
    "          _exec_run_id = self.execution_ids\n",
    "\n",
    "          # Join the final metrics with the earliest event timestamp\n",
    "          final_result = spark.sql(\"\"\"\n",
    "                                   SELECT\n",
    "                                        fr.*,\n",
    "                                        fir.event_timestamp\n",
    "                                    FROM final_record AS fr\n",
    "                                    JOIN first_record AS fir ON fr.campaign_id = fir.campaign_id\n",
    "                                    AND fr.campaign_name = fir.campaign_name\n",
    "                                    AND fr.event_day = fir.event_day\n",
    "                                    AND fr.event_date = fir.event_date\n",
    "                                    AND fr.location_name = fir.location_name\n",
    "                                    AND fr.location_id = fir.location_id\n",
    "                                    AND fr.street_address = fir.street_address\n",
    "                                    AND fr.latitude = fir.latitude\n",
    "                                    AND fr.longitude = fir.longitude\n",
    "                                    AND fr.city = fir.city\n",
    "                                    AND fr.province = fir.province\n",
    "                                    AND fr.postal_code = fir.postal_code\n",
    "                                \"\"\")\n",
    "          \n",
    "          final_result = final_result.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(final_result)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "          # calling adv_campaign_window_result function to insert data into table\n",
    "          self.adv_campaign_window_result(final_result)\n",
    "          return final_result\n",
    "      \n",
    "      except Exception as e:\n",
    "          self.sent_responce_to_parent('daily_computation_metrics', e.getErrorClass())\n",
    "  \n",
    "  def adv_campaign_total_result(self, campaign_total_data):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `adv_campaign_total_result`.\n",
    "    Parameters:\n",
    "    - campaign_total_data: DataFrame containing the data to be inserted. Must conform to the schema of the target table.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        campaign_total_data.createOrReplaceTempView(\"campaign_total_data\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO {self._catalog}.{self._drvd_schema}.adv_campaign_total_result\n",
    "                (campaign_id, campaign_name ,location_name , location_id , location_perimeter, street_address, latitude, longitude, city , province, postal_code, no_of_visits,  unique_visitors, time_spent, avg_daily_visits, avg_daily_unique_visitors, avg_daily_time_spent, _az_insert_ts,  _az_update_ts, _checksum, _exec_run_id)\n",
    "            SELECT \n",
    "                campaign_id, campaign_name ,location_name , location_id , location_perimeter, street_address, latitude, longitude, city , province, postal_code, no_of_visits,  unique_visitors, time_spent, avg_daily_visits, avg_daily_unique_visitors, avg_daily_time_spent, _az_insert_ts,  _az_update_ts, _checksum, _exec_run_id\n",
    "            FROM campaign_total_data\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def total_computation_metrics(self, computation_granularity, daily_df):\n",
    "      '''\n",
    "      Description : Calculates total_no_of_visits, total_time_spent, total_unique_visitors, avg_daily_visits, avg_daily_time_spent, avg_daily_unique_visitors.\n",
    "      Parameters : \n",
    "      - computation_granularity: DataFrame\n",
    "      - daily_df: DataFrame\n",
    "      Return value : DataFrame\n",
    "      '''\n",
    "      try:\n",
    "          # Collect distinct event days from computation_granularity\n",
    "          days_to_process = [row.event_day for row in computation_granularity.select(\"event_day\").distinct().collect()]\n",
    "          final_result = None\n",
    "          computation_granularity.createOrReplaceTempView(\"df_filtered\")\n",
    "          df = self.get_ctn_total_prev_visit_time(computation_granularity)\n",
    "          df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "          # Compute time differences between visits\n",
    "          df1 = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn, event_day, event_time_24h, event_date, event_timestamp, prev_visit_time,\n",
    "                              CASE\n",
    "                                  WHEN prev_visit_time IS NOT NULL AND (UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60 < 15 THEN \n",
    "                                      GREATEST((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60, 0)\n",
    "                                  ELSE\n",
    "                                    CASE\n",
    "                                        WHEN ((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time)) / 60 > 15\n",
    "                                            AND (SELECT COUNT(*) FROM network_event AS ne WHERE ne.event_timestamp > prev_visit_time AND ne.event_timestamp < event_timestamp AND ne.ctn = ctn) > 0)\n",
    "                                        THEN 0.0\n",
    "                                        ELSE GREATEST((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time))/60, 0)\n",
    "                                    END\n",
    "                              END AS minutes_diff    \n",
    "                          FROM temp_table\n",
    "                         \"\"\")\n",
    "          df1.createOrReplaceTempView(\"temp_table1\")\n",
    "\n",
    "          # Aggregate total minutes spent with sum of minutes_diff\n",
    "          df2 = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn,\n",
    "                              SUM(minutes_diff) AS total_minutes_spent\n",
    "                          FROM temp_table1\n",
    "                          GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn\n",
    "                         \"\"\")\n",
    "          df2.createOrReplaceTempView(\"time1\")\n",
    "\n",
    "          # Aggregate total minutes spent with sum of total_minutes_spent for location\n",
    "          df3 = spark.sql(\"\"\"\n",
    "                          SELECT \n",
    "                              location_name, location_id, street_address, latitude, longitude, city, province, postal_code,\n",
    "                              SUM(total_minutes_spent) AS total_minutes_spent\n",
    "                          FROM time1\n",
    "                          GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code\n",
    "                         \"\"\")\n",
    "          df3.createOrReplaceTempView(\"total_time_spent\")\n",
    "          self.network_df.createOrReplaceTempView(\"network_event\")\n",
    "       \n",
    "          # Calculate visit counts with a flag for new visits\n",
    "          df_count = spark.sql(\"\"\"\n",
    "                               WITH CTE AS (\n",
    "                                        SELECT\n",
    "                                            *,\n",
    "                                            LAG(ctn) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn ORDER BY event_timestamp) AS prev_ctn,\n",
    "                                            LAG(event_timestamp) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, ctn ORDER BY event_timestamp) AS prev_visit_time\n",
    "                                        FROM df_filtered\n",
    "                                    )\n",
    "                                    SELECT\n",
    "                                        *,\n",
    "                                        CASE\n",
    "                                            WHEN ctn != prev_ctn\n",
    "                                                OR prev_ctn IS NULL\n",
    "                                            THEN 1\n",
    "                                            ELSE \n",
    "                                                CASE\n",
    "                                                    WHEN ((UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(prev_visit_time)) / 60 > 5\n",
    "                                                        AND (SELECT COUNT(*) FROM network_event AS ne \n",
    "                                                        WHERE ne.event_timestamp > prev_visit_time \n",
    "                                                        AND ne.event_timestamp < event_timestamp \n",
    "                                                        AND ne.ctn = ctn) > 0)\n",
    "                                                    THEN 1\n",
    "                                                    ELSE 0\n",
    "                                                END\n",
    "                                        END AS new_visit_flag\n",
    "                                    FROM CTE;\n",
    "                               \"\"\")\n",
    "          df_count.createOrReplaceTempView(\"visit_count\")\n",
    "\n",
    "          # Compute the number of visits per location\n",
    "          df_visit = spark.sql(\"\"\"\n",
    "                               SELECT\n",
    "                                    *,\n",
    "                                    SUM(new_visit_flag) OVER (PARTITION BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, campaign_id, campaign_name \n",
    "                                    ORDER BY event_time_24h ROWS UNBOUNDED PRECEDING) AS no_of_visits\n",
    "                                FROM visit_count\n",
    "                               \"\"\")\n",
    "          df_visit.createOrReplaceTempView(\"visit_number\")\n",
    "          \n",
    "          # Count unique visitors per location\n",
    "          df_unique_visits = spark.sql(\"\"\"\n",
    "                                       SELECT\n",
    "                                            location_name, location_id, street_address, latitude, longitude, \n",
    "                                            city, province, postal_code,\n",
    "                                            campaign_id, campaign_name,\n",
    "                                            COUNT(DISTINCT ctn) AS unique_visitor\n",
    "                                        FROM df_filtered\n",
    "                                        GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code, campaign_id, campaign_name\n",
    "                                       \"\"\")\n",
    "          df_unique_visits.createOrReplaceTempView(\"unique_visits\")\n",
    "          # Combine metrics from no_of_visits, unique_visitors, and time_spent\n",
    "          final_record = spark.sql(\"\"\"\n",
    "                                   SELECT\n",
    "                                        vn.location_name, vn.location_id, vn.street_address, vn.latitude, vn.longitude, vn.city, vn.province, vn.postal_code,\n",
    "                                        vn.location_perimeter, vn.campaign_id, vn.campaign_name,\n",
    "                                        vn.no_of_visits, uv.unique_visitor,\n",
    "                                        tms.total_minutes_spent AS time_spent\n",
    "                                    FROM visit_number vn \n",
    "                                    JOIN unique_visits uv ON vn.location_name = uv.location_name \n",
    "                                    AND vn.location_id = uv.location_id\n",
    "                                    AND vn.street_address = uv.street_address\n",
    "                                    AND vn.latitude = uv.latitude\n",
    "                                    AND vn.longitude = uv.longitude\n",
    "                                    AND vn.city = uv.city \n",
    "                                    AND vn.province=uv.province\n",
    "                                    AND vn.postal_code=uv.postal_code \n",
    "                                    AND vn.campaign_id = uv.campaign_id\n",
    "                                    AND vn.campaign_name = uv.campaign_name\n",
    "                                    JOIN total_time_spent tms ON vn.location_name = tms.location_name\n",
    "                                    AND vn.location_id = tms.location_id\n",
    "                                    AND vn.street_address = tms.street_address\n",
    "                                    AND vn.latitude = tms.latitude\n",
    "                                    AND vn.longitude = tms.longitude\n",
    "                                    AND vn.street_address = tms.street_address \n",
    "                                    AND vn.city = tms.city\n",
    "                                    AND vn.province=tms.province \n",
    "                                    AND vn.postal_code=tms.postal_code\n",
    "                                    ORDER BY vn.location_name, vn.location_id, vn.street_address, vn.latitude, vn.longitude, vn.street_address, vn.city, vn.province, vn.postal_code, vn.campaign_id, vn.campaign_name, vn.no_of_visits\n",
    "                                   \"\"\")\n",
    "          final_record.createOrReplaceTempView(\"daily_computation\")          \n",
    "          daily_df.createOrReplaceTempView(\"daily_df\")\n",
    "\n",
    "           # Calculate aggregated total metrics and averages, joining with daily_df\n",
    "          final_result = spark.sql(\"\"\"\n",
    "                                   SELECT\n",
    "                                        dc.campaign_id, dc.campaign_name,  \n",
    "                                        dc.location_name, dc.location_id, \n",
    "                                        first(dc.location_perimeter) AS location_perimeter,\n",
    "                                        dc.street_address, dc.latitude, dc.longitude, dc.city, dc.province, dc.postal_code,\n",
    "                                        MAX(dc.no_of_visits) AS no_of_visits, \n",
    "                                        MAX(dc.unique_visitor) AS unique_visitors,\n",
    "                                        MAX(dc.time_spent) AS time_spent,\n",
    "                                        AVG(dd.no_of_visits) as avg_daily_visits,\n",
    "                                        AVG(dd.time_spent) as avg_daily_time_spent,\n",
    "                                        AVG(dd.unique_visitors) as avg_daily_unique_visitors\n",
    "                                    FROM daily_computation AS dc\n",
    "                                    JOIN daily_df AS dd \n",
    "                                    ON dc.location_name = dd.location_name \n",
    "                                    AND dc.location_id = dd.location_id\n",
    "                                    AND dc.street_address = dd.street_address\n",
    "                                    AND dc.latitude = dd.latitude\n",
    "                                    AND dc.longitude = dd.longitude\n",
    "                                    AND dc.city = dd.city\n",
    "                                    AND dc.province = dd.province\n",
    "                                    AND dc.postal_code = dd.postal_code\n",
    "                                    AND dc.campaign_id = dd.campaign_id\n",
    "                                    AND dc.campaign_name = dd.campaign_name\n",
    "                                    GROUP BY dc.location_name, dc.location_id, dc.street_address, dc.latitude, dc.longitude, dc.city, dc.province, dc.postal_code, dc.campaign_id, dc.campaign_name\n",
    "                                   \"\"\")\n",
    "      \n",
    "          # Select records for final processing\n",
    "          first_record = spark.sql(\"\"\"\n",
    "                                          SELECT\n",
    "                                                campaign_id, campaign_name, \n",
    "                                                location_name, location_id, street_address, latitude, longitude, city, province, postal_code\n",
    "                                            FROM daily_computation\n",
    "                                            GROUP BY location_name, location_id, street_address, latitude, longitude, city, province, postal_code,\n",
    "                                            campaign_id, campaign_name\n",
    "                                          \"\"\")\n",
    "          _exec_run_id = self.execution_ids\n",
    "\n",
    "          # Join final results with distinct records to include metadata \n",
    "          final_result_join = final_result.join(first_record, on=['campaign_id', 'campaign_name', 'location_name', 'location_id', 'street_address', 'latitude', 'longitude', 'city', 'province', 'postal_code'], how='inner')\n",
    "          final_result_join = final_result_join.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(final_result_join)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "\n",
    "          # calling adv_campaign_total_result function to insert data into table\n",
    "          self.adv_campaign_total_result(final_result_join)\n",
    "\n",
    "          # calling sent_responce_to_parent function to update adv_campaign_window_status table for exec_status = \"Success\"\n",
    "          self.sent_responce_to_parent('', '')\n",
    "          return final_result_join\n",
    "      except Exception as e:\n",
    "          self.sent_responce_to_parent('total_computation_metrics', e.getErrorClass())\n",
    "\n",
    "  def adv_campaign_pbi_poi(self, pbi_poi):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `pbi_locations_visitors_details`.\n",
    "    Parameters:\n",
    "    - pbi_poi: DataFrame with data to be inserted. \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pbi_poi.createOrReplaceTempView(\"result\")\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {self._catalog}.{self._drvd_schema}.pbi_locations_visitors_details\n",
    "        (campaign_name, location_name, tot_visits, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id)\n",
    "        SELECT\n",
    "        campaign_name, location_name, tot_visits, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id\n",
    "        FROM result\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def pbi_poi(self, poi):\n",
    "    '''\n",
    "    Description : Computes the total number of visits for each campaign and location, grouped and ordered accordingly.\n",
    "    Parameters : \n",
    "    - computation_granularity: DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        poi.createOrReplaceTempView(\"computation_granularity\")\n",
    "\n",
    "        query = \"\"\"\n",
    "         SELECT\n",
    "            campaign_name,\n",
    "            location_name,\n",
    "            COUNT(*) AS tot_visits\n",
    "            FROM\n",
    "                computation_granularity\n",
    "            GROUP BY\n",
    "                campaign_name,\n",
    "                location_name\n",
    "            ORDER BY\n",
    "                campaign_name,\n",
    "                location_name\n",
    "        \"\"\"\n",
    "        result = spark.sql(query).distinct()\n",
    "        _exec_run_id = self.execution_ids\n",
    "        result = result.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(result)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "        # calling adv_campaign_pbi_poi function to insert data into table\n",
    "        self.adv_campaign_pbi_poi(result)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('pbi_poi', e.getErrorClass())\n",
    "\n",
    "  def adv_campaign_pbi_topbillingaddress(self, pbi_topbillingaddress):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `pbi_billing_address_visitors_details`.\n",
    "    Parameters:\n",
    "    - pbi_topbillingaddress: DataFrame containing the data to be inserted.\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pbi_topbillingaddress.createOrReplaceTempView(\"result\")\n",
    "        spark.sql(f\"\"\"\n",
    "        INSERT INTO {self._catalog}.{self._drvd_schema}.pbi_billing_address_visitors_details\n",
    "        (campaign_name, billing_postalcode, tot_visits, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id)\n",
    "        SELECT\n",
    "        campaign_name, billing_postalcode, tot_visits, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id\n",
    "        FROM result\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def pbi_topbillingaddressdata(self, topbillingaddressdata):\n",
    "    '''\n",
    "    Description :  Computes the total number of visits grouped by campaign name and postal code, and returns the result sorted by these fields.\n",
    "    Parameters : \n",
    "    - computation_granularity: DataFrame\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        topbillingaddressdata.createOrReplaceTempView(\"computation_granularity\")\n",
    "\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "            campaign_name,\n",
    "            postal_code_ctn AS billing_postalcode,\n",
    "            COUNT(*) AS tot_visits\n",
    "        FROM\n",
    "            computation_granularity\n",
    "        GROUP BY\n",
    "            campaign_name,\n",
    "            postal_code_ctn\n",
    "        ORDER BY\n",
    "            campaign_name,\n",
    "            postal_code_ctn\n",
    "        \"\"\"\n",
    "        result = spark.sql(query).distinct()\n",
    "        _exec_run_id = self.execution_ids\n",
    "        result = result.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                  .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(result)))) \\\n",
    "                                  .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "        # calling adv_campaign_pbi_topbillingaddress function to insert data into table\n",
    "        self.adv_campaign_pbi_topbillingaddress(result)\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('pbi_topbillingaddressdata', e.getErrorClass())\n",
    "  \n",
    "  def adv_campaign_pbi_campaigndetails(self, pbi_campaigndetails):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `pbi_campaign_details`.\n",
    "    Parameters:\n",
    "    - pbi_campaigndetails: DataFrame with data to be inserted. \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pbi_campaigndetails.createOrReplaceTempView(\"final_result\")\n",
    "        spark.sql(f\"\"\"\n",
    "          INSERT INTO {self._catalog}.{self._drvd_schema}.pbi_campaign_details\n",
    "              (campaign_name, date, prizm_segments, time_window_num, tot_visits, datekey, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id)\n",
    "              SELECT\n",
    "              campaign_name, date, prizm_segments, time_window_num, tot_visits, datekey, _az_insert_ts, _az_update_ts, _checksum, _exec_run_id\n",
    "              FROM final_result\n",
    "          \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "   \n",
    "  def pbi_campaigndetails(self, campaigndetails):\n",
    "      '''\n",
    "      Description :  This function processes campaign details data by converting event times into time windows and adding associated numerical values. \n",
    "      Parameters : \n",
    "      - computation_granularity: DataFrame\n",
    "      Return value : DataFrame\n",
    "      '''\n",
    "      try:\n",
    "          campaigndetails = campaigndetails.withColumn(\"event_time_24h\", col(\"event_time_24h\").cast(StringType()))\n",
    "\n",
    "          def map_time_window(start_time):\n",
    "              start_time = str(start_time) \n",
    "              if \"00:00\" <= start_time < \"06:00\":\n",
    "                  return \"EarlyMorning\", 1\n",
    "              elif \"06:00\" <= start_time < \"09:00\":\n",
    "                  return \"MorningCommute\", 2\n",
    "              elif \"09:00\" <= start_time < \"12:00\":\n",
    "                  return \"LateMorning\", 3\n",
    "              elif \"12:00\" <= start_time < \"15:00\":\n",
    "                  return \"Midday\", 4\n",
    "              elif \"15:00\" <= start_time < \"18:00\":\n",
    "                  return \"EveningCommute\", 5\n",
    "              elif \"18:00\" <= start_time < \"21:00\":\n",
    "                  return \"Evening\", 6\n",
    "              elif \"21:00\" <= start_time < \"24:00\":\n",
    "                  return \"LateEvening\", 7\n",
    "              else:\n",
    "                  return \"Unknown\", 0\n",
    "              \n",
    "          time_window_udf = udf(map_time_window, StructType([\n",
    "              StructField(\"time_window\", StringType(), True),\n",
    "              StructField(\"time_window_num\", IntegerType(), True)\n",
    "          ]))\n",
    "\n",
    "          campaigndetails = campaigndetails.withColumn(\"time_window_struct\", time_window_udf(\"event_time_24h\"))\n",
    "          campaigndetails = campaigndetails.withColumn(\"time_window\", col(\"time_window_struct.time_window\"))\n",
    "          campaigndetails = campaigndetails.withColumn(\"time_window_num\", col(\"time_window_struct.time_window_num\"))\n",
    "          campaigndetails = campaigndetails.drop(\"time_window_struct\")\n",
    "          campaigndetails.createOrReplaceTempView(\"computation_granularity1\")\n",
    "          query = \"\"\"\n",
    "              SELECT\n",
    "                  campaign_name,\n",
    "                  date,\n",
    "                  prizm_segment AS prizm_segments,\n",
    "                  DATE_FORMAT(date, 'yyyyMMdd') AS datekey,\n",
    "                  first(time_window_num) AS time_window_num,\n",
    "                  COUNT(*) AS tot_visits\n",
    "              FROM\n",
    "                  computation_granularity1\n",
    "              GROUP BY\n",
    "                  campaign_name,\n",
    "                  date,\n",
    "                  prizm_segment,\n",
    "                  time_window_num\n",
    "              ORDER BY\n",
    "                  campaign_name,\n",
    "                  date,\n",
    "                  prizm_segment,\n",
    "                  time_window_num\n",
    "          \"\"\"\n",
    "\n",
    "          result = spark.sql(query).distinct()\n",
    "          _exec_run_id = self.execution_ids\n",
    "          final_result = result.withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                                    .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                                    .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(result)))) \\\n",
    "                                    .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "          # calling adv_campaign_pbi_campaigndetails function to insert data into table\n",
    "          self.adv_campaign_pbi_campaigndetails(final_result)\n",
    "          return final_result\n",
    "      except Exception as e:\n",
    "        print(str(e))\n",
    "        self.sent_responce_to_parent('pbi_campaigndetails', e.getErrorClass())\n",
    "\n",
    "  def adv_campaign_pbi_campaignList(self, pbi_campaignList):\n",
    "    \"\"\"\n",
    "    Description: Inserts data from the provided DataFrame into the target table `pbi_campaign_list`.\n",
    "    Parameters:\n",
    "    - pbi_campaignList : DataFrame containing the data to be inserted. \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pbi_campaignList.createOrReplaceTempView(\"filtered_result\")\n",
    "        spark.sql(f\"\"\"\n",
    "            INSERT INTO edl_dev.drvd__app_rsmgeo5g.pbi_campaign_list\n",
    "                (campaign_name, start_date, end_date, unique_visitors, tot_visits, processing_status, _checksum, _az_insert_ts, _az_update_ts, _exec_run_id)\n",
    "                SELECT\n",
    "                campaign_name, start_date, end_date, unique_visitors, tot_visits, processing_status, _checksum, _az_insert_ts, _az_update_ts, _exec_run_id\n",
    "                FROM filtered_result\n",
    "            \"\"\")\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "  def pbi_campaignList(self, campaignList):\n",
    "    '''\n",
    "    Description : Computes total and unique visits per campaign, including date and time window breakdown, and orders the results accordingly.\n",
    "    Parameters : computation_granularity(DataFrame)\n",
    "    Return value : DataFrame\n",
    "    '''\n",
    "    try:\n",
    "        campaignList.createOrReplaceTempView(\"computation_granularity\")\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "            campaign_name,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            COUNT(*) AS tot_visits,\n",
    "            COUNT(DISTINCT CONCAT(campaign_name, start_date, end_date)) AS unique_visitors\n",
    "        FROM\n",
    "            computation_granularity\n",
    "        GROUP BY\n",
    "            campaign_name,\n",
    "            start_date,\n",
    "            end_date\n",
    "        ORDER BY\n",
    "            campaign_name,\n",
    "            start_date,\n",
    "            end_date\n",
    "        \"\"\"\n",
    "        result = spark.sql(query).distinct()\n",
    "        exec_status_value = self.exec_status\n",
    "        result = result.withColumn(\"processing_status\", lit(exec_status_value))\n",
    "        _exec_run_id = self.execution_ids\n",
    "        result = result.withColumn(\"processing_status\", lit(exec_status_value)) \\\n",
    "                    .withColumn(\"_az_insert_ts\", current_timestamp()) \\\n",
    "                    .withColumn(\"_az_update_ts\", current_timestamp()) \\\n",
    "                    .withColumn(\"_checksum\", md5(concat_ws(\"||\", *self.concat_checksum_cols(result)))) \\\n",
    "                    .withColumn(\"_exec_run_id\", lit(_exec_run_id))\n",
    "        result.createOrReplaceTempView(\"result\")\n",
    "        filtered_result = result.filter(result.processing_status == \"Success\")\n",
    "        if filtered_result.count() > 0:\n",
    "            # calling adv_campaign_pbi_campaignList function to insert data into table\n",
    "            self.adv_campaign_pbi_campaignList(filtered_result)\n",
    "        return filtered_result\n",
    "    except Exception as e:\n",
    "        self.sent_responce_to_parent('pbi_campaignList', e.getErrorClass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6eeac56a-1b55-4e82-85a9-66b586832886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n----------------------------------------------------------------------\nRan 10 tests in 0.005s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 1\n",
    "import unittest\n",
    "from unittest.mock import patch\n",
    "\n",
    "def get_data_from_db(self, catalog, schema, table, where_clause=\"\", selection=\"*\"):\n",
    "    \"\"\"\n",
    "    Description: Retrieves data from a specified database table.\n",
    "    Parameters:\n",
    "    - catalog : The name of the catalog from which to retrieve the data.\n",
    "    - schema : The schema within the catalog where the table resides.\n",
    "    - table : The name of the table from which to retrieve data.\n",
    "    - where_clause : A condition to filter the data. Defaults to an empty string, which means no filter is applied.\n",
    "    - selection : The columns to be retrieved, either as a list of column names or \n",
    "        '*' for all columns. Defaults to '*'.\n",
    "    Return value: DataFrame.\n",
    "    \"\"\"\n",
    "    return spark.sql(f\"SELECT {selection} FROM {catalog}.{schema}.{table} {where_clause}\")\n",
    "\n",
    "class TestGetDataFromDb(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Assume the spark session is created outside the test class\n",
    "        global spark\n",
    "        cls.spark = spark\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        pass\n",
    "\n",
    "    @patch.object(spark, 'sql')  # Mocking the spark.sql method directly using the class attribute\n",
    "    def test_get_data_from_db_no_filter(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 1: Test get_data_from_db with no where_clause (default behavior).\n",
    "        \"\"\"\n",
    "        # Mock the return value of spark.sql\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "\n",
    "        # Call the function with default where_clause and selection\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3')\n",
    "\n",
    "        # Verify that the SQL query was constructed correctly\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT * FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        \n",
    "        # Assert that the result is as expected\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')  # Mocking the spark.sql method directly using the class attribute\n",
    "    def test_get_data_from_db_with_filter(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 2: Test get_data_from_db with a specific where_clause and selection.\n",
    "        \"\"\"\n",
    "        # Mock the return value of spark.sql\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "\n",
    "        # Call the function with a specific where_clause and selection\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  where_clause=\"WHERE location_id = '123'\", \n",
    "                                  selection=\"location_id, name\")\n",
    "\n",
    "        # Verify that the SQL query was constructed correctly\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT location_id, name FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 WHERE location_id = '123'\")\n",
    "        \n",
    "        # Assert that the result is as expected\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_empty_selection(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 3: Test get_data_from_db with an empty selection.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  selection=\"\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT  FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_complex_where_clause(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 4: Test get_data_from_db with a complex where_clause.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        complex_where = \"WHERE location_id = '123' AND name LIKE 'A%'\"\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  where_clause=complex_where)\n",
    "        mock_spark_sql.assert_called_once_with(f\"SELECT * FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 {complex_where}\")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_all_columns_selection(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 5: Test get_data_from_db with selection of all columns using '*'.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  selection=\"*\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT * FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_single_column_selection(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 6: Test get_data_from_db with selection of a single column.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  selection=\"location_id\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT location_id FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_multiple_columns_selection(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 7: Test get_data_from_db with selection of multiple columns.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  selection=\"location_id, name\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT location_id, name FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_no_where_clause(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 8: Test get_data_from_db with no where_clause but specific selection.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  selection=\"location_id\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT location_id FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_empty_where_clause(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 9: Test get_data_from_db with an empty where_clause.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.return_value = 'Mocked DataFrame'\n",
    "        result = get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                                  where_clause=\"\")\n",
    "        mock_spark_sql.assert_called_once_with(\"SELECT * FROM edl_dev.drvd__app_rsmgeo5g.safegraph_poi_h3 \")\n",
    "        self.assertEqual(result, 'Mocked DataFrame')\n",
    "\n",
    "    @patch.object(spark, 'sql')\n",
    "    def test_get_data_from_db_with_invalid_where_clause(self, mock_spark_sql):\n",
    "        \"\"\"\n",
    "        Test case 10: Test get_data_from_db with an invalid where_clause that causes SQL error.\n",
    "        \"\"\"\n",
    "        mock_spark_sql.side_effect = Exception(\"SQL Error\")\n",
    "        with self.assertRaises(Exception) as context:\n",
    "            get_data_from_db(self=None, catalog='edl_dev', schema='drvd__app_rsmgeo5g', table='safegraph_poi_h3', \n",
    "                             where_clause=\"WHERE invalid_column = 'some_value'\")\n",
    "        self.assertTrue(\"SQL Error\" in str(context.exception))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2d4a362-ce6b-48fe-baa1-b5ed3b611049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....................\n----------------------------------------------------------------------\nRan 20 tests in 0.010s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 2\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "class CampaignStatus:\n",
    "    def __init__(self, spark, catalog, schema):\n",
    "        self._spark = spark\n",
    "        self._catalog = catalog\n",
    "        self._drvd_schema = schema\n",
    "        # Initialize other attributes that are used in the method\n",
    "        self.camp_name = None\n",
    "        self.campaign_id = None\n",
    "        self.execution_ids = None\n",
    "        self.iteration = None\n",
    "        self.ntb_start_time = None\n",
    "        self.exec_status = None\n",
    "\n",
    "    def insert_campaign_status(self, camp_name, campaign_id, execution_ids, iteration, duration,\n",
    "                               ntb_start_time, ntb_end_time, exec_status, func_name, message,\n",
    "                               extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col):\n",
    "        self._spark.sql(f\"\"\"\n",
    "            INSERT INTO {self._catalog}.{self._drvd_schema}.adv_campaign_computation_window_status\n",
    "            (Campaign_Name, Campaign_ID, Execution_ID, Iteration, Duration, Start_Time, \n",
    "                End_Time, Execution_Status, Failed_Function, Error_Message, Extraction_To_FS,\n",
    "                *az*insert_ts, *az*update_ts, *checksum, *exec_run_id)\n",
    "            VALUES\n",
    "            ('{camp_name}', '{campaign_id}', '{execution_ids}', '{iteration}', '{duration}',\n",
    "             '{ntb_start_time}', '{ntb_end_time}', '{exec_status}', '{func_name}', \n",
    "             '{message}', '{extraction}', '{_az_insert_ts}', '{_az_update_ts}', '{checksum_col}',\n",
    "             '{exec_run_id_col}')\n",
    "        \"\"\")\n",
    "\n",
    "class TestInsertCampaignStatus(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.mock_spark = Mock()\n",
    "        self.campaign_status = CampaignStatus(self.mock_spark, \"test_catalog\", \"test_schema\")\n",
    "\n",
    "    def test_insert_campaign_status_basic(self):\n",
    "        \"\"\"Test basic insertion with all fields populated\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test Campaign\", \"123\", \"456\", 1, \"10m\", \"2023-01-01 00:00:00\",\n",
    "            \"2023-01-01 00:10:00\", \"SUCCESS\", \"\", \"\", True, \"2023-01-01\", \"2023-01-01\",\n",
    "            \"checksum123\", \"exec123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_error(self):\n",
    "        \"\"\"Test insertion with error details\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test Campaign\", \"123\", \"456\", 1, \"10m\", \"2023-01-01 00:00:00\",\n",
    "            \"2023-01-01 00:10:00\", \"FAILED\", \"test_func\", \"Error occurred\", False,\n",
    "            \"2023-01-01\", \"2023-01-01\", \"checksum123\", \"exec123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_null_values(self):\n",
    "        \"\"\"Test insertion with null values\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test Campaign\", None, None, None, None, None, None, \"SUCCESS\",\n",
    "            None, None, None, None, None, None, None\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_empty_strings(self):\n",
    "        \"\"\"Test insertion with empty strings\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_special_characters(self):\n",
    "        \"\"\"Test insertion with special characters in fields\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test'Campaign\", \"123\", \"456\", 1, \"10m\", \"2023-01-01 00:00:00\",\n",
    "            \"2023-01-01 00:10:00\", \"SUCCESS\", \"func'name\", \"Error'message\", True,\n",
    "            \"2023-01-01\", \"2023-01-01\", \"checksum'123\", \"exec'123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_numeric_values(self):\n",
    "        \"\"\"Test insertion with numeric values\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            123, 456, 789, 1, 10, \"2023-01-01 00:00:00\", \"2023-01-01 00:10:00\",\n",
    "            \"SUCCESS\", \"\", \"\", True, \"2023-01-01\", \"2023-01-01\", 12345, 67890\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_boolean_values(self):\n",
    "        \"\"\"Test insertion with boolean values\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test Campaign\", \"123\", \"456\", 1, \"10m\", \"2023-01-01 00:00:00\",\n",
    "            \"2023-01-01 00:10:00\", \"SUCCESS\", \"\", \"\", True, \"2023-01-01\", \"2023-01-01\",\n",
    "            True, False\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_long_values(self):\n",
    "        \"\"\"Test insertion with very long values\"\"\"\n",
    "        long_string = \"a\" * 1000\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            long_string, long_string, long_string, 1, long_string, long_string,\n",
    "            long_string, long_string, long_string, long_string, True,\n",
    "            long_string, long_string, long_string, long_string\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_with_unicode_characters(self):\n",
    "        \"\"\"Test insertion with unicode characters\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"cgyuy\", \"123\", \"456\", 1, \"10\", \"20230101 00:00:00\",\n",
    "            \"2023.0101 00:10:00\", \"dy\", \"\", \"\", True,\n",
    "            \"20230101\", \"20230101\", \"123\", \"123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "    def test_insert_campaign_status_sql_injection_attempt(self):\n",
    "        \"\"\"Test insertion with potential SQL injection attempt\"\"\"\n",
    "        self.campaign_status.insert_campaign_status(\n",
    "            \"Test Campaign'; DROP TABLE users; --\", \"123\", \"456\", 1, \"10m\",\n",
    "            \"2023-01-01 00:00:00\", \"2023-01-01 00:10:00\", \"SUCCESS\", \"\", \"\",\n",
    "            True, \"2023-01-01\", \"2023-01-01\", \"checksum123\", \"exec123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1febd682-847c-40cb-bd06-22812abe79da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n----------------------------------------------------------------------\nRan 10 tests in 0.008s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 3\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "class CampaignStatus:\n",
    "    def __init__(self, spark, catalog, schema):\n",
    "        self._spark = spark\n",
    "        self._catalog = catalog\n",
    "        self._drvd_schema = schema\n",
    "        self.exec_status = None\n",
    "        self.campaign_id = None\n",
    "        self.execution_ids = None\n",
    "\n",
    "    def update_campaign_status(self, campaign_id, duration, ntb_end_time, exec_status, func_name, message, extraction, execution_ids, _az_update_ts, checksum_col):\n",
    "        self._spark.sql(f\"\"\"\n",
    "            UPDATE {self._catalog}.{self._drvd_schema}.adv_campaign_computation_window_status\n",
    "            SET Duration = '{duration}',\n",
    "                End_Time = '{ntb_end_time}',\n",
    "                Execution_Status = '{self.exec_status}',\n",
    "                Failed_Function = '{func_name}',\n",
    "                Error_Message = '{message}',\n",
    "                Extraction_To_FS = '{extraction}',\n",
    "                *az*update_ts = '{_az_update_ts}',\n",
    "                *checksum = '{checksum_col}'\n",
    "            WHERE Campaign_ID = '{self.campaign_id}' AND Execution_ID = '{self.execution_ids}' AND Execution_Status = 'Processing'\n",
    "        \"\"\")\n",
    "\n",
    "class TestUpdateCampaignStatus(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.mock_spark = Mock()\n",
    "        self.campaign_status = CampaignStatus(self.mock_spark, \"test_catalog\", \"test_schema\")\n",
    "        self.campaign_status.campaign_id = \"TEST001\"\n",
    "        self.campaign_status.execution_ids = \"EXEC001\"\n",
    "        self.campaign_status.exec_status = \"COMPLETED\"\n",
    "\n",
    "    def test_update_campaign_status_basic(self):\n",
    "        \"\"\"Test basic update with all fields populated\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", \"30m\", \"2023-01-01 00:30:00\", \"COMPLETED\", \"\", \"\", True,\n",
    "            \"EXEC001\", \"2023-01-01 00:30:01\", \"checksum123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"UPDATE test_catalog.test_schema.adv_campaign_computation_window_status\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_error(self):\n",
    "        \"\"\"Test update with error details\"\"\"\n",
    "        self.campaign_status.exec_status = \"FAILED\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", \"15m\", \"2023-01-01 00:15:00\", \"FAILED\", \"process_data\", \"Data processing error\",\n",
    "            False, \"EXEC001\", \"2023-01-01 00:15:01\", \"checksum456\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"'FAILED'\", self.mock_spark.sql.call_args[0][0])\n",
    "        self.assertIn(\"'process_data'\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_long_values(self):\n",
    "        \"\"\"Test update with very long values\"\"\"\n",
    "        long_message = \"A\" * 1000  # 1000 character long message\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", \"30m\", \"2023-01-01 00:30:00\", \"COMPLETED\", \"long_func\", long_message,\n",
    "            True, \"EXEC001\", \"2023-01-01 00:30:01\", \"checksum123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(long_message, self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_null_values(self):\n",
    "        \"\"\"Test update with null values\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", None, None, None, None, None, None, \"EXEC001\", None, None\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"'None'\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_boolean_values(self):\n",
    "        \"\"\"Test update with boolean values\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", \"30m\", \"2023-01-01 00:30:00\", \"COMPLETED\", \"\", \"\", True,\n",
    "            \"EXEC001\", \"2023-01-01 00:30:01\", \"checksum123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"'True'\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_sql_injection_attempt(self):\n",
    "        \"\"\"Test update with potential SQL injection attempt\"\"\"\n",
    "        # Set the campaign_id to include the SQL injection attempt\n",
    "        self.campaign_status.campaign_id = \"TEST001', '1'); DROP TABLE users; --\"\n",
    "        \n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"Ignored\", \"30m\", \"2023-01-01 00:30:00\", \"COMPLETED\",\n",
    "            \"func_name\", \"message\", True, \"EXEC001\", \"2023-01-01 00:30:01\", \"checksum123\"\n",
    "        )\n",
    "        \n",
    "        expected_sql = \"\"\"\n",
    "            UPDATE test_catalog.test_schema.adv_campaign_computation_window_status\n",
    "            SET Duration = '30m',\n",
    "                End_Time = '2023-01-01 00:30:00',\n",
    "                Execution_Status = 'COMPLETED',\n",
    "                Failed_Function = 'func_name',\n",
    "                Error_Message = 'message',\n",
    "                Extraction_To_FS = 'True',\n",
    "                *az*update_ts = '2023-01-01 00:30:01',\n",
    "                *checksum = 'checksum123'\n",
    "            WHERE Campaign_ID = 'TEST001', '1'); DROP TABLE users; --' AND Execution_ID = 'EXEC001' AND Execution_Status = 'Processing'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertEqual(self.mock_spark.sql.call_args[0][0].strip(), expected_sql.strip())\n",
    "\n",
    "    def test_update_campaign_status_with_numeric_values(self):\n",
    "        \"\"\"Test update with numeric values\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            12345, 1800, \"2023-01-01 00:30:00\", \"COMPLETED\", \"\", \"\", 1,\n",
    "            67890, \"2023-01-01 00:30:01\", 987654321\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"'1800'\", self.mock_spark.sql.call_args[0][0])\n",
    "        # Update: The method uses self.execution_ids instead of the passed value\n",
    "        self.assertIn(\"'EXEC001'\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_special_characters(self):\n",
    "        \"\"\"Test update with special characters in fields\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST'001\", \"30m\", \"2023-01-01 00:30:00\", \"COMPLETED\", \"func'name\", \"Error'message\",\n",
    "            True, \"EXEC'001\", \"2023-01-01 00:30:01\", \"checksum'123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        # Update: The method uses self.campaign_id instead of the passed value\n",
    "        self.assertIn(\"TEST001\", self.mock_spark.sql.call_args[0][0])\n",
    "        # Update: The method doesn't escape single quotes, so we need to adjust our expectation\n",
    "        self.assertIn(\"func'name\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "    def test_update_campaign_status_with_unicode_characters(self):\n",
    "        \"\"\"Test update with unicode characters\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"TEST001\", \"30\", \"20230101 00:30:00\", \"\", \"\", \"\",\n",
    "            True, \"EXEC001\", \"20230101 00:30:01\", \"123\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"'30'\", self.mock_spark.sql.call_args[0][0])\n",
    "        self.assertIn(\"''\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "\n",
    "    def test_update_campaign_status_with_empty_strings(self):\n",
    "        \"\"\"Test update with empty strings\"\"\"\n",
    "        self.campaign_status.update_campaign_status(\n",
    "            \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "        )\n",
    "        self.mock_spark.sql.assert_called_once()\n",
    "        self.assertIn(\"''\", self.mock_spark.sql.call_args[0][0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "16178772-3e5b-4b70-b900-9bd670e2dbe4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n----------------------------------------------------------------------\nRan 10 tests in 0.006s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 4\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "\n",
    "class CampaignStatus:\n",
    "    def __init__(self):\n",
    "        self.exec_status = None\n",
    "\n",
    "    def adv_campaign_computation_window_status(self, campaign_id, camp_name, execution_ids, iteration, duration, exec_status, ntb_start_time, ntb_end_time, func_name, message, extraction, az_insert_ts, az_update_ts, checksum_col, exec_run_id_col):\n",
    "        if self.exec_status == \"Processing\":\n",
    "            self.insert_campaign_status(camp_name, campaign_id, execution_ids, iteration, duration, ntb_start_time, ntb_end_time, exec_status, func_name, message, extraction, az_insert_ts, az_update_ts, checksum_col, exec_run_id_col)\n",
    "        else:\n",
    "            self.update_campaign_status(campaign_id, duration, ntb_end_time, exec_status, func_name, message, extraction, execution_ids, az_update_ts, checksum_col)\n",
    "\n",
    "    def insert_campaign_status(self, *args):\n",
    "        pass\n",
    "\n",
    "    def update_campaign_status(self, *args):\n",
    "        pass\n",
    "\n",
    "class TestAdvCampaignComputationWindowStatus(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.campaign_status = CampaignStatus()\n",
    "        self.campaign_status.insert_campaign_status = Mock()\n",
    "        self.campaign_status.update_campaign_status = Mock()\n",
    "\n",
    "    def test_processing_status_calls_insert(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Processing', '2023-01-01 00:00:00', '2023-01-01 01:00:00', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_completed_status_calls_update(self):\n",
    "        self.campaign_status.exec_status = \"Completed\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Completed', '2023-01-01 00:00:00', '2023-01-01 01:00:00', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.update_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_failed_status_calls_update(self):\n",
    "        self.campaign_status.exec_status = \"Failed\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Failed', '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'error_func', 'Error occurred', False, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.update_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_insert_called_with_correct_parameters(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Processing', '2023-01-01 00:00:00', '2023-01-01 01:00:00', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.insert_campaign_status.assert_called_with('Test Campaign', 'CAMP001', 'EXEC001', 1, 3600, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'Processing', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "\n",
    "    def test_update_called_with_correct_parameters(self):\n",
    "        self.campaign_status.exec_status = \"Completed\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Completed', '2023-01-01 00:00:00', '2023-01-01 01:00:00', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.update_campaign_status.assert_called_with('CAMP001', 3600, '2023-01-01 01:00:00', 'Completed', '', '', True, 'EXEC001', '2023-01-01 01:00:00', 'checksum123')\n",
    "\n",
    "    def test_processing_status_with_error_info(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001', 1, 3600, 'Processing', '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'error_func', 'Error occurred', False, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_multiple_execution_ids(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('CAMP001', 'Test Campaign', 'EXEC001,EXEC002', 1, 3600, 'Processing', '2023-01-01 00:00:00', '2023-01-01 01:00:00', '', '', True, '2023-01-01 00:00:00', '2023-01-01 01:00:00', 'checksum123', 'run001')\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_empty_string_parameters(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status('', '', '', 0, 0, '', '', '', '', '', '', '', '', '', '')\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_none_parameters(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        self.campaign_status.adv_campaign_computation_window_status(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "    def test_long_parameter_values(self):\n",
    "        self.campaign_status.exec_status = \"Processing\"\n",
    "        long_string = 'a' * 1000\n",
    "        self.campaign_status.adv_campaign_computation_window_status(long_string, long_string, long_string, 1, 3600, 'Processing', long_string, long_string, long_string, long_string, True, long_string, long_string, long_string, long_string)\n",
    "        self.campaign_status.insert_campaign_status.assert_called_once()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b87cd051-221d-4cbe-b971-5fcd6e0eef2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n----------------------------------------------------------------------\nRan 10 tests in 0.014s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 5\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "def sent_responce_to_parent(self, func_name, message):\n",
    "    ntb_end_time = datetime.now()\n",
    "    duration = ntb_end_time - self.ntb_start_time\n",
    "    \n",
    "    if message == \"Processing\":\n",
    "        message = \"\" \n",
    "        self.exec_status = \"Processing\"\n",
    "        extraction = \"Processing is ongoing.\"\n",
    "    elif message:\n",
    "        self.exec_status = \"Failed\"\n",
    "        extraction = \"Can not be extracted due to failure.\"\n",
    "    else:\n",
    "        self.exec_status = \"Success\"\n",
    "        extraction = \"yet to start.\"\n",
    "\n",
    "    _az_insert_ts = datetime.now()\n",
    "    _az_update_ts = datetime.now()\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"campaign_id\", StringType(), False),\n",
    "        StructField(\"camp_name\", StringType(), False),\n",
    "        StructField(\"execution_ids\", StringType(), False),\n",
    "        StructField(\"iteration\", IntegerType(), False),\n",
    "        StructField(\"duration\", StringType(), False),\n",
    "        StructField(\"exec_status\", StringType(), False),\n",
    "        StructField(\"error_message\", StringType(), True),\n",
    "        StructField(\"extraction\", StringType(), True),\n",
    "        StructField(\"func_name\", StringType(), False),\n",
    "        StructField(\"_az_insert_ts\", TimestampType(), False),\n",
    "        StructField(\"_az_update_ts\", TimestampType(), False),\n",
    "        StructField(\"ntb_start_time\", TimestampType(), False),\n",
    "        StructField(\"ntb_end_time\", TimestampType(), False)\n",
    "    ])\n",
    "\n",
    "    Adv_window_status_df = spark.createDataFrame([(self.campaign_id, self.camp_name, self.execution_ids, \n",
    "                            self.iteration, str(duration), self.exec_status, message, extraction,\n",
    "                            func_name, _az_insert_ts, _az_update_ts, self.ntb_start_time,\n",
    "                            ntb_end_time)], schema)\n",
    "\n",
    "    Adv_window_checksum_df = Adv_window_status_df.withColumn(\"checksum\", md5(concat_ws(\"||\", \n",
    "            lit(self.campaign_id), lit(self.camp_name), lit(self.execution_ids), lit(self.iteration),\n",
    "            lit(duration), lit(self.exec_status), lit(message), lit(extraction),\n",
    "            lit(func_name), lit(_az_insert_ts), lit(_az_update_ts),\n",
    "            lit(self.ntb_start_time), lit(ntb_end_time)\n",
    "        )))\n",
    "    \n",
    "    checksum_col = Adv_window_checksum_df.select(\"checksum\").collect()[0][0]\n",
    "    exec_run_id_col = self.execution_ids\n",
    "\n",
    "    self.adv_campaign_computation_window_status(self.campaign_id, self.camp_name, self.execution_ids, self.iteration, duration, self.exec_status, self.ntb_start_time, ntb_end_time, func_name, message, extraction, _az_insert_ts, _az_update_ts, checksum_col, exec_run_id_col)\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "class MockRow:\n",
    "    def __init__(self, checksum):\n",
    "        self.checksum = checksum\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.checksum\n",
    "\n",
    "class TestSentResponseToParent(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.mock_spark = Mock(spec=SparkSession)\n",
    "        self.mock_dataframe = Mock()\n",
    "        self.mock_spark.createDataFrame.return_value = self.mock_dataframe\n",
    "        self.mock_dataframe.withColumn.return_value = self.mock_dataframe\n",
    "        self.mock_dataframe.select.return_value = self.mock_dataframe\n",
    "        \n",
    "        # Use MockRow instead of Mock for the row\n",
    "        mock_row = MockRow(\"test_checksum\")\n",
    "        self.mock_dataframe.collect.return_value = [mock_row]\n",
    "\n",
    "        self.campaign_status = Mock()\n",
    "        self.campaign_status.campaign_id = \"TEST001\"\n",
    "        self.campaign_status.camp_name = \"Test Campaign\"\n",
    "        self.campaign_status.execution_ids = \"EXEC001\"\n",
    "        self.campaign_status.iteration = 1\n",
    "        self.campaign_status.ntb_start_time = datetime.now() - timedelta(minutes=30)\n",
    "        self.campaign_status.adv_campaign_computation_window_status = Mock()\n",
    "        \n",
    "        # Attach the sent_responce_to_parent method to the mock\n",
    "        self.campaign_status.sent_responce_to_parent = sent_responce_to_parent.__get__(self.campaign_status)\n",
    "\n",
    "        # Mock the spark object\n",
    "        self.patcher1 = patch('__main__.spark', self.mock_spark)\n",
    "        self.patcher2 = patch('__main__.md5', Mock(return_value='md5_result'))\n",
    "        self.patcher3 = patch('__main__.concat_ws', Mock(return_value='concat_result'))\n",
    "        self.patcher4 = patch('__main__.lit', Mock(side_effect=lambda x: x))\n",
    "\n",
    "        self.patcher1.start()\n",
    "        self.patcher2.start()\n",
    "        self.patcher3.start()\n",
    "        self.patcher4.start()\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.patcher1.stop()\n",
    "        self.patcher2.stop()\n",
    "        self.patcher3.stop()\n",
    "        self.patcher4.stop()\n",
    "\n",
    "    def test_processing_status(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"Processing\")\n",
    "        self.assertEqual(self.campaign_status.exec_status, \"Processing\")\n",
    "        self.campaign_status.adv_campaign_computation_window_status.assert_called_once()\n",
    "\n",
    "    def test_failed_status(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"Error occurred\")\n",
    "        self.assertEqual(self.campaign_status.exec_status, \"Failed\")\n",
    "        self.campaign_status.adv_campaign_computation_window_status.assert_called_once()\n",
    "\n",
    "    def test_success_status(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        self.assertEqual(self.campaign_status.exec_status, \"Success\")\n",
    "        self.campaign_status.adv_campaign_computation_window_status.assert_called_once()\n",
    "\n",
    "    def test_dataframe_creation(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        self.mock_spark.createDataFrame.assert_called_once()\n",
    "\n",
    "    def test_checksum_calculation(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        self.mock_dataframe.withColumn.assert_called_once()\n",
    "        self.assertEqual(self.mock_dataframe.withColumn.call_args[0][0], \"checksum\")\n",
    "\n",
    "    def test_adv_campaign_computation_window_status_call(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        self.campaign_status.adv_campaign_computation_window_status.assert_called_once()\n",
    "        self.assertEqual(self.campaign_status.adv_campaign_computation_window_status.call_args[0][0], \"TEST001\")\n",
    "\n",
    "    def test_duration_calculation(self):\n",
    "        start_time = datetime.now() - timedelta(minutes=45)\n",
    "        self.campaign_status.ntb_start_time = start_time\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        duration = self.campaign_status.adv_campaign_computation_window_status.call_args[0][4]\n",
    "        self.assertGreater(duration.total_seconds(), 2600)  # 43 minutes in seconds\n",
    "\n",
    "    def test_extraction_message_for_processing(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"Processing\")\n",
    "        extraction = self.campaign_status.adv_campaign_computation_window_status.call_args[0][10]\n",
    "        self.assertEqual(extraction, \"Processing is ongoing.\")\n",
    "\n",
    "    def test_extraction_message_for_failure(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"Error\")\n",
    "        extraction = self.campaign_status.adv_campaign_computation_window_status.call_args[0][10]\n",
    "        self.assertEqual(extraction, \"Can not be extracted due to failure.\")\n",
    "\n",
    "    def test_extraction_message_for_success(self):\n",
    "        self.campaign_status.sent_responce_to_parent(\"test_func\", \"\")\n",
    "        extraction = self.campaign_status.adv_campaign_computation_window_status.call_args[0][10]\n",
    "        self.assertEqual(extraction, \"yet to start.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8e30aba0-e6c8-4359-8973-a579f9c6b0ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n----------------------------------------------------------------------\nRan 10 tests in 0.015s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "#done 6 \n",
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function here, outside of any class\n",
    "def concat_checksum_cols(df, ignore_column_list_md5):\n",
    "    '''\n",
    "    Description : This filters columns in a DataFrame by excluding those listed in ignore_column_list_md5\n",
    "    Parameters : dataframe, ignore_column_list_md5\n",
    "    Return value : columns list(that are not in the ignore_column_list_md5)\n",
    "    '''\n",
    "    bizColList = [col for col in df.columns if (col not in ignore_column_list_md5)]\n",
    "    columnList = []\n",
    "    for column in bizColList:\n",
    "        if column is None:\n",
    "            columnList.append(':')\n",
    "        else:\n",
    "            columnList.append(column)\n",
    "    return columnList\n",
    "\n",
    "class TestConcatChecksumCols(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.ignore_column_list_md5 = ['ignore_col1', 'ignore_col2']\n",
    "\n",
    "    def test_all_columns_included(self):\n",
    "        df = pd.DataFrame({'col1': [1], 'col2': [2], 'col3': [3]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col1', 'col2', 'col3'])\n",
    "\n",
    "    def test_ignore_columns_excluded(self):\n",
    "        df = pd.DataFrame({'col1': [1], 'ignore_col1': [2], 'col3': [3]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col1', 'col3'])\n",
    "\n",
    "    def test_empty_dataframe(self):\n",
    "        df = pd.DataFrame()\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, [])\n",
    "\n",
    "    def test_all_columns_ignored(self):\n",
    "        df = pd.DataFrame({'ignore_col1': [1], 'ignore_col2': [2]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, [])\n",
    "\n",
    "    def test_none_values_replaced(self):\n",
    "        df = pd.DataFrame({'col1': [1], 'col2': [None], 'col3': [3]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col1', 'col2', 'col3'])\n",
    "\n",
    "    def test_mixed_column_types(self):\n",
    "        df = pd.DataFrame({'col1': [1], 'ignore_col1': [2], None: [3], 'col4': [4]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col1', ':', 'col4'])\n",
    "\n",
    "    def test_duplicate_columns(self):\n",
    "        df = pd.DataFrame(columns=['col1', 'col2', 'col1', 'ignore_col1'])\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col1', 'col2', 'col1'])\n",
    "\n",
    "    def test_special_characters_in_columns(self):\n",
    "        df = pd.DataFrame({'col@1': [1], 'col#2': [2], 'ignore_col1': [3]})\n",
    "        result = concat_checksum_cols(df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(result, ['col@1', 'col#2'])\n",
    "\n",
    "    def test_large_number_of_columns(self):\n",
    "        large_df = pd.DataFrame({f'col{i}': [i] for i in range(1000)})\n",
    "        result = concat_checksum_cols(large_df, self.ignore_column_list_md5)\n",
    "        self.assertEqual(len(result), 1000)\n",
    "\n",
    "    def test_ignore_list_empty(self):\n",
    "        df = pd.DataFrame({'col1': [1], 'col2': [2], 'col3': [3]})\n",
    "        result = concat_checksum_cols(df, [])\n",
    "        self.assertEqual(result, ['col1', 'col2', 'col3'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bfcb90-b72b-45a1-869f-1542c60f37b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E.EEEEE\n======================================================================\nERROR: test_empty_config_df (__main__.TestProcessSafegraph.test_empty_config_df)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 141, in test_empty_config_df\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n======================================================================\nERROR: test_multiple_matching_records (__main__.TestProcessSafegraph.test_multiple_matching_records)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 74, in test_multiple_matching_records\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n======================================================================\nERROR: test_no_matching_records (__main__.TestProcessSafegraph.test_no_matching_records)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 62, in test_no_matching_records\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n======================================================================\nERROR: test_null_values_in_join_columns (__main__.TestProcessSafegraph.test_null_values_in_join_columns)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 160, in test_null_values_in_join_columns\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n======================================================================\nERROR: test_partial_matching_records (__main__.TestProcessSafegraph.test_partial_matching_records)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 88, in test_partial_matching_records\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n======================================================================\nERROR: test_successful_join (__main__.TestProcessSafegraph.test_successful_join)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/spark-d7ff5a85-4830-43b8-b111-8b/.ipykernel/133745/command-530765357514773-3738858712\", line 49, in test_successful_join\n    safegraph_df = self.spark.createDataFrame([\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/connect/session.py\", line 654, in createDataFrame\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.\n\n----------------------------------------------------------------------\nRan 7 tests in 0.234s\n\nFAILED (errors=6)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "class AdvertisementCampaign:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "\n",
    "    def process_safegraph(self, safegraph_df, config_df):\n",
    "        '''\n",
    "        Description : This function joins DataFrames (safegraph_df and config_df) on matching location and address fields, filters for non-null POI location names, and selects relevant columns.\n",
    "        Parameters : \n",
    "        - safegraph_df: safegraph_poi_h3 dataframe\n",
    "        - config_df: advertisement_campaign config dataframe\n",
    "        Return value : dataframe\n",
    "        '''\n",
    "        filtered_safegraph = (\n",
    "            safegraph_df\n",
    "            .join(config_df, [\n",
    "                safegraph_df.location_name == config_df.poi_loc_name,\n",
    "                safegraph_df.street_address == config_df.street_addr,\n",
    "                safegraph_df.city == config_df.city,\n",
    "                safegraph_df.province == config_df.province,\n",
    "                safegraph_df.postal_code == config_df.postal_code,\n",
    "                safegraph_df.top_category == config_df.top_category,\n",
    "                safegraph_df.sub_category == config_df.sub_category\n",
    "            ], \"left\")\n",
    "            .filter(config_df.poi_loc_name.isNotNull())\n",
    "            .select(\n",
    "                \"location_name\", \"location_id\", \"brands\", \"latitude\", \"longitude\", \"location_perimeter\",\n",
    "                lit(None).alias(\"location_radius\"), \"street_address\", safegraph_df[\"city\"], safegraph_df[\"province\"], safegraph_df[\"postal_code\"],\n",
    "                safegraph_df[\"top_category\"], safegraph_df[\"sub_category\"], safegraph_df[\"category_tags\"], \"opened_on\", \"closed_on\",\n",
    "                \"iso_country_code\", \"naics_code\", \"census_code\",\n",
    "                \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\",\n",
    "                \"eff_to\", \"eff_from\", \"hexagon_wkt\"\n",
    "            )\n",
    "        )\n",
    "        return filtered_safegraph\n",
    "\n",
    "class TestProcessSafegraph(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Use the pre-existing spark object in Databricks\n",
    "        cls.spark = SparkSession.builder.getOrCreate()\n",
    "        cls.ad_campaign = AdvertisementCampaign(cls.spark)\n",
    "\n",
    "    def test_successful_join(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 1)\n",
    "        self.assertTrue(\"location_name\" in result.columns)\n",
    "\n",
    "    def test_no_matching_records(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store B\", \"456 Elm St\", \"City2\", \"Province2\", \"67890\", \"Services\", \"Banking\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 0)\n",
    "\n",
    "    def test_multiple_matching_records(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None),\n",
    "            (\"Store B\", \"456 Elm St\", \"City2\", \"Province2\", \"67890\", \"Services\", \"Banking\", \"Tag3\", 41.8781, -87.6298, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\"),\n",
    "            (\"Store B\", \"456 Elm St\", \"City2\", \"Province2\", \"67890\", \"Services\", \"Banking\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 2)\n",
    "\n",
    "    def test_partial_matching_records(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None),\n",
    "            (\"Store B\", \"456 Elm St\", \"City2\", \"Province2\", \"67890\", \"Services\", \"Banking\", \"Tag3\", 41.8781, -87.6298, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 1)\n",
    "\n",
    "    def test_empty_safegraph_df(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"location_name\", StringType(), True),\n",
    "            StructField(\"street_address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"province\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"top_category\", StringType(), True),\n",
    "            StructField(\"sub_category\", StringType(), True),\n",
    "            StructField(\"category_tags\", StringType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"location_id\", StringType(), True),\n",
    "            StructField(\"brands\", StringType(), True),\n",
    "            StructField(\"location_perimeter\", StringType(), True),\n",
    "            StructField(\"opened_on\", StringType(), True),\n",
    "            StructField(\"closed_on\", StringType(), True),\n",
    "            StructField(\"iso_country_code\", StringType(), True),\n",
    "            StructField(\"naics_code\", StringType(), True),\n",
    "            StructField(\"census_code\", StringType(), True),\n",
    "            StructField(\"hexagon_id\", StringType(), True),\n",
    "            StructField(\"cellid\", StringType(), True),\n",
    "            StructField(\"site_name\", StringType(), True),\n",
    "            StructField(\"sitecode\", StringType(), True),\n",
    "            StructField(\"opened_no_later_than\", StringType(), True),\n",
    "            StructField(\"tracking_closed_since\", StringType(), True),\n",
    "            StructField(\"eff_to\", StringType(), True),\n",
    "            StructField(\"eff_from\", StringType(), True),\n",
    "            StructField(\"hexagon_wkt\", StringType(), True),\n",
    "            StructField(\"location_radius\", StringType(), True)\n",
    "        ])\n",
    "        safegraph_df = self.spark.createDataFrame([], schema)\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 0)\n",
    "\n",
    "    def test_empty_config_df(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", \"123 Main St\", \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"poi_loc_name\", StringType(), True),\n",
    "            StructField(\"street_addr\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"province\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"top_category\", StringType(), True),\n",
    "            StructField(\"sub_category\", StringType(), True)\n",
    "        ])\n",
    "        config_df = self.spark.createDataFrame([], schema)\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 0)\n",
    "\n",
    "    def test_null_values_in_join_columns(self):\n",
    "        safegraph_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", None, \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\", \"Tag1,Tag2\", 40.7128, -74.0060, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
    "        ], [\"location_name\", \"street_address\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\", \"category_tags\", \"latitude\", \"longitude\", \"location_id\", \"brands\", \"location_perimeter\", \"opened_on\", \"closed_on\", \"iso_country_code\", \"naics_code\", \"census_code\", \"hexagon_id\", \"cellid\", \"site_name\", \"sitecode\", \"opened_no_later_than\", \"tracking_closed_since\", \"eff_to\", \"eff_from\", \"hexagon_wkt\", \"location_radius\"])\n",
    "\n",
    "        config_df = self.spark.createDataFrame([\n",
    "            (\"Store A\", None, \"City1\", \"Province1\", \"12345\", \"Retail\", \"Grocery\")\n",
    "        ], [\"poi_loc_name\", \"street_addr\", \"city\", \"province\", \"postal_code\", \"top_category\", \"sub_category\"])\n",
    "\n",
    "        result = self.ad_campaign.process_safegraph(safegraph_df, config_df)\n",
    "        self.assertEqual(result.count(), 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e2f67a-33b7-4c29-819e-9a48b37989e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3169838804236636,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Uc1 new",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
