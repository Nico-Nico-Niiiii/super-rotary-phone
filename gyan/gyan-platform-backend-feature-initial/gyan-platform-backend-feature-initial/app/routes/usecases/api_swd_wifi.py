# from fastapi import APIRouter, File, UploadFile, HTTPException, Form
# from fastapi.responses import JSONResponse, FileResponse

# from src.software_debugger.wifi.infer import WifiInfer
# from src.software_debugger.wifi.anomaly_detector import AnomalyDetector

# from core.utils import load_endpoints, get_prefix

# from scapy.all import *
# from scapy.layers.dot11 import *

# import os
# import sys
# import csv
# import time
# import json
# import torch
# import datetime
# import pandas as pd
# from decimal import Decimal

# from transformers import BertTokenizer, BertForSequenceClassification
# from peft import PeftModel, PeftConfig
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# from concurrent.futures import ThreadPoolExecutor 
# import tqdm as tqdm


# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# ENDPOINTS = load_endpoints()
# router_wifi = APIRouter(prefix=ENDPOINTS["software_debugger"])

# folder_name = 'uploads'
# current_timestamp = datetime.now()

# timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")

# Packets = []

# # Create new folder
# # app.config['UPLOAD_FOLDER'] = f"temp/{folder_name}_{timestamp_str}"
# # if not os.path.exists(app.config['UPLOAD_FOLDER']):
# #     os.makedirs(app.config['UPLOAD_FOLDER'])

# UPLOAD_FOLDER = f"temp/{folder_name}_{timestamp_str}"  # Edit this to work with config path

# """
# 1. Upload fles -- upload pacets - hostapd fle & pact fle
# 2. Run Analyss -- uses only hostapd fle
# 3. Pacet analyss -- uses only pcap fle -- returns pcer summary for error logs generated by run anaylsus
# 4. Export report -- download falure csv 
# """

# def process_packets(text_list, output):

#     global Packets

#     station_list = set()
#     list_ts = []

#     for i in range(len(text_list)):
#         ts = text_list[i].split('.')[0]
#         try:
#             ts = int(ts)
#         except Exception as e:
#             continue
        
#         if 'NORMAL' in output[i]:
#             continue

#         split_for_mac = text_list[i].split(' ')
    
#         for t in split_for_mac:
#             if len(t) == 17 and t.count(':') == 5:
#                 station_list.add(t)
    
#         list_ts.append([ts, text_list[i]])

#     packets = []
#     cnt = 0

#     for radio_pkt in Packets:
#         if radio_pkt.haslayer(Dot11WEP):
#             dot11_wep_layer = radio_pkt.getlayer(Dot11WEP)

#             # Check for the WPA Information Element ID (IE) field
#             if 221 in dot11_wep_layer and dot11_wep_layer[221].oui == 0x00FAC and dot11_wep_layer[221].subtype == 13:
#                 # The reason code is typically located at a specific offset within the IE field
#                 reason_code_offset = 23  # Adjust this offset based on the actual structure of your frames

#                 # Extract the reason code
#                 reason_code = dot11_wep_layer[221].info[reason_code_offset]
#                 packets.append({"Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": reason_code})
        
#         if radio_pkt.haslayer(Dot11):
#             consider = 0
#             for mac in station_list:
#                 if mac in radio_pkt.summary():
#                     consider = 1
#                 if consider == 0:
#                     continue
            
#             Logs = []
            
#             if radio_pkt.type == 0 and (radio_pkt.subtype == 10 or radio_pkt.subtype == 12):
#                 for time_text in list_ts:
#                     print(radio_pkt.time, time_text[0])
#                     if ((Decimal(radio_pkt.time) - Decimal(time_text[0]))<10) and ((Decimal(radio_pkt.time) - Decimal(time_text[0])) > -10):
#                         Logs.append(time_text[1])
#                         cnt += 1
#             if cnt > 0:
#                 cnt = 0
#             else:
#                 continue
                
#             if radio_pkt.type == 0 and radio_pkt.subtype == 10:  # Type: Management, Subtype: Disassociation
#                 try:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
#                 except Exception as e:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
#             if radio_pkt.type == 0 and radio_pkt.subtype == 12:  # Type: Management, Subtype: Disassociation
#                 try:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
#                 except Exception as e:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
#     return packets


# # API route for running analysis
# @router_wifi.post(ENDPOINTS["software_debugger"]["wifi"]["routes"]["generate"])
# async def run_analysis(
#     domain: str = Form(...), 
#     option: str = Form(...), 
#     hapd_file: UploadFile = File(...)
#     ):
    
#     infer = WifiInfer()
#     sw_infer = AnomalyDetector()

#     # selected_radio = request.form['domain']
#     # selected_option = request.form['option']
#     # hapd_file = request.files['hapd_file']

#     # print(upload_folder)

#     #file = request.files['hapd_file']
#     # print("enterrrrr")
#     # Get the uploaded files
#     # start_time = time.time() 
#     # print("response:",start_time)
#     # print(f'Radio = {selected_radio} Option = {selected_option}')
#     # print(f'Filename {hapd_file.filename}')

#     # hapd_file = request.files['hapd_file']
    
#     if hapd_file.filename == '':
#         return HTTPException(status_code = 400, detail='No selected file')
    
#     if not hapd_file.filename.endswith(".txt"):
#         raise HTTPException(status_code=400, detail="Invalid file format")
        
#     # Save the files to the upload folder
#     hapd_file_path = os.path.join(UPLOAD_FOLDER, hapd_file.filename)
#     with open(hapd_file_path, "wb") as f:
#         f.write(await hapd_file.read())
    

#     if domain == "wifi":
#         if option == '10':
            
#             print("Under 10")
#             ##### EDIT PATH HERE
#             model_path = f'{sw_infer.model_path}'  # Point to the model file #user input
#             model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)  # Initialize a new model
#             model.load_state_dict(torch.load(model_path))  # Load the model's state dictionary
#             model.eval() 
#             tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
#             print("Under 10")

#             # with open(hapd_file_path, "r") as file_fp:
#             #     data = file_fp.readlines()

#             # df = pd.DataFrame()
#             err_lst = list()

#             lst = ["AP-STA-DISCONNECTED",
#                 "deauthenticated due to local deauth request",
#                 "IEEE 802.11: disassociated",
#                 "WPA: invalid MIC",
#                 "Kernel Crash",
#                 "HWWD Reset",
#                 "WLAN Dongle Firmware crash",
#                 "Power On reset",
#                 "PCD reboot"]

#             explain = ["Station Disconnected", 
#                     "Deauthentication", 
#                     "Disassociation",  
#                     "Invalid Password",
#                     "LastRebootReason Kernel Crash",
#                     "LastRebootReasonHardware Reset",
#                     "LastRebootReason Firmware Dongle crash", 
#                     "LastRebootReason Power On reset",
#                     "LastRebootReason PCD reboot"]

#             with open(hapd_file_path, "r") as file_fp:
#                 data = file_fp.readlines()
            
#             # data = file_fp.readlines()

#             for i in tqdm(range(len(data))):
#                 l = data[i]
#                 l = l.decode("latin-1")
#                 l.replace("\r\n", "")
#                 for err_code in lst:
#                     if err_code in l:
#                         desc = explain[lst.index(err_code)]
#                         err_lst.append([l, 1, desc])
#                         break
#                     else:
#                         err_lst.append([l, 0, "NORMAL"])
                        
#             df = pd.DataFrame(err_lst)
#             df.columns = ["Logs", "Labels", "LabelExplanation"]
                
#             df = df[df['Logs'].str.strip() != '']
#                 # Reset the index after removing rows
#             df = df.reset_index(drop=True)
#             df.to_csv("Hapd_Logs_infer.csv")

#                 # Create a new input text by combining log, TemplateMined, and Cluster_ID
#             df["input_text"] = df["Logs"].astype(str) #+ "," + data["TemplateMined"] + "," + data["Cluster_ID"]
#             # print("Line 2:")

#                 # Define manual encoding and decoding mappings
#             label_to_index = {'Deauthentication': 0, 'Disassociation': 1, 'Invalid Password': 2, 'NORMAL': 3, 'Station Disconnected': 4}

#                 # Replace the 'LabelExplanation' column with the numerical labels
#             df['LabelExplanation'] = df['LabelExplanation'].map(label_to_index)

#                 # Display the mapping of original labels to numerical indices
#             label_mapping = {label: label_to_index[label] for label in label_to_index}
        
#             index_to_label = {index: label for label, index in label_to_index.items()}

        
#             input_encodings = tokenizer(df["input_text"].tolist(), truncation=True, padding="max_length", max_length=128, return_tensors='pt')
                
#             inference_dataset = torch.utils.data.TensorDataset(input_encodings["input_ids"], input_encodings["attention_mask"])

#             inference_loader = torch.utils.data.DataLoader(inference_dataset, batch_size=16)

#                 # Perform inference
#             predicted_labels = []
            
#             with torch.no_grad():
                
#                 def inference(batch):
#                     inputs = {"input_ids": batch[0], "attention_mask": batch[1]}
#                     outputs = model(**inputs)
#                     batch_predictions = torch.argmax(outputs.logits, dim=1)
#                     return batch_predictions.tolist()

#                 # Use ThreadPoolExecutor to parallelize inference
#                 with ThreadPoolExecutor(max_workers=6) as executor:  # Set the number of threads (adjust as needed)
#                     # Submit the inference tasks for each batch
#                     futures = [executor.submit(inference, batch) for batch in inference_loader]

#                     # Wait for all threads to complete and get the results
#                     results = [future.result() for future in tqdm(futures, total=len(futures))]


#             # Flatten the list of results
#             predicted_labels = [label for sublist in results for label in sublist]

#             # Decode numerical labels to actual anomaly classes
#             predicted_classes = [index_to_label[index] for index in predicted_labels]

#             # Add predicted classes to the DataFrame
#             df["PredictedClasses"] = predicted_classes

#             total_logs = len(df) #total_logs
#             found_anomalous_log= len(df[df['PredictedClasses'] != 'NORMAL'])
#             found_normal_log = len(df[df['PredictedClasses'] == 'NORMAL'])
#             Disassoc = len(df[df['PredictedClasses'] == 'Disassociation'])
#             Deauth = len(df[df['PredictedClasses'] == 'Deauthentication'])
#             Disconnect = len(df[df['PredictedClasses'] == 'Station Disconnected'])
#             g = len(df[df['PredictedClasses'] == 'Invalid Password'])
#             anomaly_label = df[df['PredictedClasses'] != 'NORMAL']
#             anomaly_logs = anomaly_label['Logs']
#             # print(type(anomaly_logs))
#             anomaly_logs_2 = anomaly_logs.str.cat(sep='\n\n\n') + '\n\n\n'
#             normal = df[df['PredictedClasses'] == 'NORMAL']
#             normal_logs = normal['Logs']
#             responses = anomaly_label['PredictedClasses']
#             # print('Responses =  ', responses)
#             responses = responses.str.cat(sep='\n\n\n\n\n') + '\n\n\n\n\n'
#             percent_anom = (found_anomalous_log/total_logs) * 100
#             percent_norm = 100 - percent_anom

#             EPL = 0
#             for i in normal['Logs']:
#                 if "WPA: received EAPOL-Key frame" in i or "4-Way Handshake" in i:
#                     EPL += 1


#             df.to_csv('Wifi_results.csv')
#             responses_2 = responses
            
#         elif option == '11': 
#             # Load peft config for pre-trained checkpoint etc.
#             logs = []
#             normal_logs = []
#             anomaly_logs = []
#             responses = []
#             Disassoc = 0
#             Deauth = 0
#             Disconnect = 0
#             EPL = 0
#             print("Under 11")

#             #pcap = rdpcap(packet_file)
            
#             print("Model loading...")
#             entire_logs, out = infer.ai_infer(hapd_file_path)
#             print("Model loaded and analysed log file")
           
#             i = 0
#             correct = 0
#             sum_time = 0
#             # print("at 328")

            
#             for i in range(len(entire_logs)):
#                 if out[i] == 'NORMAL':
#                     normal_logs.append({"status": "Normal", "text":entire_logs[i]})
#                     logs.append({"status": "Normal", "text":entire_logs[i]})
#                 else:
#                     logs.append({"status": "Anomaly", "text":entire_logs[i]})
#                     anomaly_logs.append({"status": "Anomaly", "text":entire_logs[i]})
#                     responses.append(out)
#                     if out[i] == 'Disassociation':
#                         Disassoc +=1
#                     elif out[i] == 'Deauthentication':
#                         Deauth +=1
#                     elif out[i] == 'Station Disconnected':
#                         Disconnect += 1

#             # print(Disassoc, Deauth, Disconnect)
                    
#             found_anomalous_log = len(anomaly_logs)
#             found_normal_log = len(normal_logs)
#             total_logs = found_anomalous_log + found_normal_log
#             percent_disassoc = (Disassoc/total_logs) * 100
#             percent_disassoc = round(percent_disassoc, 2)

#             percent_deauth = (Deauth/total_logs) * 100
#             percent_deauth = round(percent_deauth, 2)

#             percent_disconnect = (Disconnect/total_logs) * 100
#             percent_disconnect = round(percent_disconnect, 2)

#             percent_anom = (found_anomalous_log/total_logs) * 100
#             percent_norm = 100 - percent_anom

#             percent_norm = round(percent_norm, 2)
#             anomaly_logs_2 = ' '
#             responses_2 = ' '

#             # for i in anomaly_logs:
#             #     anomaly_logs_2 += i
#             #     anomaly_logs_2 += '\n\n\n'
#             # for i in responses:
#             #     responses_2 += i 
#             #     responses_2 += '\n\n\n\n\n'
                
            
            
#         # end_time_wifi = time.time()
#         # elapsed_time_wifi = end_time_wifi - start_time
#         elapsed_time_wifi = round(elapsed_time_wifi, 2)
#         show_btn_container = True
#         percentage=((Disassoc+Deauth+Disconnect)/(Disassoc+Deauth+Disconnect+found_anomalous_log+found_normal_log))*100

#         # print(percentage)

#         result_3 = round(percentage, 1)

#         # print(result_3)

#         percent_str = str(result_3) + "%"
#         response = {
#             "Placeholder1": elapsed_time_wifi,
#             "Placeholder2": total_logs,
#             "Placeholder3": found_anomalous_log,
#             "Placeholder4": found_normal_log,
#             "Placeholder5": Disassoc,
#             "Placeholder6": Deauth,
#             "Placeholder7": Disconnect,
#             "Placeholder8": percent_str,
#             "Placeholder9": percent_disassoc,
#             "Placeholder10": percent_deauth,
#             "Placeholder11": percent_disconnect,
#             "Placeholder12": percent_norm,
#             "total_logs": entire_logs,
#             "output": out,
#             "anomalies": anomaly_logs,
#             "logs": logs
#         }
        
#         return JSONResponse(content = response)
    
# @router_wifi.post(ENDPOINTS["software_debugger"]["wifi"]["routes"]["packet_analysis"])
# async def packet_analysis(
#     upload_name: str = Form(...),
#     logs: UploadFile = File(...),
#     output: UploadFile = File(...)
# ):

#     print("Packets processing!!")

#     # upload_folder = request.form['upload_name']
#     # packets = request.form['packets']

#     # for file in (os.listdir(upload_folder)):
#     #     if (file.endswith(".txt") == False):
#     #         packet_file = f"{upload_folder}/{file}"

#     # logs = json.loads(request.files['logs'].read())
#     # output = json.loads(request.files['output'].read())

#     logs_data = json.loads(await logs.read())
#     output_data = json.loads(await output.read())


#     #To process the packets and obtain result about failure packets
#     failure_packets = process_packets(logs_data, output_data)      
#     print("Packets processed")

#     # pprint.pprint(failure_packets)

#     current_timestamp = datetime.now()
#     timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")

#     filename = f"failure_packets_{timestamp_str}.csv"
#     csv_file_path = f'./temp/{filename}'

#     # CSV file path
#     if failure_packets:
#         # Open the CSV file in write mode
#         with open(csv_file_path, mode='w', newline='') as csv_file:
#             # Define the fieldnames based on the keys of the dictionaries
#             fieldnames = failure_packets[0].keys()

#             # Create a CSV DictWriter object
#             writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

#             # Write the header row
#             writer.writeheader()

#             # Write each dictionary as a row in the CSV file
#             for row in failure_packets:
#                 writer.writerow(row)

#     return JSONResponse(content={"filename": filename, "failure_packets": failure_packets})

# @router_wifi.post(ENDPOINTS["software_debugger"]["wifi"]["routes"]["export"])
# def export_report(filename : str = Form(...)):
#     # filename = request.form['filename']

#     folder_path = "./temp"
#     file_path = f'{folder_path}/{filename}'

#     if(os.path.isfile(file_path)):
#         print("File present")
#         return FileResponse(file_path, filename=filename, media_type="application/octet-stream")
    
#     else:
#         raise HTTPException(status_code = 400, detail="File not present")
    
# @router_wifi.post(ENDPOINTS["software_debugger"]["wifi"]["routes"]["packet_loading"])
# async def uplaod_files(
#     hapd_file : UploadFile = File(...), 
#     pcap_file : UploadFile = File(...) 
# ):
    
#     if hapd_file.filename == "" or pcap_file.filename == "":
#         raise HTTPException(status_code=400, detail="No selected file")

#     hapd_file_path = os.path.join(UPLOAD_FOLDER, hapd_file.filename)
#     pcap_file_path = os.path.join(UPLOAD_FOLDER, pcap_file.filename)
    
#     # Save the files to the upload folder
#     # if hapd_file.filename.endswith('.txt'):
#     #     hapd_file_path =  os.path.join(app.config['UPLOAD_FOLDER'], hapd_file.filename)
#     #     pcap_file_path = os.path.join(app.config['UPLOAD_FOLDER'], pcap_file.filename)
#     #     hapd_file.save(hapd_file_path)
#     #     pcap_file.save(pcap_file_path)
#     # else:
#     #     return jsonify({'error': 'Invalid file format'}), 400

#     with open(hapd_file_path, "wb") as f:
#         f.write(await hapd_file.read())

#     with open(pcap_file_path, "wb") as f:
#         f.write(await pcap_file.read())
    
#     # packets = rdpcap(pcap_file_path)

#     # global Packets

#     # Packets = packets

#     return JSONResponse(content={"upload_name": UPLOAD_FOLDER})

################################################################################
# from fastapi import File, UploadFile, HTTPException, Form, APIRouter
# from fastapi.responses import JSONResponse, FileResponse

# import os
# import time

# from scapy.all import *
# from scapy.layers.dot11 import *

# import sys
# import os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# import torch
# import pandas as pd
# import time
# import tqdm as tqdm
# import json
# import csv
# # import pprint
# from datetime import datetime
# from decimal import Decimal
# from concurrent.futures import ThreadPoolExecutor 

# from app.usecases.software_debugger.ai_infer import Infer 
# from app.core.utils import load_endpoints, get_prefix

# ENDPOINTS = load_endpoints()

# router_swd = APIRouter(prefix=ENDPOINTS["usecases"]["software_debugger"]["prefix"], tags=["swd_wifi"])


# folder_name = 'uploads'
# current_timestamp = datetime.now()
# timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")


# #Create new folder
# UPLOAD_FOLDER = f"temp/{folder_name}_{timestamp_str}"
# if not os.path.exists(UPLOAD_FOLDER):
#     os.makedirs(UPLOAD_FOLDER)

# infer = Infer()

# def process_packets(Packets, text_list, output, list_ts):

#     station_list = set()
#     ts_list = []

#     for i in range(len(text_list)):
#         if 'Normal, This is Normal log' in output[i]:
#             continue

#         split_for_mac = text_list[i].split(' ')
    
#         for t in split_for_mac:
#             if len(t) == 17 and t.count(':') == 5:
#                 station_list.add(t)

#         ts_list.append([list_ts[i], text_list[i]])

#     packets = []
#     cnt = 0

#     for radio_pkt in Packets:
#         if radio_pkt.haslayer(Dot11WEP):
#             dot11_wep_layer = radio_pkt.getlayer(Dot11WEP)

#             # Check for the WPA Information Element ID (IE) field
#             if 221 in dot11_wep_layer and dot11_wep_layer[221].oui == 0x00FAC and dot11_wep_layer[221].subtype == 13:
#                 # The reason code is typically located at a specific offset within the IE field
#                 reason_code_offset = 23  # Adjust this offset based on the actual structure of your frames

#                 # Extract the reason code
#                 reason_code = dot11_wep_layer[221].info[reason_code_offset]
#                 packets.append({"Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": reason_code})
        
#         if radio_pkt.haslayer(Dot11):
#             consider = 0
#             for mac in station_list:
#                 if mac in radio_pkt.summary():
#                     consider = 1
#                 if consider == 0:
#                     continue
#             Logs = []
#             if radio_pkt.type == 0 and (radio_pkt.subtype == 10 or radio_pkt.subtype == 12):
#                 for time_text in ts_list:
#                     # print(radio_pkt.time, time_text[0])
#                     if ((Decimal(radio_pkt.time) - time_text[0])<10) and ((Decimal(radio_pkt.time) - time_text[0]) > -10):
#                         Logs.append(time_text[1])
#                         cnt += 1
#             if cnt > 0:
#                 cnt = 0
#             else:
#                 continue
                
#             if radio_pkt.type == 0 and radio_pkt.subtype == 10:  # Type: Management, Subtype: Disassociation
#                 try:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
#                 except Exception as e:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
#             if radio_pkt.type == 0 and radio_pkt.subtype == 12:  # Type: Management, Subtype: Disassociation
#                 try:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
#                 except Exception as e:
#                     packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
#     return packets


# # API route for running analysis
# @router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["run"])
# async def run_analysis(
#     selected_radio: str = Form(...), 
#     upload_folder: str = Form(...),
#     selected_option: str = Form(...)
# ): 
#     # sw_infer = AnomalyDetector()
#     # print(f'Radio = {selected_radio}, Option = {selected_option}')

#     for file in (os.listdir(upload_folder)):
#         if (file.endswith(".txt")):
#              hapd_file_path = f"{upload_folder}/{file}"
    
#     if selected_radio == "wifi":
#         if selected_option == '11':# Load peft config for pre-trained checkpoint etc.
#             logs = []
#             normal_logs = []
#             anomaly_logs = []
#             responses = []
#             Disassoc = 0
#             Deauth = 0
#             Disconnect = 0
#             EPL = 0

#             #pcap = rdpcap(packet_file)
            
#             print("Model loading...")
#             start_time = time.time()
#             entire_logs, out, list_ts = infer.aix_infer(hapd_file_path)
#             end_time_wifi = time.time()
#             print("Model loaded and analysed log file")
           
#             i = 0
#             correct = 0
#             sum_time = 0
#             # print("at 328")

#             results = []

#             for i in range(len(out)):
#                 if(out[i] == "Normal, This is Normal log") == False:
#                     results.append({"Text": entire_logs[i], "Response": out[i]})

#             # CSV file path
#             if (len(results) > 0):
#                 csv_file_path = f'failure_logs_report.csv'

#                 # Open the CSV file in write mode
#                 with open(csv_file_path, mode='w', newline='') as csv_file:
#                     # Define the fieldnames based on the keys of the dictionaries
#                     fieldnames = results[0].keys()

#                     # Create a CSV DictWriter object
#                     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

#                     # Write the header row
#                     writer.writeheader()

#                     # Write each dictionary as a row in the CSV file
#                     for row in results:
#                         writer.writerow(row)

            
#             for i in range(len(entire_logs)):
#                 # print(f"Reading text: {text}")
#                 # input_ids = tokenizer(text, return_tensors="pt", truncation=True).input_ids.cuda()
#                 # # print("at 335")
#                 # s = time.time()
#                 # outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9)
#                 # # print("at 338")
#                 # e = time.time()
#                 # sum_time = sum_time + (e - s)
#                 # # print(sum_time)

#                 # ## Out is the predicted output.
#                 # out = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]
               
#                 if out[i] == 'Normal, This is Normal log':
#                     normal_logs.append({"status": "Normal", "text":entire_logs[i]})
#                     logs.append({"status": "Normal", "text":entire_logs[i]})
#                 else:
#                     logs.append({"status": "Anomaly", "text":entire_logs[i]})
#                     anomaly_logs.append({"status": "Anomaly", "text":entire_logs[i]})
#                     responses.append(out)
#                     if out[i] == 'Disassociated':
#                         Disassoc +=1
#                     elif out[i] == 'Deauthenticate':
#                         Deauth +=1
#                     elif out[i] == 'Disconnected':
#                         Disconnect += 1

#             # print(Disassoc, Deauth, Disconnect)
                    
#             found_anomalous_log = len(anomaly_logs)
#             found_normal_log = len(normal_logs)
#             total_logs = found_anomalous_log + found_normal_log
#             percent_disassoc = (Disassoc/total_logs) * 100
#             percent_disassoc = round(percent_disassoc, 2)

#             percent_deauth = (Deauth/total_logs) * 100
#             percent_deauth = round(percent_deauth, 2)

#             percent_disconnect = (Disconnect/total_logs) * 100
#             percent_disconnect = round(percent_disconnect, 2)

#             percent_anom = (found_anomalous_log/total_logs) * 100
#             percent_norm = 100 - percent_anom

#             percent_norm = round(percent_norm, 2)
#             anomaly_logs_2 = ' '
#             responses_2 = ' '

#             # for i in anomaly_logs:
#             #     anomaly_logs_2 += i
#             #     anomaly_logs_2 += '\n\n\n'
#             # for i in responses:
#             #     responses_2 += i 
#             #     responses_2 += '\n\n\n\n\n'
                
#         elapsed_time_wifi = end_time_wifi - start_time
#         elapsed_time_wifi = round(elapsed_time_wifi, 2)
#         show_btn_container = True
#         percentage=((Disassoc+Deauth+Disconnect)/(Disassoc+Deauth+Disconnect+found_anomalous_log+found_normal_log))*100

#         # print(percentage)

#         result_3 = round(percentage, 1)

#         # print(result_3)

#         percent_str = str(result_3) + "%"
#         response = {
#             "Placeholder1": elapsed_time_wifi,
#             "Placeholder2": total_logs,
#             "Placeholder3": found_anomalous_log,
#             "Placeholder4": found_normal_log,
#             "Placeholder5": Disassoc,
#             "Placeholder6": Deauth,
#             "Placeholder7": Disconnect,
#             "Placeholder8": percent_str,
#             "Placeholder9": percent_disassoc,
#             "Placeholder10": percent_deauth,
#             "Placeholder11": percent_disconnect,
#             "Placeholder12": percent_norm,
#             "total_logs": entire_logs,
#             "output": out,
#             "anomalies": anomaly_logs,
#             "logs": logs,
#             "list_ts": list_ts
#         }
#         return JSONResponse(content=response)


# @router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["packet_analysis"])
# async def packet_analysis(
#     upload_name: str = Form(...),
#     logs: UploadFile = File(...),
#     output: UploadFile = File(...),
#     list_ts: UploadFile = File(...)
# ):
#     print("Packets processing!!")
    
#     # Save the uploaded files
#     upload_folder = upload_name
#     print(upload_folder)
    
#     packet_file = None
#     for file in os.listdir(upload_folder):
#         if not file.endswith(".txt"):
#             packet_file = os.path.join(upload_folder, file)

#     if packet_file is None:
#         return JSONResponse(content={"error": "Packet file not found"}, status_code=400)

#     logs_data = json.loads(await logs.read())
#     output_data = json.loads(await output.read())
#     list_ts_data = json.loads(await list_ts.read())

#     # To load packet
#     print("Packet is loading....")
#     packets = []
#     pcap = rdpcap(packet_file)
#     print("Packet loaded")

#     # print(logs_data)
#     # print(output_data)

#     # To process the packets and obtain result about failure packets
#     failure_packets = process_packets(pcap, logs_data, output_data, list_ts_data)
#     print("Packets processed")
    
#     # pprint.pprint(failure_packets)

#     current_timestamp = datetime.now()
#     timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")
#     filename = f"failure_packets_{timestamp_str}.csv"

#     # CSV file path
#     if len(failure_packets) > 0:
#         csv_file_path = f'/media/sahil/data1/gyan_backend/temp_packets/temp_packets/{filename}'

#         # Open the CSV file in write mode
#         with open(csv_file_path, mode='w', newline='') as csv_file:
#             # Define the fieldnames based on the keys of the dictionaries
#             fieldnames = failure_packets[0].keys()

#             # Create a CSV DictWriter object
#             writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

#             # Write the header row
#             writer.writeheader()

#             # Write each dictionary as a row in the CSV file
#             for row in failure_packets:
#                 writer.writerow(row)

#     return {
#         "filename": filename,
#         "failure_packets": failure_packets
#     }


# @router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["export"])
# async def export_report(filename: str = Form(...)):
#     folder_path = "/media/sahil/data1/usecases/gyan-platform-backend/temp_packets"
#     file_path = os.path.join(folder_path, filename)

#     if os.path.isfile(file_path):
#         print("File present")
#         return FileResponse(file_path, media_type='application/octet-stream', filename=filename)
#     else:
#         raise HTTPException(status_code=400, detail="File not present")
 
    
# @router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["packet_loading"])
# async def upload_files(
#     hapd_file: UploadFile = File(...),  # Required file
#     packet_file: UploadFile = File(...)  # Required file
# ):
#     # Check if filenames are empty
#     if hapd_file.filename == '' or packet_file.filename == '':
#         raise HTTPException(status_code=400, detail="No selected file")

#     # Check file format of hapd_file
#     if not hapd_file.filename.endswith('.txt'):
#         raise HTTPException(status_code=400, detail="Invalid file format. Expected .txt")

#     # Create the upload folder if it doesn't exist
#     if not os.path.exists(UPLOAD_FOLDER):
#         os.makedirs(UPLOAD_FOLDER)

#     # Save the files
#     hapd_file_path = os.path.join(UPLOAD_FOLDER, hapd_file.filename)
#     pcap_file_path = os.path.join(UPLOAD_FOLDER, packet_file.filename)

#     with open(hapd_file_path, "wb") as hapd_f, open(pcap_file_path, "wb") as pcap_f:
#         hapd_f.write(await hapd_file.read())
#         pcap_f.write(await packet_file.read())

#     # Return a JSON response
#     return JSONResponse(content={"upload_name": UPLOAD_FOLDER})


from fastapi import File, UploadFile, HTTPException, Form, APIRouter
from fastapi.responses import JSONResponse, FileResponse

import os
import time

from scapy.all import *
from scapy.layers.dot11 import *

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import pandas as pd
import time
import tqdm as tqdm
import json
import csv
# import pprint
from datetime import datetime
from decimal import Decimal
from concurrent.futures import ThreadPoolExecutor 

from app.usecases.software_debugger.ai_infer import Infer 
from app.core.utils import load_endpoints, get_prefix

ENDPOINTS = load_endpoints()

router_swd = APIRouter(prefix=ENDPOINTS["usecases"]["software_debugger"]["prefix"], tags=["swd_wifi"])


folder_name = 'uploads'
current_timestamp = datetime.now()
timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")


#Create new folder
UPLOAD_FOLDER = f"temp/{folder_name}_{timestamp_str}"
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)

infer = Infer()

def process_packets(Packets, text_list, output, list_ts):

    station_list = set()
    ts_list = []

    for i in range(len(text_list)):
        if 'Normal, This is Normal log' in output[i]:
            continue

        split_for_mac = text_list[i].split(' ')
    
        for t in split_for_mac:
            if len(t) == 17 and t.count(':') == 5:
                station_list.add(t)

        ts_list.append([list_ts[i], text_list[i]])

    packets = []
    cnt = 0

    for radio_pkt in Packets:
        if radio_pkt.haslayer(Dot11WEP):
            dot11_wep_layer = radio_pkt.getlayer(Dot11WEP)

            # Check for the WPA Information Element ID (IE) field
            if 221 in dot11_wep_layer and dot11_wep_layer[221].oui == 0x00FAC and dot11_wep_layer[221].subtype == 13:
                # The reason code is typically located at a specific offset within the IE field
                reason_code_offset = 23  # Adjust this offset based on the actual structure of your frames

                # Extract the reason code
                reason_code = dot11_wep_layer[221].info[reason_code_offset]
                packets.append({"Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": reason_code})
        
        if radio_pkt.haslayer(Dot11):
            consider = 0
            for mac in station_list:
                if mac in radio_pkt.summary():
                    consider = 1
                if consider == 0:
                    continue
            Logs = []
            if radio_pkt.type == 0 and (radio_pkt.subtype == 10 or radio_pkt.subtype == 12):
                for time_text in ts_list:
                    # print(radio_pkt.time, time_text[0])
                    if ((Decimal(radio_pkt.time) - time_text[0])<10) and ((Decimal(radio_pkt.time) - time_text[0]) > -10):
                        Logs.append(time_text[1])
                        cnt += 1
            if cnt > 0:
                cnt = 0
            else:
                continue
                
            if radio_pkt.type == 0 and radio_pkt.subtype == 10:  # Type: Management, Subtype: Disassociation
                try:
                    packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
                except Exception as e:
                    packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
            if radio_pkt.type == 0 and radio_pkt.subtype == 12:  # Type: Management, Subtype: Disassociation
                try:
                    packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": radio_pkt.reason})
                except Exception as e:
                    packets.append({"Logs": Logs, "Packet_time": radio_pkt.time, "Packet_summary": radio_pkt.summary(),"Reason Code": "unknown"})
    return packets


# API route for running analysis
@router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["run"])
async def run_analysis(
    selected_radio: str = Form(...), 
    upload_folder: str = Form(...),
    selected_option: str = Form(...)
): 
    # sw_infer = AnomalyDetector()
    # print(f'Radio = {selected_radio}, Option = {selected_option}')

    for file in (os.listdir(upload_folder)):
        if (file.endswith(".txt")):
             hapd_file_path = f"{upload_folder}/{file}"
    
    if selected_radio == "wifi":
        if selected_option == '11':# Load peft config for pre-trained checkpoint etc.
            logs = []
            normal_logs = []
            anomaly_logs = []
            responses = []
            Disassoc = 0
            Deauth = 0
            Disconnect = 0
            EPL = 0

            #pcap = rdpcap(packet_file)
            
            print("Model loading...")
            start_time = time.time()
            entire_logs, out, list_ts = infer.ai_infer(hapd_file_path)
            end_time_wifi = time.time()
            print("Model loaded and analysed log file")
           
            i = 0
            correct = 0
            sum_time = 0
            # print("at 328")

            results = []

            for i in range(len(out)):
                if(out[i] == "Normal, This is Normal log") == False:
                    results.append({"Text": entire_logs[i], "Response": out[i]})

            # CSV file path
            if (len(results) > 0):
                csv_file_path = f'failure_logs_report.csv'

                # Open the CSV file in write mode
                with open(csv_file_path, mode='w', newline='') as csv_file:
                    # Define the fieldnames based on the keys of the dictionaries
                    fieldnames = results[0].keys()

                    # Create a CSV DictWriter object
                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

                    # Write the header row
                    writer.writeheader()

                    # Write each dictionary as a row in the CSV file
                    for row in results:
                        writer.writerow(row)

            
            for i in range(len(entire_logs)):
                # print(f"Reading text: {text}")
                # input_ids = tokenizer(text, return_tensors="pt", truncation=True).input_ids.cuda()
                # # print("at 335")
                # s = time.time()
                # outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9)
                # # print("at 338")
                # e = time.time()
                # sum_time = sum_time + (e - s)
                # # print(sum_time)

                # ## Out is the predicted output.
                # out = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]
               
                if out[i] == 'Normal, This is Normal log':
                    normal_logs.append({"status": "Normal", "text":entire_logs[i]})
                    logs.append({"status": "Normal", "text":entire_logs[i]})
                else:
                    logs.append({"status": "Anomaly", "text":entire_logs[i]})
                    anomaly_logs.append({"status": "Anomaly", "text":entire_logs[i]})
                    responses.append(out)
                    if out[i] == 'Disassociated':
                        Disassoc +=1
                    elif out[i] == 'Deauthenticate':
                        Deauth +=1
                    elif out[i] == 'Disconnected':
                        Disconnect += 1

            # print(Disassoc, Deauth, Disconnect)
                    
            found_anomalous_log = len(anomaly_logs)
            found_normal_log = len(normal_logs)
            total_logs = found_anomalous_log + found_normal_log
            percent_disassoc = (Disassoc/total_logs) * 100
            percent_disassoc = round(percent_disassoc, 2)

            percent_deauth = (Deauth/total_logs) * 100
            percent_deauth = round(percent_deauth, 2)

            percent_disconnect = (Disconnect/total_logs) * 100
            percent_disconnect = round(percent_disconnect, 2)

            percent_anom = (found_anomalous_log/total_logs) * 100
            percent_norm = 100 - percent_anom

            percent_norm = round(percent_norm, 2)
            anomaly_logs_2 = ' '
            responses_2 = ' '

            # for i in anomaly_logs:
            #     anomaly_logs_2 += i
            #     anomaly_logs_2 += '\n\n\n'
            # for i in responses:
            #     responses_2 += i 
            #     responses_2 += '\n\n\n\n\n'
                
        elapsed_time_wifi = end_time_wifi - start_time
        elapsed_time_wifi = round(elapsed_time_wifi, 2)
        show_btn_container = True
        percentage=((Disassoc+Deauth+Disconnect)/(Disassoc+Deauth+Disconnect+found_anomalous_log+found_normal_log))*100

        # print(percentage)

        result_3 = round(percentage, 1)

        # print(result_3)

        percent_str = str(result_3) + "%"
        response = {
            "Placeholder1": elapsed_time_wifi,
            "Placeholder2": total_logs,
            "Placeholder3": found_anomalous_log,
            "Placeholder4": found_normal_log,
            "Placeholder5": Disassoc,
            "Placeholder6": Deauth,
            "Placeholder7": Disconnect,
            "Placeholder8": percent_str,
            "Placeholder9": percent_disassoc,
            "Placeholder10": percent_deauth,
            "Placeholder11": percent_disconnect,
            "Placeholder12": percent_norm,
            "total_logs": entire_logs,
            "output": out,
            "anomalies": anomaly_logs,
            "logs": logs,
            "list_ts": list_ts
        }
        return JSONResponse(content=response)


@router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["packet_analysis"])
async def packet_analysis(
    upload_name: str = Form(...),
    logs: UploadFile = File(...),
    output: UploadFile = File(...),
    list_ts: UploadFile = File(...)
):
    print("Packets processing!!")
    
    # Save the uploaded files
    upload_folder = upload_name
    print(upload_folder)
    
    packet_file = None
    for file in os.listdir(upload_folder):
        if not file.endswith(".txt"):
            packet_file = os.path.join(upload_folder, file)

    if packet_file is None:
        return JSONResponse(content={"error": "Packet file not found"}, status_code=400)

    logs_data = json.loads(await logs.read())
    output_data = json.loads(await output.read())
    list_ts_data = json.loads(await list_ts.read())

    # To load packet
    print("Packet is loading....")
    packets = []
    pcap = rdpcap(packet_file)
    print("Packet loaded")

    # print(logs_data)
    # print(output_data)

    # To process the packets and obtain result about failure packets
    failure_packets = process_packets(pcap, logs_data, output_data, list_ts_data)
    print("Packets processed")
    
    # pprint.pprint(failure_packets)

    current_timestamp = datetime.now()
    timestamp_str = current_timestamp.strftime("%Y-%m-%d-%H-%M-%S")
    filename = f"failure_packets_{timestamp_str}.csv"

    # CSV file path
    if len(failure_packets) > 0:
        csv_file_path = f'/media/sahil/data1/gyan_backend/temp_packets/temp_packets/{filename}'

        # Open the CSV file in write mode
        with open(csv_file_path, mode='w', newline='') as csv_file:
            # Define the fieldnames based on the keys of the dictionaries
            fieldnames = failure_packets[0].keys()

            # Create a CSV DictWriter object
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

            # Write the header row
            writer.writeheader()

            # Write each dictionary as a row in the CSV file
            for row in failure_packets:
                writer.writerow(row)

    return {
        "filename": filename,
        "failure_packets": failure_packets
    }


@router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["export"])
async def export_report(filename: str = Form(...)):
    folder_path = "/media/sahil/data1/gyan-backend/temp_packets"
    file_path = os.path.join(folder_path, filename)

    if os.path.isfile(file_path):
        print("File present")
        return FileResponse(file_path, media_type='application/octet-stream', filename=filename)
    else:
        raise HTTPException(status_code=400, detail="File not present")
 
    
@router_swd.post(ENDPOINTS["usecases"]["software_debugger"]["routes"]["wifi_hf"]["packet_loading"])
async def upload_files(
    hapd_file: UploadFile = File(...),  # Required file
    packet_file: UploadFile = File(...)  # Required file
):
    # Check if filenames are empty
    if hapd_file.filename == '' or packet_file.filename == '':
        raise HTTPException(status_code=400, detail="No selected file")

    # Check file format of hapd_file
    if not hapd_file.filename.endswith('.txt'):
        raise HTTPException(status_code=400, detail="Invalid file format. Expected .txt")

    # Create the upload folder if it doesn't exist
    if not os.path.exists(UPLOAD_FOLDER):
        os.makedirs(UPLOAD_FOLDER)

    # Save the files
    hapd_file_path = os.path.join(UPLOAD_FOLDER, hapd_file.filename)
    pcap_file_path = os.path.join(UPLOAD_FOLDER, packet_file.filename)

    with open(hapd_file_path, "wb") as hapd_f, open(pcap_file_path, "wb") as pcap_f:
        hapd_f.write(await hapd_file.read())
        pcap_f.write(await packet_file.read())

    # Return a JSON response
    return JSONResponse(content={"upload_name": UPLOAD_FOLDER})
