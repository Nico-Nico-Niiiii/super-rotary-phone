apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  namespace: default
  name: job1-vllm
  annotations:
    original-service-name: job1
spec:
  predictor:
    minReplicas: 0
    maxReplicas: 1
    containers:
    - args:
      - --port
      - '8080'
      - --model
      - /mnt/models
      command:
      - python3
      - -m
      - vllm.entrypoints.openai.api_server
      env:
      - name: STORAGE_URI
        value: pvc://job1-pvc/model_files/
      image: docker.io/kserve/vllmserver:latest
      imagePullPolicy: IfNotPresent
      name: kserve-container
      resources:
        limits:
          cpu: 32
          memory: 30Gi
          nvidia.com/gpu: 1
        requests:
          cpu: 10
          memory: 10Gi
          nvidia.com/gpu: 1
